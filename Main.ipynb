{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc825eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89fb4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase Of Update\n",
    "# response = requests.get('https://db.ygoprodeck.com/api/v7/cardinfo.php')\n",
    "# json_response = response.json()\n",
    "# dataset = pd.DataFrame(json_response['data'])\n",
    "\n",
    "# dataset.to_csv('Dataset/Yugioh_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc1ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import random as random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "plt.style.use('Solarize_Light2')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "#import requests\n",
    "#import itertools\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0] , True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef56086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "'''Load Dataset'''\n",
    "dataset = pd.read_csv('Dataset/Yugioh_Database.csv')\n",
    "dataset = dataset.drop(['Unnamed: 0' , 'frameType' , 'archetype' , 'ygoprodeck_url' , 'card_sets' , 'card_images' , 'card_prices' , 'banlist_info'],axis=1)\n",
    "dataset = dataset[dataset['type'] != 'XYZ Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Normal Monster']\n",
    "\n",
    "dataset = dataset[dataset['type'] != 'Skill Card']\n",
    "dataset = dataset[dataset['type'] != 'Monster Token']\n",
    "\t \n",
    "# dataset = dataset[dataset['type'] != 'Spell Card']\n",
    "# dataset = dataset[dataset['type'] != 'Trap Card']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Monster']\n",
    "dataset = dataset[dataset['type'] != 'XYZ Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Monster']\n",
    "dataset = dataset[dataset['type'] != 'Link Monster']\n",
    "# Staple Removal\n",
    "dataset = dataset[[not i for i in dataset['name'].isin(['Ash Blossom & Joyous Spring' , 'Effect Veiler' , 'Ghost Ogre & Snow Rabbit' ,'Ghost Belle & Haunted Mansion',\n",
    "                                                        'Infinite Impermanence' , 'Red Reboot' , 'Called by the Grave' , 'Forbidden Droplet' , 'Crossout Designator',\n",
    "                                                        'Nibiru, the Primal Being', 'Harpie\\\"s Feather Duster' , 'Lightning Storm' , 'Pot of Prosperity' , 'Pot of Desires',\n",
    "                                                        'Pot of Duality' , 'Pot of Extravagance' , 'Triple Tactics Talents' , 'Torrential Tribute' , 'Dark Ruler No More' , \n",
    "                                                        'Red Reboot', 'D.D. Crow' , 'PSY-Framegear Gamma' , 'Maxx \\\"C\\\"' , 'Dimension Shifter' , 'Droll & Lock Bird' , \n",
    "                                                        'Accesscode Talker', 'Apollousa, Bow of the Goddess', 'Borreload Dragon' , 'Borrelsword Dragon', 'Knightmare Unicorn',\n",
    "                                                        'Predaplant Verte Anaconda' , 'Knightmare Phoenix' , 'Knightmare Cerberus' , 'Underworld Goddess of the Closed World',\n",
    "                                                        'Borreload Savage Dragon' , 'Token Collector' , 'Evenly Matched' , 'Forbidden Chalice' , 'Cosmic Cyclone' , 'Contact \\\"C\\\"',\n",
    "                                                        'Retaliating \\\"C\\\"' , 'Gadarla, the Mystery Dust Kaiju' , 'Solemn Judgment' , 'Dimensional Barrier' , 'Solemn Strike',\n",
    "                                                         'Ice Dragon\\'s Prison' , 'Gozen Match' ])]]\n",
    "\n",
    "dataset.loc[dataset['type']=='Normal Monster', ['desc']] = 'NoInfo'\n",
    "dataset = dataset.fillna('0')\n",
    "dataset['level'] = dataset['level'].astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "'''Create Tokenized sequence database'''\n",
    "\n",
    "\n",
    "df = dataset['desc']         #Tokenizer is only trained on desc and based on that . Otherwise if trained on names it would blow vocab up to absurd amounts\n",
    "Sliced_df = dataset[['level' , 'race' , 'type' , 'attribute' , 'name' , 'desc']]\n",
    "\n",
    "for i in range(1,11,2):\n",
    "    Sliced_df.insert(loc=i, column='A'+str(i), value=-1)        # Adds seperator columns\n",
    "\n",
    "\n",
    "Sliced_df = Sliced_df.reset_index(drop=True)            # Need to reset the indexes so they are consistent\n",
    "df = df.reset_index(drop=True)                              \n",
    "\n",
    "tokenizer = Tokenizer(filters='\\r , \\n , \\\" ') # Speech marks stop names from being recognised by tokenizer\n",
    "tokenizer.fit_on_texts(df)\n",
    "tokenizer.word_index['0'] = 0           #Signifies Empty values\n",
    "tokenizer.word_index['-1'] = -1           #Signifies Seperators\n",
    "\n",
    "sequences = []\n",
    "padded_sequences = []\n",
    "Tokenized_sequence_database = []\n",
    "count = 0\n",
    "for i in Sliced_df.astype('string').to_numpy():\n",
    "    \n",
    "    sequences.append(tokenizer.texts_to_sequences(i))\n",
    "    \n",
    "\n",
    "for i in range(0,11):\n",
    "    padded_sequences.append( pad_sequences(np.array(sequences , dtype='object')[:,i], padding='post') ) \n",
    "\n",
    "Tokenized_sequence_database = np.concatenate(([padded_sequences[i] for i in range(11)]) , axis=1 )\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>A1</th>\n",
       "      <th>race</th>\n",
       "      <th>A3</th>\n",
       "      <th>type</th>\n",
       "      <th>A5</th>\n",
       "      <th>attribute</th>\n",
       "      <th>A7</th>\n",
       "      <th>name</th>\n",
       "      <th>A9</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Breeding Device</td>\n",
       "      <td>-1</td>\n",
       "      <td>During each of your Standby Phases, put 1 A-Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Incubator</td>\n",
       "      <td>-1</td>\n",
       "      <td>Each time an A-Counter(s) is removed from play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Quick-Play</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Recombination Device</td>\n",
       "      <td>-1</td>\n",
       "      <td>Target 1 face-up monster on the field; send 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Quick-Play</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Scatter Burst</td>\n",
       "      <td>-1</td>\n",
       "      <td>Select 1 face-up \"Alien\" monster you control. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Equip</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"Infernoble Arms - Almace\"</td>\n",
       "      <td>-1</td>\n",
       "      <td>While this card is equipped to a monster: You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10685</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Sleipnir Mail</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Utopia\" monster you control;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Sylphid Wing</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can only control 1 \"ZW - Sylphid Wing\". Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10687</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>WIND</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Tornado Bringer</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Utopia\" monster you control;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10688</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Aqua</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>EARTH</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Ultimate Shield</td>\n",
       "      <td>-1</td>\n",
       "      <td>When this card is Normal or Special Summoned: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Unicorn Spear</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Number C39: Utopia Ray\" you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10690 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level  A1        race  A3            type  A5 attribute  A7  \\\n",
       "0          0  -1  Continuous  -1      Spell Card  -1         0  -1   \n",
       "1          0  -1  Continuous  -1      Spell Card  -1         0  -1   \n",
       "2          0  -1  Quick-Play  -1      Spell Card  -1         0  -1   \n",
       "3          0  -1  Quick-Play  -1      Spell Card  -1         0  -1   \n",
       "4          0  -1       Equip  -1      Spell Card  -1         0  -1   \n",
       "...      ...  ..         ...  ..             ...  ..       ...  ..   \n",
       "10685      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "10686      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "10687      5  -1      Dragon  -1  Effect Monster  -1      WIND  -1   \n",
       "10688      4  -1        Aqua  -1  Effect Monster  -1     EARTH  -1   \n",
       "10689      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "\n",
       "                                name  A9  \\\n",
       "0           \"A\" Cell Breeding Device  -1   \n",
       "1                 \"A\" Cell Incubator  -1   \n",
       "2      \"A\" Cell Recombination Device  -1   \n",
       "3             \"A\" Cell Scatter Burst  -1   \n",
       "4         \"Infernoble Arms - Almace\"  -1   \n",
       "...                              ...  ..   \n",
       "10685             ZW - Sleipnir Mail  -1   \n",
       "10686              ZW - Sylphid Wing  -1   \n",
       "10687           ZW - Tornado Bringer  -1   \n",
       "10688           ZW - Ultimate Shield  -1   \n",
       "10689             ZW - Unicorn Spear  -1   \n",
       "\n",
       "                                                    desc  \n",
       "0      During each of your Standby Phases, put 1 A-Co...  \n",
       "1      Each time an A-Counter(s) is removed from play...  \n",
       "2      Target 1 face-up monster on the field; send 1 ...  \n",
       "3      Select 1 face-up \"Alien\" monster you control. ...  \n",
       "4      While this card is equipped to a monster: You ...  \n",
       "...                                                  ...  \n",
       "10685  You can target 1 \"Utopia\" monster you control;...  \n",
       "10686  You can only control 1 \"ZW - Sylphid Wing\". Yo...  \n",
       "10687  You can target 1 \"Utopia\" monster you control;...  \n",
       "10688  When this card is Normal or Special Summoned: ...  \n",
       "10689  You can target 1 \"Number C39: Utopia Ray\" you ...  \n",
       "\n",
       "[10690 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sliced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf6d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deck_Loader(directory):\n",
    "    '''Loads Decks from Deck_Lists.txt as arrays and stores those arrays in altered'''\n",
    "    file = open(directory , 'r')\n",
    "    read = file.readlines()\n",
    "    Deck_Array = []\n",
    "    flag = False\n",
    "\n",
    "    temp=[]\n",
    "\n",
    "    for count,line in enumerate(read):\n",
    "        \n",
    "        if '//' in read[count]:\n",
    "            flag = not flag\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            read[count] = read[count].replace('\\n','')\n",
    "            \n",
    "            if ('=='  in read[count]) or ('//'  in read[count])  :\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                for i in range(int(read[count][0])):\n",
    "                    temp.append(read[count][1:].strip())          #skip appending also remove white space\n",
    "\n",
    "        if (not flag) or (count == len(read) - 1):\n",
    "            Deck_Array.append(temp)\n",
    "            temp = []\n",
    "            flag = not flag\n",
    "            \n",
    "            \n",
    "    file.close()\n",
    "    return Deck_Array \n",
    "\n",
    "def Dataset_Builder(Direc):\n",
    "    Loaded_Decks = Deck_Loader(Direc)\n",
    "\n",
    "    Output = []\n",
    "    for Deck in Loaded_Decks:\n",
    "        for _ in range(30):\n",
    "            choices = Sliced_df[Sliced_df['name'].isin(Deck)].index.values          # Prevents staples from leaking into data as sliceddf is filtered from staples.\n",
    "\n",
    "            indexes = random.sample(choices.tolist() , 6)\n",
    "            Decider = [Tokenized_sequence_database[i] for i in indexes[0:5]]\n",
    "            Subject = [Tokenized_sequence_database[indexes[5]] , np.zeros(181 , dtype='int32') , np.zeros(181 , dtype='int32') , np.zeros(181 , dtype='int32') , np.zeros(181 , dtype='int32')]\n",
    "            Output.append([Decider , Subject , [1]])\n",
    "\n",
    "    for _ in range(len(Output)):\n",
    "        Decks = random.sample(Loaded_Decks , 2)\n",
    "        choices1 = Sliced_df[Sliced_df['name'].isin(Decks[0])].index.values \n",
    "        choices2 = Sliced_df[Sliced_df['name'].isin(Decks[1])].index.values\n",
    "        \n",
    "        Decider = [Tokenized_sequence_database[i] for i in random.sample(choices1.tolist() , 5)]\n",
    "        Subject = [Tokenized_sequence_database[random.choice(choices2)] , np.zeros(181 , dtype='int32') , np.zeros(181 , dtype='int32') , np.zeros(181 , dtype='int32') , np.zeros(181 , dtype='int32')]\n",
    "        Output.append([Decider , Subject , [0]])\n",
    "    \n",
    "    random.shuffle(Output)\n",
    "    random.shuffle(Output)\n",
    "    random.shuffle(Output)\n",
    "    return Output\n",
    "\n",
    "Training_Dataset = Dataset_Builder('Dataset/Training_Deck_Lists.txt')\n",
    "Validation_Dataset = Dataset_Builder('Dataset/Validation_Deck_Lists.txt')\n",
    "\n",
    "\n",
    "#pd.DataFrame(Training_Dataset)\n",
    "#x,y = Training_Dataset[0][0]\n",
    "#tokenizer.sequences_to_texts(np.array(y))\n",
    "#np.array(x).shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5796e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d58ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5140d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate( [np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "\n",
    "class Custom_Embedder(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(vocab_size), \n",
    "      tf.keras.layers.Dense(d_model )\n",
    "      \n",
    "    ])\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self,x):\n",
    "\n",
    "    x = self.seq(x)\n",
    "    x = self.layer_norm(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "class Positional_Embedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = Custom_Embedder(vocab_size, d_model) \n",
    "    self.pos_encoding = positional_encoding(length = 5, depth=d_model)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x*= np.sqrt(self.d_model) # Scale Values by their embedding dimensionality otherwise they could get overwhelmed by positional encoder\n",
    "    x = x + self.pos_encoding \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class DeciderSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, ffn, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ffn, activation='relu'), \n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Single Encoder Layer with DeciderMHA and Feed Forward layer\"\"\"\n",
    "  def __init__(self, d_model, ffn , dropout_rate ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.DSA = DeciderSelfAttention(num_heads=4, key_dim=d_model , dropout = dropout_rate)       # Scaling Number of Heads increases parameters as this is a different implementation of mha compared to attention is all you need paper.\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    \n",
    "    x = self.DSA(x)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Full Encoder with embedding layer with dropout and encoder layers\"\"\"\n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.EL = [EncoderLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self,x):\n",
    "    \n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.EL[i](x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class SubjectSelfAttention(BaseAttention):\n",
    "  \n",
    "  def call(self, x):\n",
    "    \n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class SubjectCrossAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x , context):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        key=context)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class ComparatorLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self , d_model, ffn , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.SSA = SubjectSelfAttention(num_heads=4, key_dim=d_model , dropout = dropout_rate)      \n",
    "    self.SCA = SubjectCrossAttention(num_heads=4, key_dim=d_model , dropout = dropout_rate)\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    \n",
    "    x = self.SSA(x)\n",
    "\n",
    "    x = self.SCA(x , context)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "class Comparator(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.CL = [ComparatorLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    \n",
    "    x = self.Pos_Embedding(x)\n",
    "    \n",
    "    x = self.dropout(x)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.CL[i](x , context)\n",
    "\n",
    "    return x\n",
    "\n",
    "class FinalFeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),          #Flattens sentances for each card comparision , into a single 1d array , so it can generate probabilities properly, instead of shoving 100 x905 matrix straight through and generating 100 probabilities for each card comparision feature embedding\n",
    "      # tf.keras.layers.Dense(50, activation='relu'),\n",
    "      # tf.keras.layers.Dense(25, activation='relu'),\n",
    "      # tf.keras.layers.Dropout(dropout_rate),\n",
    "      tf.keras.layers.Dense(1 , activation='sigmoid')\n",
    "      \n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.seq(x) \n",
    "    return x\n",
    "  \n",
    "class FullModel(tf.keras.Model):\n",
    "   def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = Encoder(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.com = Comparator(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.FFF = FinalFeedForward(dropout_rate)\n",
    "\n",
    "   def call(self, inputs):\n",
    "     context , x = inputs\n",
    "     \n",
    "     \n",
    "     \n",
    "     context = self.enc(context)\n",
    "    \n",
    "     x = self.com(x , context)\n",
    "\n",
    "     x = self.FFF(x)\n",
    "\n",
    "     return x\n",
    "   \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46854f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba82c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e1a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e68f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Dataset]\n",
    "    for _ in range(10000):\n",
    "        r_int = random.randint(0 , len(Training_Dataset) - 50)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50])\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50])\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+50])\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def Val_Gen():\n",
    "    seti0 = [i[0] for i in Validation_Dataset]\n",
    "    seti1 = [i[1] for i in Validation_Dataset]\n",
    "    seti2 = [i[2] for i in Validation_Dataset]\n",
    "    for _ in range(10000):\n",
    "        r_int = random.randint(0 , len(Validation_Dataset) - 50)\n",
    "        val_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50])\n",
    "        val_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50])\n",
    "        val_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+50])\n",
    "\n",
    "        yield (val_seti0,val_seti1), val_seti2\n",
    "\n",
    "def Train_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Dataset]\n",
    "    for _ in range(10000):\n",
    "        r_int = random.randint(0 , int(len(Training_Dataset)*0.8) - 50)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50])\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50])\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+50])\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def Val_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Dataset]\n",
    "    for _ in range(10000):\n",
    "        r_int = random.randint(int(len(Training_Dataset)*0.8) , len(Training_Dataset) - 50)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50])\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50])\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+50])\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def Pred_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Dataset]\n",
    "    for _ in range(1):\n",
    "        r_int = random.randint(int(len(Training_Dataset)*0.8) , len(Training_Dataset) - 300)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+300])\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+300])\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+300])\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2603da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 10s 49ms/step - loss: 0.7774 - accuracy: 0.5258 - val_loss: 0.7437 - val_accuracy: 0.4856\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.7115 - accuracy: 0.5568 - val_loss: 0.8688 - val_accuracy: 0.4704\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.6758 - accuracy: 0.6044 - val_loss: 0.8197 - val_accuracy: 0.4492\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.6191 - accuracy: 0.6580 - val_loss: 0.9815 - val_accuracy: 0.4516\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 0.5858 - accuracy: 0.6906 - val_loss: 0.8864 - val_accuracy: 0.5056\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 0.5411 - accuracy: 0.7206 - val_loss: 0.9341 - val_accuracy: 0.4936\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 0.5357 - accuracy: 0.7310 - val_loss: 0.8808 - val_accuracy: 0.5196\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.4988 - accuracy: 0.7524 - val_loss: 1.0228 - val_accuracy: 0.4948\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.4893 - accuracy: 0.7594 - val_loss: 1.0364 - val_accuracy: 0.5128\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.4906 - accuracy: 0.7578 - val_loss: 1.1464 - val_accuracy: 0.4940\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.4685 - accuracy: 0.7770 - val_loss: 0.9343 - val_accuracy: 0.5380\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.4287 - accuracy: 0.7992 - val_loss: 1.3427 - val_accuracy: 0.5056\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.4493 - accuracy: 0.7922 - val_loss: 1.1684 - val_accuracy: 0.4880\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.4333 - accuracy: 0.7998 - val_loss: 1.2270 - val_accuracy: 0.5344\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.4126 - accuracy: 0.8116 - val_loss: 1.1487 - val_accuracy: 0.5396\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3922 - accuracy: 0.8286 - val_loss: 1.1462 - val_accuracy: 0.5684\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.4228 - accuracy: 0.8044 - val_loss: 0.8666 - val_accuracy: 0.5640\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3874 - accuracy: 0.8256 - val_loss: 0.9593 - val_accuracy: 0.5640\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3996 - accuracy: 0.8230 - val_loss: 0.9708 - val_accuracy: 0.5152\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.4378 - accuracy: 0.8052 - val_loss: 1.1267 - val_accuracy: 0.5712\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.4783 - accuracy: 0.7710 - val_loss: 0.9938 - val_accuracy: 0.5404\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.4668 - accuracy: 0.7782 - val_loss: 1.0641 - val_accuracy: 0.5272\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.4832 - accuracy: 0.7686 - val_loss: 1.1213 - val_accuracy: 0.5268\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.4833 - accuracy: 0.7758 - val_loss: 0.9196 - val_accuracy: 0.6020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.5104 - accuracy: 0.7558 - val_loss: 1.0687 - val_accuracy: 0.5188\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.5438 - accuracy: 0.7314 - val_loss: 0.9032 - val_accuracy: 0.5268\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.5558 - accuracy: 0.7192 - val_loss: 0.9737 - val_accuracy: 0.5164\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.5845 - accuracy: 0.6916 - val_loss: 0.7282 - val_accuracy: 0.5412\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.5948 - accuracy: 0.6892 - val_loss: 0.7657 - val_accuracy: 0.5508\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.5950 - accuracy: 0.6722 - val_loss: 0.7933 - val_accuracy: 0.5308\n"
     ]
    }
   ],
   "source": [
    "Model = FullModel(200 , 5000 , 1000 , 0.2 , 2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model = 200)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate , beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "\n",
    "Model.compile(\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=optimizer,\n",
    "    metrics= 'accuracy' )\n",
    "\n",
    "history = Model.fit(Train_Split_Gen() , epochs=30, \n",
    "                               validation_data = Val_Split_Gen()  , steps_per_epoch=100 , batch_size=50 , \n",
    "                               validation_steps=50 , validation_batch_size=50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f5b2e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"full_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_3 (Encoder)         multiple                  3999800   \n",
      "                                                                 \n",
      " comparator_3 (Comparator)   multiple                  5285800   \n",
      "                                                                 \n",
      " final_feed_forward_3 (Final  multiple                 1001      \n",
      " FeedForward)                                                    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,286,601\n",
      "Trainable params: 9,286,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "576e9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 12ms/step\n",
      "Accuracy is: 0.47333333333333333threshold is: 0.0\n",
      "Accuracy is: 0.4766666666666667threshold is: 0.05\n",
      "Accuracy is: 0.4766666666666667threshold is: 0.1\n",
      "Accuracy is: 0.48threshold is: 0.15\n",
      "Accuracy is: 0.49666666666666665threshold is: 0.2\n",
      "Accuracy is: 0.49threshold is: 0.25\n",
      "Accuracy is: 0.5166666666666667threshold is: 0.3\n",
      "Accuracy is: 0.52threshold is: 0.35\n",
      "Accuracy is: 0.5433333333333333threshold is: 0.4\n",
      "Accuracy is: 0.52threshold is: 0.45\n",
      "Accuracy is: 0.5366666666666666threshold is: 0.5\n",
      "Accuracy is: 0.56threshold is: 0.55\n",
      "Accuracy is: 0.5566666666666666threshold is: 0.6\n",
      "Accuracy is: 0.55threshold is: 0.65\n",
      "Accuracy is: 0.5566666666666666threshold is: 0.7\n",
      "Accuracy is: 0.55threshold is: 0.75\n",
      "Accuracy is: 0.55threshold is: 0.8\n",
      "Accuracy is: 0.5333333333333333threshold is: 0.85\n",
      "Accuracy is: 0.5433333333333333threshold is: 0.9\n",
      "Accuracy is: 0.5466666666666666threshold is: 0.95\n"
     ]
    }
   ],
   "source": [
    "### Plot prediction values against true Labels\n",
    "Data , True_Labels = next(Pred_Split_Gen())\n",
    "Predictions = Model.predict(Data)\n",
    "\n",
    "vals = pd.DataFrame([{'z' : 0 ,'x' : i[0],'y' : j[0]} for i,j in zip(Predictions , True_Labels.numpy())])\n",
    "\n",
    "#sns.scatterplot(data=vals , x='z' , y='x' , hue='y')\n",
    "\n",
    "for threshold in range(0,20,1):\n",
    "    count = 0\n",
    "    for i in vals.values:\n",
    "        if i[1] > threshold/20:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = 0\n",
    "        \n",
    "        if pred == i[2]:\n",
    "            count+=1\n",
    "    print('Accuracy is: ' + str(count/len(vals)) + 'threshold is: ' + str(threshold/20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5880b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in vals.values:\n",
    "    print(len(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50364525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9022968]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ,'Galaxy Summoner']\n",
    "    \n",
    "decider = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "subject_card = tf.convert_to_tensor([np.concatenate( (Tokenized_sequence_database[4281] , np.zeros(724)) , axis=None )])\n",
    "\n",
    "Model((decider , subject_card))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6848a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_matches():\n",
    "    #input_array = [input('enter') for _ in range(5)]\n",
    "    input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ,'Galaxy Summoner']\n",
    "    \n",
    "    decider = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "    count = 0\n",
    "    for indx in pd.DataFrame(Tokenized_sequence_database).index.values:\n",
    "        if count<0:\n",
    "            pass\n",
    "        else:\n",
    "            if (count % 100) == 0:\n",
    "                print(count)\n",
    "            subject_card = tf.convert_to_tensor([np.concatenate( (Tokenized_sequence_database[indx] , np.zeros(724)) , axis=None )])\n",
    "            \n",
    "            \n",
    "            if Model.predict((decider , subject_card) , verbose=False)[0][0] > 0.5:\n",
    "                print(Sliced_df[indx:indx+1]['name'])\n",
    "                \n",
    "                \n",
    "        count+=1\n",
    "predict_matches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1cb6a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 8s 12ms/step\n",
      "0.16468253968253968\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame(Model.predict(Experimentation_Gen()))\n",
    "count = 0\n",
    "for i in pred[0]:\n",
    "    if i > 0.5:\n",
    "        count+=1\n",
    "print(count/len(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 219ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9966325]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k , trash = next(Experimentation_Gen())\n",
    "l1 , l2 = k\n",
    "singular_pred = Model.predict(k)\n",
    "singular_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9e8875d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"2 aqua effect monster water cannot be used as a synchro material. this card's name becomes des frog while it is on the field. if this card is in your graveyard: you can banish 1 frog monster from your graveyard; special summon this card.\"]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.sequences_to_texts([l2[0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d9ca161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1065,   -1,  398,    0,   -1,  294,   20,    8,    0,   -1,  251,\n",
       "         -1, 1618,   10,    5,  731,    0,    0,    0,    0,    0,   -1,\n",
       "        200,    1,    6,   87,    7,   10,  231,  508,   78,  134,   83,\n",
       "        315,   78,  129,   89,   75,   22,  113,   76,   38,    5,   44,\n",
       "          9,    5,   47,   78,   58,   89,   32,    3,   42,  262,   78,\n",
       "        384,  125,    3,   48,   29,   98,  197,    7,    4,   13,  140,\n",
       "         29,  188,    5,   92,    1,    6,   24,   31,    2,   20,   10,\n",
       "       1618,   10,    5,  731,   18,   19,   25,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenized_sequence_database[4281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638286c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1332: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.start` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1383: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1383: save (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\profiler.py:150: maybe_create_event_file (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
     ]
    }
   ],
   "source": [
    "Model = FullModel(100 , 12647 , 1000 , 0.1 , 2)\n",
    "writer = tf.summary.create_file_writer('Logs/GraphViz')\n",
    "\n",
    "input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ,'Galaxy Summoner']\n",
    "\n",
    "decider1 = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "subject_card1 = tf.convert_to_tensor(    [np.concatenate( (Tokenized_sequence_database[7] , np.zeros(724)) , axis=None )]     ) \n",
    "\n",
    "@tf.function\n",
    "def my_func(x):\n",
    "    return Model(x)\n",
    "\n",
    "tf.summary.trace_on(graph=True,profiler=True)\n",
    "out = my_func((decider1 , subject_card1))\n",
    "\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name='function_Trace',\n",
    "        step=0,\n",
    "        profiler_outdir='Logs\\\\GraphViz\\\\'\n",
    "\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67126460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25bc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd683030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
