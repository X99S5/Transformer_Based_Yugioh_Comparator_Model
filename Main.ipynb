{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc825eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase Of Update\n",
    "response = requests.get('https://db.ygoprodeck.com/api/v7/cardinfo.php')\n",
    "json_response = response.json()\n",
    "dataset = pd.DataFrame(json_response['data'])\n",
    "\n",
    "dataset.to_csv('Dataset/Yugioh_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc1ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import random as random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "plt.style.use('Solarize_Light2')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "#import requests\n",
    "#import itertools\n",
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef56086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "'''Load Dataset'''\n",
    "dataset = pd.read_csv('Dataset/Yugioh_Database.csv')\n",
    "dataset = dataset.drop(['Unnamed: 0' , 'frameType' , 'archetype' , 'ygoprodeck_url' , 'card_sets' , 'card_images' , 'card_prices' , 'banlist_info'],axis=1)\n",
    "dataset = dataset[dataset['type'] != 'XYZ Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Normal Monster']\n",
    "\n",
    "dataset = dataset[dataset['type'] != 'Skill Card']\n",
    "dataset = dataset[dataset['type'] != 'Monster Token']\n",
    "# Staple Removal\n",
    "dataset = dataset[[not i for i in dataset['name'].isin(['Ash Blossom & Joyous Spring' , 'Effect Veiler' , 'Ghost Ogre & Snow Rabbit' ,'Ghost Belle & Haunted Mansion',\n",
    "                                                        'Infinite Impermanence' , 'Red Reboot' , 'Called by the Grave' , 'Forbidden Droplet' , 'Crossout Designator',\n",
    "                                                        'Nibiru, the Primal Being', 'Harpie\\\"s Feather Duster' , 'Lightning Storm' , 'Pot of Prosperity' , 'Pot of Desires',\n",
    "                                                        'Pot of Duality' , 'Pot of Extravagance' , 'Triple Tactics Talents' , 'Torrential Tribute' , 'Dark Ruler No More' , \n",
    "                                                        'Red Reboot', 'D.D. Crow' , 'PSY-Framegear Gamma' , 'Maxx \\\"C\\\"' , 'Dimension Shifter' , 'Droll & Lock Bird' , \n",
    "                                                        'Accesscode Talker', 'Apollousa, Bow of the Goddess', 'Borreload Dragon' , 'Borrelsword Dragon', 'Knightmare Unicorn',\n",
    "                                                        'Predaplant Verte Anaconda' , 'Knightmare Phoenix' , 'Knightmare Cerberus' , 'Underworld Goddess of the Closed World',\n",
    "                                                        'Borreload Savage Dragon'])]]\n",
    "\n",
    "dataset.loc[dataset['type']=='Normal Monster', ['desc']] = 'NoInfo'\n",
    "dataset = dataset.fillna('0')\n",
    "dataset['level'] = dataset['level'].astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "'''Create Tokenized sequence database'''\n",
    "\n",
    "\n",
    "df = dataset['desc']         #Tokenizer is only trained on desc and based on that . Otherwise if trained on names it would blow vocab up to absurd amounts\n",
    "Sliced_df = dataset[['level' , 'race' , 'type' , 'attribute' , 'name' , 'desc']]\n",
    "\n",
    "for i in range(1,11,2):\n",
    "    Sliced_df.insert(loc=i, column='A'+str(i), value=-1)        # Adds seperator columns\n",
    "\n",
    "\n",
    "Sliced_df = Sliced_df.reset_index(drop=True)            # Need to reset the indexes so they are consistent\n",
    "df = df.reset_index(drop=True)                              \n",
    "\n",
    "tokenizer = Tokenizer(filters='\\r , \\n , \\\" ') # Speech marks stop names from being recognised by tokenizer\n",
    "tokenizer.fit_on_texts(df)\n",
    "tokenizer.word_index['0'] = 0           #Signifies Empty values\n",
    "tokenizer.word_index['-1'] = -1           #Signifies Seperators\n",
    "\n",
    "sequences = []\n",
    "padded_sequences = []\n",
    "Tokenized_sequence_database = []\n",
    "count = 0\n",
    "for i in Sliced_df.astype('string').to_numpy():\n",
    "    \n",
    "    sequences.append(tokenizer.texts_to_sequences(i))\n",
    "    \n",
    "\n",
    "for i in range(0,11):\n",
    "    padded_sequences.append( pad_sequences(np.array(sequences , dtype='object')[:,i], padding='post') ) \n",
    "\n",
    "Tokenized_sequence_database = np.concatenate(([padded_sequences[i] for i in range(11)]) , axis=1 )\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>A1</th>\n",
       "      <th>race</th>\n",
       "      <th>A3</th>\n",
       "      <th>type</th>\n",
       "      <th>A5</th>\n",
       "      <th>attribute</th>\n",
       "      <th>A7</th>\n",
       "      <th>name</th>\n",
       "      <th>A9</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Breeding Device</td>\n",
       "      <td>-1</td>\n",
       "      <td>During each of your Standby Phases, put 1 A-Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Incubator</td>\n",
       "      <td>-1</td>\n",
       "      <td>Each time an A-Counter(s) is removed from play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Quick-Play</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Recombination Device</td>\n",
       "      <td>-1</td>\n",
       "      <td>Target 1 face-up monster on the field; send 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Quick-Play</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Scatter Burst</td>\n",
       "      <td>-1</td>\n",
       "      <td>Select 1 face-up \"Alien\" monster you control. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Equip</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"Infernoble Arms - Almace\"</td>\n",
       "      <td>-1</td>\n",
       "      <td>While this card is equipped to a monster: You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Sleipnir Mail</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Utopia\" monster you control;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Sylphid Wing</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can only control 1 \"ZW - Sylphid Wing\". Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12458</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>WIND</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Tornado Bringer</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Utopia\" monster you control;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Aqua</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>EARTH</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Ultimate Shield</td>\n",
       "      <td>-1</td>\n",
       "      <td>When this card is Normal or Special Summoned: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12460</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Unicorn Spear</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Number C39: Utopia Ray\" you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12461 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level  A1        race  A3            type  A5 attribute  A7  \\\n",
       "0          0  -1  Continuous  -1      Spell Card  -1         0  -1   \n",
       "1          0  -1  Continuous  -1      Spell Card  -1         0  -1   \n",
       "2          0  -1  Quick-Play  -1      Spell Card  -1         0  -1   \n",
       "3          0  -1  Quick-Play  -1      Spell Card  -1         0  -1   \n",
       "4          0  -1       Equip  -1      Spell Card  -1         0  -1   \n",
       "...      ...  ..         ...  ..             ...  ..       ...  ..   \n",
       "12456      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "12457      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "12458      5  -1      Dragon  -1  Effect Monster  -1      WIND  -1   \n",
       "12459      4  -1        Aqua  -1  Effect Monster  -1     EARTH  -1   \n",
       "12460      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "\n",
       "                                name  A9  \\\n",
       "0           \"A\" Cell Breeding Device  -1   \n",
       "1                 \"A\" Cell Incubator  -1   \n",
       "2      \"A\" Cell Recombination Device  -1   \n",
       "3             \"A\" Cell Scatter Burst  -1   \n",
       "4         \"Infernoble Arms - Almace\"  -1   \n",
       "...                              ...  ..   \n",
       "12456             ZW - Sleipnir Mail  -1   \n",
       "12457              ZW - Sylphid Wing  -1   \n",
       "12458           ZW - Tornado Bringer  -1   \n",
       "12459           ZW - Ultimate Shield  -1   \n",
       "12460             ZW - Unicorn Spear  -1   \n",
       "\n",
       "                                                    desc  \n",
       "0      During each of your Standby Phases, put 1 A-Co...  \n",
       "1      Each time an A-Counter(s) is removed from play...  \n",
       "2      Target 1 face-up monster on the field; send 1 ...  \n",
       "3      Select 1 face-up \"Alien\" monster you control. ...  \n",
       "4      While this card is equipped to a monster: You ...  \n",
       "...                                                  ...  \n",
       "12456  You can target 1 \"Utopia\" monster you control;...  \n",
       "12457  You can only control 1 \"ZW - Sylphid Wing\". Yo...  \n",
       "12458  You can target 1 \"Utopia\" monster you control;...  \n",
       "12459  When this card is Normal or Special Summoned: ...  \n",
       "12460  You can target 1 \"Number C39: Utopia Ray\" you ...  \n",
       "\n",
       "[12461 rows x 11 columns]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sliced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf6d568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knightmare Phoenix', 'Ash Blossom & Joyous Spring', 'Dimension Shifter', 'Ash Blossom & Joyous Spring', 'Knightmare Cerberus']\n",
      "['Nibiru, the Primal Being', 'Crossout Designator', 'Ash Blossom & Joyous Spring', 'Ghost Belle & Haunted Mansion', 'Ghost Belle & Haunted Mansion']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 468...</td>\n",
       "      <td>[97.0, -1.0, 866.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[61, -1, 146, 0, -1, 170, 8, 0, 0, -1, 232, -1...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[97, -1, 301, 0, -1, 21, 8, 0, 0, -1, 138, -1,...</td>\n",
       "      <td>[276.0, -1.0, 146.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[114, -1, 246, 0, -1, 21, 8, 0, 0, -1, 198, -1...</td>\n",
       "      <td>[114.0, -1.0, 286.0, 146.0, -1.0, 21.0, 8.0, 0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[114, -1, 329, 0, -1, 21, 8, 0, 0, -1, 198, -1...</td>\n",
       "      <td>[276.0, -1.0, 99.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>[0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 96...</td>\n",
       "      <td>[276.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>[61, -1, 253, 0, -1, 93, 170, 8, 0, -1, 198, -...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>[0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 199...</td>\n",
       "      <td>[97.0, -1.0, 246.0, 0.0, -1.0, 170.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>[334, -1, 274, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[276.0, -1.0, 99.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>[0, -1, 374, 0, -1, 120, 8, 0, 0, -1, 138, -1,...</td>\n",
       "      <td>[97.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1588 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     [0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 468...   \n",
       "1     [61, -1, 146, 0, -1, 170, 8, 0, 0, -1, 232, -1...   \n",
       "2     [97, -1, 301, 0, -1, 21, 8, 0, 0, -1, 138, -1,...   \n",
       "3     [114, -1, 246, 0, -1, 21, 8, 0, 0, -1, 198, -1...   \n",
       "4     [114, -1, 329, 0, -1, 21, 8, 0, 0, -1, 198, -1...   \n",
       "...                                                 ...   \n",
       "1583  [0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 96...   \n",
       "1584  [61, -1, 253, 0, -1, 93, 170, 8, 0, -1, 198, -...   \n",
       "1585  [0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 199...   \n",
       "1586  [334, -1, 274, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "1587  [0, -1, 374, 0, -1, 120, 8, 0, 0, -1, 138, -1,...   \n",
       "\n",
       "                                                      1    2  \n",
       "0     [97.0, -1.0, 866.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [0]  \n",
       "1     [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [1]  \n",
       "2     [276.0, -1.0, 146.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [0]  \n",
       "3     [114.0, -1.0, 286.0, 146.0, -1.0, 21.0, 8.0, 0...  [0]  \n",
       "4     [276.0, -1.0, 99.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [0]  \n",
       "...                                                 ...  ...  \n",
       "1583  [276.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [0]  \n",
       "1584  [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [0]  \n",
       "1585  [97.0, -1.0, 246.0, 0.0, -1.0, 170.0, 8.0, 0.0...  [0]  \n",
       "1586  [276.0, -1.0, 99.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [0]  \n",
       "1587  [97.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [1]  \n",
       "\n",
       "[1588 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Deck_Loader(directory):\n",
    "    '''Loads Decks from Deck_Lists.txt as arrays and stores those arrays in altered'''\n",
    "    file = open(directory , 'r')\n",
    "    read = file.readlines()\n",
    "    Deck_Array = []\n",
    "    flag = False\n",
    "\n",
    "    temp=[]\n",
    "\n",
    "    for count,line in enumerate(read):\n",
    "        \n",
    "        if '//' in read[count]:\n",
    "            flag = not flag\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            read[count] = read[count].replace('\\n','')\n",
    "            \n",
    "            if ('=='  in read[count]) or ('//'  in read[count])  :\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                for i in range(int(read[count][0])):\n",
    "                    temp.append(read[count][1:].strip())          #skip appending also remove white space\n",
    "\n",
    "        if (not flag) or (count == len(read) - 1):\n",
    "            Deck_Array.append(temp)\n",
    "            temp = []\n",
    "            flag = not flag\n",
    "            \n",
    "            \n",
    "    file.close()\n",
    "    return Deck_Array \n",
    "\n",
    "\n",
    "def stitcher(Deck_Index , Deck_Array , Random_Flag , Input_Array):\n",
    "    '''Picks 5 random cards from a certain deck in a deck array and stitches them together. Or just stitches cards from input array together.'''\n",
    "    if Random_Flag:\n",
    "        decider = [random.choice(Deck_Array[Deck_Index]) for _ in range(5)]\n",
    "    else:\n",
    "        decider = Input_Array\n",
    "\n",
    "    ###If decider picks all staples then error can occur with an empty array hence recalls stitcher function.    \n",
    "    try:\n",
    "        output = np.concatenate(([Tokenized_sequence_database[i] for i in Sliced_df[Sliced_df['name'].isin(decider)].index.values]) )\n",
    "    except:\n",
    "        print(decider)\n",
    "        return stitcher(Deck_Index , Deck_Array , Random_Flag , Input_Array)\n",
    "\n",
    "    if len(output) != 905:\n",
    "        return stitcher(Deck_Index , Deck_Array , Random_Flag , Input_Array)\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "def no_match_generator(length , Deck_Array):\n",
    "    '''Generated Datapoints which correspond to a card that doesnt relate to a given group of decider cards'''\n",
    "    out = []\n",
    "    for _ in range(length):\n",
    "        ran_deck = random.choice(Deck_Array)\n",
    "        ran_indx = random.choice(Sliced_df[Sliced_df['name'].isin(ran_deck)].index.values)\n",
    "\n",
    "        temp = []\n",
    "        #Incase card chosen is a staple, it skips it.\n",
    "        try:\n",
    "            subject_card = np.concatenate( ( Tokenized_sequence_database[ran_indx]  , np.zeros(724)) , axis=None )\n",
    "            decider_cards = np.concatenate([random.choice(Tokenized_sequence_database) for _ in range(5)])\n",
    "\n",
    "            temp.append(decider_cards)\n",
    "            temp.append(subject_card)\n",
    "            temp.append([0])\n",
    "\n",
    "            out.append(temp)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return out\n",
    "\n",
    "def Dataset_Builder(directory , include_no_match):\n",
    "    '''Builds a dataset with some specificaiton. This could be a training/validation dataset or a experimentation dataset'''\n",
    "    altered = Deck_Loader(directory)\n",
    "    Built_Dataset = []\n",
    "    for deck_index,deck in enumerate(altered):\n",
    "        \n",
    "        for card_index in Sliced_df[Sliced_df['name'].isin(deck)].index.values:\n",
    "            \n",
    "            subject_card =  np.concatenate( (Tokenized_sequence_database[card_index] , np.zeros(724)) , axis=None ) # Extends subject to equal length of decider cards\n",
    "            for _ in range(2):\n",
    "                temp = []\n",
    "                decider_cards = stitcher(deck_index , altered , True, [])\n",
    "                temp.append(decider_cards)\n",
    "                temp.append(subject_card)\n",
    "                temp.append([1])\n",
    "                Built_Dataset.append(temp)\n",
    "\n",
    "\n",
    "    if (include_no_match):\n",
    "        Built_Dataset.extend( no_match_generator(len(Built_Dataset) , altered) )\n",
    "    random.shuffle(Built_Dataset)\n",
    "    random.shuffle(Built_Dataset)\n",
    "    random.shuffle(Built_Dataset)\n",
    "    return Built_Dataset\n",
    "\n",
    "\n",
    "\n",
    "Training_Validation_Dataset = Dataset_Builder('Dataset/Deck_Lists.txt' , True)\n",
    "pd.DataFrame(Training_Validation_Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d58ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[114, -1, 363, 0, -1, 75, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[61.0, -1.0, 387.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 11...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 111.0, 5.0, 0.0, ...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[61, -1, 363, 0, -1, 21, 8, 0, 0, -1, 138, -1,...</td>\n",
       "      <td>[0.0, -1.0, 41.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, -1, 711, 0, -1, 80, 5, 0, 0, -1, 0, -1, 14...</td>\n",
       "      <td>[276.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 41...</td>\n",
       "      <td>[276.0, -1.0, 374.0, 0.0, -1.0, 93.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>[114, -1, 371, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 111.0, 5.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>[0, -1, 327, 0, -1, 80, 5, 0, 0, -1, 0, -1, 18...</td>\n",
       "      <td>[114.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>[0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 11...</td>\n",
       "      <td>[97.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>[334, -1, 246, 0, -1, 21, 8, 0, 0, -1, 198, -1...</td>\n",
       "      <td>[61.0, -1.0, 387.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>[0, -1, 711, 0, -1, 80, 5, 0, 0, -1, 0, -1, 19...</td>\n",
       "      <td>[0.0, -1.0, 711.0, 0.0, -1.0, 80.0, 5.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    [114, -1, 363, 0, -1, 75, 8, 0, 0, -1, 138, -1...   \n",
       "1    [0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 11...   \n",
       "2    [61, -1, 363, 0, -1, 21, 8, 0, 0, -1, 138, -1,...   \n",
       "3    [0, -1, 711, 0, -1, 80, 5, 0, 0, -1, 0, -1, 14...   \n",
       "4    [0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 41...   \n",
       "..                                                 ...   \n",
       "499  [114, -1, 371, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "500  [0, -1, 327, 0, -1, 80, 5, 0, 0, -1, 0, -1, 18...   \n",
       "501  [0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 11...   \n",
       "502  [334, -1, 246, 0, -1, 21, 8, 0, 0, -1, 198, -1...   \n",
       "503  [0, -1, 711, 0, -1, 80, 5, 0, 0, -1, 0, -1, 19...   \n",
       "\n",
       "                                                     1    2  \n",
       "0    [61.0, -1.0, 387.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [1]  \n",
       "1    [0.0, -1.0, 59.0, 0.0, -1.0, 111.0, 5.0, 0.0, ...  [0]  \n",
       "2    [0.0, -1.0, 41.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [1]  \n",
       "3    [276.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [0]  \n",
       "4    [276.0, -1.0, 374.0, 0.0, -1.0, 93.0, 8.0, 0.0...  [0]  \n",
       "..                                                 ...  ...  \n",
       "499  [0.0, -1.0, 59.0, 0.0, -1.0, 111.0, 5.0, 0.0, ...  [1]  \n",
       "500  [114.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [0]  \n",
       "501  [97.0, -1.0, 371.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [1]  \n",
       "502  [61.0, -1.0, 387.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [0]  \n",
       "503  [0.0, -1.0, 711.0, 0.0, -1.0, 80.0, 5.0, 0.0, ...  [1]  \n",
       "\n",
       "[504 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a experimentation decklist with same format and push it to builder to build out a dataset. Put sequences into the sequence to text converters to see relation table with names.\n",
    "# See how accurate model is at predicting card relations withing a deck -> It should predict most cards as 1.\n",
    "\n",
    "# Create a function which allows you to type 5 cards and the create a dataset with every other card in the game as a subject and see what cards the model predicts will go well with your chosen cards.\n",
    "\n",
    "Experimentation_Dataset = Dataset_Builder('Dataset/Experimental_Deck_Lists.txt' , True)\n",
    "pd.DataFrame(Experimentation_Dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ac5140d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  -1, 327, ...,   0,   0,   0],\n",
       "       [  0,  -1, 327, ...,   0,   0,   0],\n",
       "       [  0,  -1, 711, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [194,  -1,  99, ...,   0,   0,   0],\n",
       "       [114,  -1, 866, ...,   0,   0,   0],\n",
       "       [114,  -1, 146, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenized_sequence_database\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate( [np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "class Positional_Embedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=False) \n",
    "    self.pos_encoding = positional_encoding(length=905, depth=d_model)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    #x*= np.sqrt(self.d_model) # Scale Values by their embedding dimensionality otherwise they could get overwhelmed by positional encoder\n",
    "    #x = x + self.pos_encoding \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class DeciderSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, ffn, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ffn, activation='relu'), \n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Single Encoder Layer with DeciderMHA and Feed Forward layer\"\"\"\n",
    "  def __init__(self, d_model, ffn , dropout_rate ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.DSA = DeciderSelfAttention(num_heads=4, key_dim=100)       # Scaling Number of Heads increases parameters as this is a different implementation of mha compared to attention is all you need paper.\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    \n",
    "    x = self.DSA(x)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Full Encoder with embedding layer with dropout and encoder layers\"\"\"\n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.EL = [EncoderLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self,x):\n",
    "    print(\"Enc ran\")\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.EL[i](x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class SubjectSelfAttention(BaseAttention):\n",
    "  \n",
    "  def call(self, x):\n",
    "    print('In SSa , x has a shape of:')\n",
    "    print(tf.shape(x))\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class SubjectCrossAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x , context):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        key=context)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class ComparatorLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self , d_model, ffn , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.SSA = SubjectSelfAttention(num_heads=4, key_dim=100)      \n",
    "    self.SCA = SubjectCrossAttention(num_heads=4, key_dim=100)\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    print('In ComparatorLayer, before SSa , x has a shape of:')\n",
    "    print(tf.shape(x))\n",
    "    x = self.SSA(x)\n",
    "\n",
    "    x = self.SCA(x , context)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "class Comparator(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.CL = [ComparatorLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    print('In comparator, before passing through pos embedding x , has a shape of:')\n",
    "    print(tf.shape(x))\n",
    "    x = self.Pos_Embedding(x)\n",
    "    print('In comparator, after passing through pos embedding x , has a shape of:')\n",
    "    print(tf.shape(x))\n",
    "    x = self.dropout(x)\n",
    "    print('In comparator, after passing through dropout x , has a shape of:')\n",
    "    print(tf.shape(x))\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.CL[i](x , context)\n",
    "\n",
    "    return x\n",
    "\n",
    "class FinalFeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),          #Flattens sentances for each card comparision , into a single 1d array , so it can generate probabilities properly, instead of shoving 100 x905 matrix straight through and generating 100 probabilities for each card comparision feature embedding\n",
    "      tf.keras.layers.Dense(50, activation='relu'),\n",
    "      tf.keras.layers.Dense(25, activation='relu'),\n",
    "      tf.keras.layers.Dropout(dropout_rate),\n",
    "      tf.keras.layers.Dense(1 , activation='sigmoid')\n",
    "      \n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.seq(x) \n",
    "    return x\n",
    "  \n",
    "class FullModel(tf.keras.Model):\n",
    "   def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = Encoder(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.com = Comparator(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.FFF = FinalFeedForward(dropout_rate)\n",
    "\n",
    "   def call(self, inputs):\n",
    "     context , x = inputs\n",
    "     \n",
    "     \n",
    "     \n",
    "     context = self.enc(context)\n",
    "    \n",
    "     x = self.com(x , context)\n",
    "\n",
    "     x = self.FFF(x)\n",
    "\n",
    "     return x\n",
    "   \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0d46854f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 905), dtype=int32, numpy=\n",
       "array([[ 97,  -1, 146,   0,  -1,  59,   8,   0,   0,  -1, 279,  -1,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,  -1, 150,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0]])>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7035acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#subject_card = Positional_Embedding(12647 , 100 )(subject_card)\n",
    "\n",
    "emb = Positional_Embedding(12647 , 100)\n",
    "comp_test = Comparator(100 , 12647 , 1000 , 0.3 , 2)\n",
    "compL_test = ComparatorLayer(100 , 1000 , 0.3)\n",
    "FM_Test = FullModel(100 , 12647 , 1000 , 0.3 , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "cfba82c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 905, 100), dtype=float32, numpy=\n",
       "array([[[-0.04667787, -0.00644787,  0.00574112, ...,  0.01819483,\n",
       "          0.02517973, -0.01179622],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.00734416,  0.04132183, -0.01791579, ..., -0.03784325,\n",
       "          0.03174325, -0.04850903],\n",
       "        ...,\n",
       "        [-0.04667787, -0.00644787,  0.00574112, ...,  0.01819483,\n",
       "          0.02517973, -0.01179622],\n",
       "        [-0.04667787, -0.00644787,  0.00574112, ...,  0.01819483,\n",
       "          0.02517973, -0.01179622],\n",
       "        [-0.04667787, -0.00644787,  0.00574112, ...,  0.01819483,\n",
       "          0.02517973, -0.01179622]]], dtype=float32)>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(subject_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b86e1a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc ran\n",
      "In comparator, before passing through pos embedding x , has a shape of:\n",
      "tf.Tensor([  1 905], shape=(2,), dtype=int32)\n",
      "In comparator, after passing through pos embedding x , has a shape of:\n",
      "tf.Tensor([  1 905 100], shape=(3,), dtype=int32)\n",
      "In comparator, after passing through dropout x , has a shape of:\n",
      "tf.Tensor([  1 905 100], shape=(3,), dtype=int32)\n",
      "In ComparatorLayer, before SSa , x has a shape of:\n",
      "tf.Tensor([  1 905 100], shape=(3,), dtype=int32)\n",
      "In SSa , x has a shape of:\n",
      "tf.Tensor([  1 905 100], shape=(3,), dtype=int32)\n",
      "In ComparatorLayer, before SSa , x has a shape of:\n",
      "tf.Tensor([  1 905 100], shape=(3,), dtype=int32)\n",
      "In SSa , x has a shape of:\n",
      "tf.Tensor([  1 905 100], shape=(3,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.42610976]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ,'Galaxy Summoner']\n",
    "\n",
    "decider1 = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "subject_card1 = tf.convert_to_tensor(    [np.concatenate( (Tokenized_sequence_database[7] , np.zeros(724)) , axis=None )]     ) \n",
    "\n",
    "\n",
    "# decider2 = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "# subject_card2 = tf.convert_to_tensor(    [np.concatenate( (Tokenized_sequence_database[8434] , np.zeros(724)) , axis=None )]     ) \n",
    "# sm = 0\n",
    "# for i in (compL_test(emb(subject_card1) , emb(decider1)) - compL_test(emb(subject_card2) , emb(decider2)))[0][0]:\n",
    "#     sm+=i\n",
    "#compL_test(emb(subject_card1) , emb(decider1))\n",
    "# print(sm) \n",
    "#print(Model.predict((decider , subject_card))[0])\n",
    "#sum(abs((compL_test(emb(subject_card1) , emb(decider1)) - compL_test(emb(subject_card2) , emb(decider2)))[0][0]))\n",
    "FM_Test((decider1 , subject_card1))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "6e68f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Train_Gen():\n",
    "    seti0 = [i[0] for i in Training_Validation_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Validation_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Validation_Dataset]\n",
    "    for _ in range(2000):\n",
    "        r_int = random.randint(0 , len(Training_Validation_Dataset) - 30)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+30])\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+30])\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+30])\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def Val_Gen():\n",
    "    seti0 = [i[0] for i in Experimentation_Dataset]\n",
    "    seti1 = [i[1] for i in Experimentation_Dataset]\n",
    "    seti2 = [i[2] for i in Experimentation_Dataset]\n",
    "    for _ in range(1000):\n",
    "        r_int = random.randint(0 , len(Experimentation_Dataset) - 50)\n",
    "        val_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50])\n",
    "        val_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50])\n",
    "        val_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+50])\n",
    "\n",
    "        yield (val_seti0,val_seti1), val_seti2\n",
    "\n",
    "def Experimentation_Gen():\n",
    "    seti0 = [i[0] for i in Experimentation_Dataset]\n",
    "    seti1 = [i[1] for i in Experimentation_Dataset]\n",
    "    seti2 = [i[2] for i in Experimentation_Dataset]\n",
    "\n",
    "    for _ in range(len(Experimentation_Dataset)):\n",
    "        # For some reason i doesnt iterate and gets stuck at 0 , so had to use random int\n",
    "        r_int = random.randint(0 , 90)\n",
    "        exp_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+1])\n",
    "        exp_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+1])\n",
    "        exp_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+1])\n",
    "\n",
    "        yield (exp_seti0,exp_seti1), exp_seti2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "2603da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc ran\n",
      "Com ran\n",
      "Epoch 1/20\n",
      "Enc ran\n",
      "Com ran\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['full_model_53/comparator_68/positional__embedding_131/embedding_131/embeddings:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/query/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/query/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/key/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/key/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/value/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/value/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/attention_output/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/attention_output/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/layer_normalization_645/gamma:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/layer_normalization_645/beta:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/query/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/query/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/key/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/key/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/value/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/value/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/attention_output/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/attention_output/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/layer_normalization_646/gamma:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/layer_normalization_646/beta:0', 'dense_665/kernel:0', 'dense_665/bias:0', 'dense_666/kernel:0', 'dense_666/bias:0', 'full_model_53/comparator_68/comparator_layer_139/feed_forward_253/layer_normalization_647/gamma:0', 'full_model_53/comparator_68/comparator_layer_139/feed_forward_253/layer_normalization_647/beta:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/query/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/query/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/key/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/key/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/value/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/value/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/attention_output/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/attention_output/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/layer_normalization_648/gamma:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/layer_normalization_648/beta:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/query/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/query/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/key/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/key/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/value/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/value/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/attention_output/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/attention_output/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/layer_normalization_649/gamma:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/layer_normalization_649/beta:0', 'dense_667/kernel:0', 'dense_667/bias:0', 'dense_668/kernel:0', 'dense_668/bias:0', 'full_model_53/comparator_68/comparator_layer_140/feed_forward_254/layer_normalization_650/gamma:0', 'full_model_53/comparator_68/comparator_layer_140/feed_forward_254/layer_normalization_650/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "Enc ran\n",
      "Com ran\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['full_model_53/comparator_68/positional__embedding_131/embedding_131/embeddings:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/query/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/query/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/key/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/key/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/value/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/value/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/attention_output/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/multi_head_attention_392/attention_output/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/layer_normalization_645/gamma:0', 'full_model_53/comparator_68/comparator_layer_139/subject_self_attention_139/layer_normalization_645/beta:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/query/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/query/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/key/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/key/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/value/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/value/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/attention_output/kernel:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/multi_head_attention_393/attention_output/bias:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/layer_normalization_646/gamma:0', 'full_model_53/comparator_68/comparator_layer_139/subject_cross_attention_139/layer_normalization_646/beta:0', 'dense_665/kernel:0', 'dense_665/bias:0', 'dense_666/kernel:0', 'dense_666/bias:0', 'full_model_53/comparator_68/comparator_layer_139/feed_forward_253/layer_normalization_647/gamma:0', 'full_model_53/comparator_68/comparator_layer_139/feed_forward_253/layer_normalization_647/beta:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/query/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/query/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/key/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/key/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/value/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/value/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/attention_output/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/multi_head_attention_394/attention_output/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/layer_normalization_648/gamma:0', 'full_model_53/comparator_68/comparator_layer_140/subject_self_attention_140/layer_normalization_648/beta:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/query/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/query/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/key/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/key/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/value/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/value/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/attention_output/kernel:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/multi_head_attention_395/attention_output/bias:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/layer_normalization_649/gamma:0', 'full_model_53/comparator_68/comparator_layer_140/subject_cross_attention_140/layer_normalization_649/beta:0', 'dense_667/kernel:0', 'dense_667/bias:0', 'dense_668/kernel:0', 'dense_668/bias:0', 'full_model_53/comparator_68/comparator_layer_140/feed_forward_254/layer_normalization_650/gamma:0', 'full_model_53/comparator_68/comparator_layer_140/feed_forward_254/layer_normalization_650/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9650 - accuracy: 0.5167Enc ran\n",
      "Com ran\n",
      "10/10 [==============================] - 7s 416ms/step - loss: 0.9650 - accuracy: 0.5167 - val_loss: 0.8824 - val_accuracy: 0.4860\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 3s 332ms/step - loss: 0.8442 - accuracy: 0.4667 - val_loss: 0.7069 - val_accuracy: 0.5140\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 3s 314ms/step - loss: 0.7204 - accuracy: 0.5000 - val_loss: 0.7056 - val_accuracy: 0.5080\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 3s 314ms/step - loss: 0.7136 - accuracy: 0.4900 - val_loss: 0.6966 - val_accuracy: 0.5100\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 3s 313ms/step - loss: 0.6882 - accuracy: 0.5700 - val_loss: 0.7652 - val_accuracy: 0.4900\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 3s 314ms/step - loss: 0.7058 - accuracy: 0.5467 - val_loss: 0.6998 - val_accuracy: 0.5220\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 3s 315ms/step - loss: 0.6891 - accuracy: 0.5400 - val_loss: 0.7046 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 3s 315ms/step - loss: 0.6709 - accuracy: 0.5567 - val_loss: 0.7112 - val_accuracy: 0.5680\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 3s 317ms/step - loss: 0.6982 - accuracy: 0.5533 - val_loss: 0.7724 - val_accuracy: 0.4940\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 3s 321ms/step - loss: 0.6792 - accuracy: 0.5667 - val_loss: 0.7754 - val_accuracy: 0.4700\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 3s 330ms/step - loss: 0.6393 - accuracy: 0.6300 - val_loss: 0.8293 - val_accuracy: 0.5060\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 3s 338ms/step - loss: 0.6002 - accuracy: 0.7000 - val_loss: 1.0219 - val_accuracy: 0.4800\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 3s 347ms/step - loss: 0.6684 - accuracy: 0.6200 - val_loss: 0.8307 - val_accuracy: 0.5700\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 3s 347ms/step - loss: 0.6004 - accuracy: 0.6967 - val_loss: 0.7782 - val_accuracy: 0.5720\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 3s 347ms/step - loss: 0.5767 - accuracy: 0.6967 - val_loss: 0.7615 - val_accuracy: 0.5780\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 3s 348ms/step - loss: 0.5701 - accuracy: 0.6967 - val_loss: 0.8418 - val_accuracy: 0.5120\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 3s 348ms/step - loss: 0.5665 - accuracy: 0.7300 - val_loss: 1.0511 - val_accuracy: 0.5520\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 3s 349ms/step - loss: 0.5512 - accuracy: 0.7267 - val_loss: 1.2567 - val_accuracy: 0.5040\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 3s 347ms/step - loss: 0.5508 - accuracy: 0.7367 - val_loss: 1.4477 - val_accuracy: 0.5280\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 3s 348ms/step - loss: 0.5705 - accuracy: 0.7100 - val_loss: 1.1067 - val_accuracy: 0.4380\n"
     ]
    }
   ],
   "source": [
    "#Testing accuracy is much higher than training accuracy. This is due to dropouts causing lower accuracy during training but giving a more robust model when testing\n",
    "Model = FullModel(100 , 12647 , 1000 , 0.3 , 2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model = 100)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "Model.compile(\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=optimizer,\n",
    "    metrics= 'accuracy' )\n",
    "\n",
    "history = Model.fit(Train_Gen() , epochs=20, \n",
    "                               validation_data = Val_Gen()  , steps_per_epoch=10 , batch_size=30 , validation_steps=10 , validation_batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "c6848a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "Enc ran\n",
      "Com ran\n",
      "1/1 [==============================] - 1s 837ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[0.6818564]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[0.6818564]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [384], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[38;5;66;03m# if Model.predict((decider , subject_card) , verbose=False)[0][0] > 0.1:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;66;03m#     print(Sliced_df[indx:indx+1]['name'])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mpredict_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [384], line 17\u001b[0m, in \u001b[0;36mpredict_matches\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#subject_card = tf.convert_to_tensor([np.zeros(905) ])\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#print(subject_card)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Model\u001b[38;5;241m.\u001b[39mpredict((decider , subject_card))[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# if Model.predict((decider , subject_card) , verbose=False)[0][0] > 0.1:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#     print(Sliced_df[indx:indx+1]['name'])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     22\u001b[0m count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def predict_matches():\n",
    "    #input_array = [input('enter') for _ in range(5)]\n",
    "    input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ,'Galaxy Summoner']\n",
    "    \n",
    "    decider = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "    count = 0\n",
    "    for indx in pd.DataFrame(Tokenized_sequence_database).index.values:\n",
    "        if count<2500:\n",
    "            pass\n",
    "        else:\n",
    "            if (count % 100) == 0:\n",
    "                print(count)\n",
    "            subject_card = tf.convert_to_tensor([np.concatenate( (Tokenized_sequence_database[indx] , np.zeros(724)) , axis=None )])\n",
    "            #subject_card = tf.convert_to_tensor([np.zeros(905) ])\n",
    "            #print(subject_card)\n",
    "            print(Model.predict((decider , subject_card))[0])\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # if Model.predict((decider , subject_card) , verbose=False)[0][0] > 0.1:\n",
    "            #     print(Sliced_df[indx:indx+1]['name'])\n",
    "            #     break\n",
    "        count+=1\n",
    "predict_matches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1cb6a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 8s 12ms/step\n",
      "0.16468253968253968\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame(Model.predict(Experimentation_Gen()))\n",
    "count = 0\n",
    "for i in pred[0]:\n",
    "    if i > 0.5:\n",
    "        count+=1\n",
    "print(count/len(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 219ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9966325]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k , trash = next(Experimentation_Gen())\n",
    "l1 , l2 = k\n",
    "singular_pred = Model.predict(k)\n",
    "singular_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9e8875d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"2 aqua effect monster water cannot be used as a synchro material. this card's name becomes des frog while it is on the field. if this card is in your graveyard: you can banish 1 frog monster from your graveyard; special summon this card.\"]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.sequences_to_texts([l2[0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d9ca161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200 % 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638286c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67126460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25bc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd683030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
