{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc825eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase Of Update\n",
    "response = requests.get('https://db.ygoprodeck.com/api/v7/cardinfo.php')\n",
    "json_response = response.json()\n",
    "dataset = pd.DataFrame(json_response['data'])\n",
    "\n",
    "dataset.to_csv('Dataset/Yugioh_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cdc1ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import random as random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "plt.style.use('Solarize_Light2')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "#import requests\n",
    "#import itertools\n",
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ef56086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "'''Load Dataset'''\n",
    "dataset = pd.read_csv('Dataset/Yugioh_Database.csv')\n",
    "dataset = dataset.drop(['Unnamed: 0' , 'frameType' , 'archetype' , 'ygoprodeck_url' , 'card_sets' , 'card_images' , 'card_prices' , 'banlist_info'],axis=1)\n",
    "dataset = dataset[dataset['type'] != 'XYZ Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Normal Monster']\n",
    "\n",
    "dataset = dataset[dataset['type'] != 'Skill Card']\n",
    "dataset = dataset[dataset['type'] != 'Monster Token']\n",
    "# Staple Removal\n",
    "dataset = dataset[[not i for i in dataset['name'].isin(['Ash Blossom & Joyous Spring' , 'Effect Veiler' , 'Ghost Ogre & Snow Rabbit' ,'Ghost Belle & Haunted Mansion',\n",
    "                                                        'Infinite Impermanence' , 'Red Reboot' , 'Called by the Grave' , 'Forbidden Droplet' , 'Crossout Designator',\n",
    "                                                        'Nibiru, the Primal Being', 'Harpie\\\"s Feather Duster' , 'Lightning Storm' , 'Pot of Prosperity' , 'Pot of Desires',\n",
    "                                                        'Pot of Duality' , 'Pot of Extravagance' , 'Triple Tactics Talents' , 'Torrential Tribute' , 'Dark Ruler No More' , \n",
    "                                                        'Red Reboot', 'D.D. Crow' , 'PSY-Framegear Gamma' , 'Maxx \\\"C\\\"' , 'Dimension Shifter' , 'Droll & Lock Bird' , \n",
    "                                                        'Accesscode Talker', 'Apollousa, Bow of the Goddess', 'Borreload Dragon' , 'Borrelsword Dragon', 'Knightmare Unicorn',\n",
    "                                                        'Predaplant Verte Anaconda' , 'Knightmare Phoenix' , 'Knightmare Cerberus' , 'Underworld Goddess of the Closed World',\n",
    "                                                        'Borreload Savage Dragon'])]]\n",
    "\n",
    "dataset.loc[dataset['type']=='Normal Monster', ['desc']] = 'NoInfo'\n",
    "dataset = dataset.fillna('0')\n",
    "dataset['level'] = dataset['level'].astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "'''Create Tokenized sequence database'''\n",
    "\n",
    "\n",
    "df = dataset['desc']         #Tokenizer is only trained on desc and based on that . Otherwise if trained on names it would blow vocab up to absurd amounts\n",
    "Sliced_df = dataset[['level' , 'race' , 'type' , 'attribute' , 'name' , 'desc']]\n",
    "\n",
    "for i in range(1,11,2):\n",
    "    Sliced_df.insert(loc=i, column='A'+str(i), value=-1)        # Adds seperator columns\n",
    "\n",
    "\n",
    "Sliced_df = Sliced_df.reset_index(drop=True)            # Need to reset the indexes so they are consistent\n",
    "df = df.reset_index(drop=True)                              \n",
    "\n",
    "tokenizer = Tokenizer(filters='\\r , \\n , \\\" ') # Speech marks stop names from being recognised by tokenizer\n",
    "tokenizer.fit_on_texts(df)\n",
    "tokenizer.word_index['0'] = 0           #Signifies Empty values\n",
    "tokenizer.word_index['-1'] = -1           #Signifies Seperators\n",
    "\n",
    "sequences = []\n",
    "padded_sequences = []\n",
    "Tokenized_sequence_database = []\n",
    "count = 0\n",
    "for i in Sliced_df.astype('string').to_numpy():\n",
    "    \n",
    "    sequences.append(tokenizer.texts_to_sequences(i))\n",
    "    \n",
    "\n",
    "for i in range(0,11):\n",
    "    padded_sequences.append( pad_sequences(np.array(sequences , dtype='object')[:,i], padding='post') ) \n",
    "\n",
    "Tokenized_sequence_database = np.concatenate(([padded_sequences[i] for i in range(11)]) , axis=1 )\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "382c99c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>A1</th>\n",
       "      <th>race</th>\n",
       "      <th>A3</th>\n",
       "      <th>type</th>\n",
       "      <th>A5</th>\n",
       "      <th>attribute</th>\n",
       "      <th>A7</th>\n",
       "      <th>name</th>\n",
       "      <th>A9</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Breeding Device</td>\n",
       "      <td>-1</td>\n",
       "      <td>During each of your Standby Phases, put 1 A-Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Incubator</td>\n",
       "      <td>-1</td>\n",
       "      <td>Each time an A-Counter(s) is removed from play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Quick-Play</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Recombination Device</td>\n",
       "      <td>-1</td>\n",
       "      <td>Target 1 face-up monster on the field; send 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Quick-Play</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"A\" Cell Scatter Burst</td>\n",
       "      <td>-1</td>\n",
       "      <td>Select 1 face-up \"Alien\" monster you control. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Equip</td>\n",
       "      <td>-1</td>\n",
       "      <td>Spell Card</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"Infernoble Arms - Almace\"</td>\n",
       "      <td>-1</td>\n",
       "      <td>While this card is equipped to a monster: You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Sleipnir Mail</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Utopia\" monster you control;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Sylphid Wing</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can only control 1 \"ZW - Sylphid Wing\". Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12458</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>WIND</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Tornado Bringer</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Utopia\" monster you control;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Aqua</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>EARTH</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Ultimate Shield</td>\n",
       "      <td>-1</td>\n",
       "      <td>When this card is Normal or Special Summoned: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12460</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Beast</td>\n",
       "      <td>-1</td>\n",
       "      <td>Effect Monster</td>\n",
       "      <td>-1</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>-1</td>\n",
       "      <td>ZW - Unicorn Spear</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can target 1 \"Number C39: Utopia Ray\" you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12461 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level  A1        race  A3            type  A5 attribute  A7  \\\n",
       "0          0  -1  Continuous  -1      Spell Card  -1         0  -1   \n",
       "1          0  -1  Continuous  -1      Spell Card  -1         0  -1   \n",
       "2          0  -1  Quick-Play  -1      Spell Card  -1         0  -1   \n",
       "3          0  -1  Quick-Play  -1      Spell Card  -1         0  -1   \n",
       "4          0  -1       Equip  -1      Spell Card  -1         0  -1   \n",
       "...      ...  ..         ...  ..             ...  ..       ...  ..   \n",
       "12456      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "12457      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "12458      5  -1      Dragon  -1  Effect Monster  -1      WIND  -1   \n",
       "12459      4  -1        Aqua  -1  Effect Monster  -1     EARTH  -1   \n",
       "12460      4  -1       Beast  -1  Effect Monster  -1     LIGHT  -1   \n",
       "\n",
       "                                name  A9  \\\n",
       "0           \"A\" Cell Breeding Device  -1   \n",
       "1                 \"A\" Cell Incubator  -1   \n",
       "2      \"A\" Cell Recombination Device  -1   \n",
       "3             \"A\" Cell Scatter Burst  -1   \n",
       "4         \"Infernoble Arms - Almace\"  -1   \n",
       "...                              ...  ..   \n",
       "12456             ZW - Sleipnir Mail  -1   \n",
       "12457              ZW - Sylphid Wing  -1   \n",
       "12458           ZW - Tornado Bringer  -1   \n",
       "12459           ZW - Ultimate Shield  -1   \n",
       "12460             ZW - Unicorn Spear  -1   \n",
       "\n",
       "                                                    desc  \n",
       "0      During each of your Standby Phases, put 1 A-Co...  \n",
       "1      Each time an A-Counter(s) is removed from play...  \n",
       "2      Target 1 face-up monster on the field; send 1 ...  \n",
       "3      Select 1 face-up \"Alien\" monster you control. ...  \n",
       "4      While this card is equipped to a monster: You ...  \n",
       "...                                                  ...  \n",
       "12456  You can target 1 \"Utopia\" monster you control;...  \n",
       "12457  You can only control 1 \"ZW - Sylphid Wing\". Yo...  \n",
       "12458  You can target 1 \"Utopia\" monster you control;...  \n",
       "12459  When this card is Normal or Special Summoned: ...  \n",
       "12460  You can target 1 \"Number C39: Utopia Ray\" you ...  \n",
       "\n",
       "[12461 rows x 11 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sliced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8bf6d568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pot of Duality', 'Underworld Goddess of the Closed World', 'Accesscode Talker', 'Crossout Designator', 'Ash Blossom & Joyous Spring']\n",
      "['Ash Blossom & Joyous Spring', 'Dimension Shifter', 'Called by the Grave', 'Pot of Duality', 'Ash Blossom & Joyous Spring']\n",
      "['Crossout Designator', 'Ash Blossom & Joyous Spring', 'Knightmare Unicorn', 'Called by the Grave', 'Crossout Designator']\n",
      "['Nibiru, the Primal Being', 'Knightmare Unicorn', 'Called by the Grave', 'Ghost Ogre & Snow Rabbit', 'D.D. Crow']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, -1, 112, 0, -1, 80, 5, 0, 0, -1, 0, -1, 37...</td>\n",
       "      <td>[276.0, -1.0, 99.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[114, -1, 411, 0, -1, 21, 8, 0, 0, -1, 202, -1...</td>\n",
       "      <td>[276.0, -1.0, 286.0, 146.0, -1.0, 21.0, 8.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, -1, 711, 0, -1, 80, 5, 0, 0, -1, 0, -1, 45...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[334, -1, 371, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[114.0, -1.0, 146.0, 0.0, -1.0, 93.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, -1, 327, 0, -1, 80, 5, 0, 0, -1, 0, -1, 29...</td>\n",
       "      <td>[97.0, -1.0, 866.0, 0.0, -1.0, 170.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>[114, -1, 286, 146, -1, 109, 21, 8, 0, -1, 297...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 111.0, 5.0, 0.0, ...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>[276, -1, 436, 618, -1, 21, 8, 0, 0, -1, 138, ...</td>\n",
       "      <td>[1246.0, -1.0, 253.0, 0.0, -1.0, 75.0, 8.0, 0....</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>[0, -1, 246, 0, -1, 75, 8, 0, 0, -1, 198, -1, ...</td>\n",
       "      <td>[0.0, -1.0, 374.0, 0.0, -1.0, 120.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>[0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 207...</td>\n",
       "      <td>[114.0, -1.0, 274.0, 0.0, -1.0, 75.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>[114, -1, 363, 0, -1, 21, 8, 0, 0, -1, 232, -1...</td>\n",
       "      <td>[97.0, -1.0, 146.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1588 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     [0, -1, 112, 0, -1, 80, 5, 0, 0, -1, 0, -1, 37...   \n",
       "1     [114, -1, 411, 0, -1, 21, 8, 0, 0, -1, 202, -1...   \n",
       "2     [0, -1, 711, 0, -1, 80, 5, 0, 0, -1, 0, -1, 45...   \n",
       "3     [334, -1, 371, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "4     [0, -1, 327, 0, -1, 80, 5, 0, 0, -1, 0, -1, 29...   \n",
       "...                                                 ...   \n",
       "1583  [114, -1, 286, 146, -1, 109, 21, 8, 0, -1, 297...   \n",
       "1584  [276, -1, 436, 618, -1, 21, 8, 0, 0, -1, 138, ...   \n",
       "1585  [0, -1, 246, 0, -1, 75, 8, 0, 0, -1, 198, -1, ...   \n",
       "1586  [0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 207...   \n",
       "1587  [114, -1, 363, 0, -1, 21, 8, 0, 0, -1, 232, -1...   \n",
       "\n",
       "                                                      1    2  \n",
       "0     [276.0, -1.0, 99.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [0]  \n",
       "1     [276.0, -1.0, 286.0, 146.0, -1.0, 21.0, 8.0, 0...  [1]  \n",
       "2     [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [1]  \n",
       "3     [114.0, -1.0, 146.0, 0.0, -1.0, 93.0, 8.0, 0.0...  [0]  \n",
       "4     [97.0, -1.0, 866.0, 0.0, -1.0, 170.0, 8.0, 0.0...  [1]  \n",
       "...                                                 ...  ...  \n",
       "1583  [0.0, -1.0, 59.0, 0.0, -1.0, 111.0, 5.0, 0.0, ...  [0]  \n",
       "1584  [1246.0, -1.0, 253.0, 0.0, -1.0, 75.0, 8.0, 0....  [1]  \n",
       "1585  [0.0, -1.0, 374.0, 0.0, -1.0, 120.0, 8.0, 0.0,...  [1]  \n",
       "1586  [114.0, -1.0, 274.0, 0.0, -1.0, 75.0, 8.0, 0.0...  [0]  \n",
       "1587  [97.0, -1.0, 146.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [0]  \n",
       "\n",
       "[1588 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Deck_Loader(directory):\n",
    "    '''Loads Decks from Deck_Lists.txt as arrays and stores those arrays in altered'''\n",
    "    file = open(directory , 'r')\n",
    "    read = file.readlines()\n",
    "    Deck_Array = []\n",
    "    flag = False\n",
    "\n",
    "    temp=[]\n",
    "\n",
    "    for count,line in enumerate(read):\n",
    "        \n",
    "        if '//' in read[count]:\n",
    "            flag = not flag\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            read[count] = read[count].replace('\\n','')\n",
    "            \n",
    "            if ('=='  in read[count]) or ('//'  in read[count])  :\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                for i in range(int(read[count][0])):\n",
    "                    temp.append(read[count][1:].strip())          #skip appending also remove white space\n",
    "\n",
    "        if (not flag) or (count == len(read) - 1):\n",
    "            Deck_Array.append(temp)\n",
    "            temp = []\n",
    "            flag = not flag\n",
    "            \n",
    "            \n",
    "    file.close()\n",
    "    return Deck_Array \n",
    "\n",
    "\n",
    "def stitcher(Deck_Index , Deck_Array , Random_Flag , Input_Array):\n",
    "    '''Picks 5 random cards from a certain deck in a deck array and stitches them together. Or just stitches cards from input array together.'''\n",
    "    if Random_Flag:\n",
    "        decider = [random.choice(Deck_Array[Deck_Index]) for _ in range(5)]\n",
    "    else:\n",
    "        decider = Input_Array\n",
    "\n",
    "    ###If decider picks all staples then error can occur with an empty array hence recalls stitcher function.    \n",
    "    try:\n",
    "        output = np.concatenate(([Tokenized_sequence_database[i] for i in Sliced_df[Sliced_df['name'].isin(decider)].index.values]) )\n",
    "    except:\n",
    "        print(decider)\n",
    "        return stitcher(Deck_Index , Deck_Array , Random_Flag , Input_Array)\n",
    "\n",
    "    if len(output) != 905:\n",
    "        return stitcher(Deck_Index , Deck_Array , Random_Flag , Input_Array)\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "def no_match_generator(length , Deck_Array):\n",
    "    '''Generated Datapoints which correspond to a card that doesnt relate to a given group of decider cards'''\n",
    "    out = []\n",
    "    for _ in range(length):\n",
    "        ran_deck = random.choice(Deck_Array)\n",
    "        ran_indx = random.choice(Sliced_df[Sliced_df['name'].isin(ran_deck)].index.values)\n",
    "\n",
    "        temp = []\n",
    "        #Incase card chosen is a staple, it skips it.\n",
    "        try:\n",
    "            subject_card = np.concatenate( ( Tokenized_sequence_database[ran_indx]  , np.zeros(724)) , axis=None )\n",
    "            decider_cards = np.concatenate([random.choice(Tokenized_sequence_database) for _ in range(5)])\n",
    "\n",
    "            temp.append(decider_cards)\n",
    "            temp.append(subject_card)\n",
    "            temp.append([0])\n",
    "\n",
    "            out.append(temp)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return out\n",
    "\n",
    "def Dataset_Builder(directory , include_no_match):\n",
    "    '''Builds a dataset with some specificaiton. This could be a training/validation dataset or a experimentation dataset'''\n",
    "    altered = Deck_Loader(directory)\n",
    "    Built_Dataset = []\n",
    "    for deck_index,deck in enumerate(altered):\n",
    "        \n",
    "        for card_index in Sliced_df[Sliced_df['name'].isin(deck)].index.values:\n",
    "            \n",
    "            subject_card =  np.concatenate( (Tokenized_sequence_database[card_index] , np.zeros(724)) , axis=None ) # Extends subject to equal length of decider cards\n",
    "            for _ in range(2):\n",
    "                temp = []\n",
    "                decider_cards = stitcher(deck_index , altered , True, [])\n",
    "                temp.append(decider_cards)\n",
    "                temp.append(subject_card)\n",
    "                temp.append([1])\n",
    "                Built_Dataset.append(temp)\n",
    "\n",
    "\n",
    "    if (include_no_match):\n",
    "        Built_Dataset.extend( no_match_generator(len(Built_Dataset) , altered) )\n",
    "    random.shuffle(Built_Dataset)\n",
    "    random.shuffle(Built_Dataset)\n",
    "    random.shuffle(Built_Dataset)\n",
    "    return Built_Dataset\n",
    "\n",
    "\n",
    "\n",
    "Training_Validation_Dataset = Dataset_Builder('Dataset/Deck_Lists.txt' , True)\n",
    "pd.DataFrame(Training_Validation_Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15d58ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[114, -1, 246, 0, -1, 21, 8, 0, 0, -1, 279, -1...</td>\n",
       "      <td>[334.0, -1.0, 900.0, 0.0, -1.0, 59.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[272, -1, 363, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[114.0, -1.0, 363.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[7, -1, 253, 0, -1, 21, 8, 0, 0, -1, 198, -1, ...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[114, -1, 371, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[334.0, -1.0, 900.0, 0.0, -1.0, 59.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 94...</td>\n",
       "      <td>[334.0, -1.0, 301.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>[0, -1, 253, 0, -1, 120, 8, 0, 0, -1, 232, -1,...</td>\n",
       "      <td>[61.0, -1.0, 387.0, 0.0, -1.0, 75.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>[0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 11...</td>\n",
       "      <td>[465.0, -1.0, 371.0, 0.0, -1.0, 93.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>[0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 13...</td>\n",
       "      <td>[0.0, -1.0, 274.0, 0.0, -1.0, 120.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>[0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 547...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 111.0, 5.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>[0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 41...</td>\n",
       "      <td>[276.0, -1.0, 374.0, 0.0, -1.0, 93.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    [114, -1, 246, 0, -1, 21, 8, 0, 0, -1, 279, -1...   \n",
       "1    [272, -1, 363, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "2    [7, -1, 253, 0, -1, 21, 8, 0, 0, -1, 198, -1, ...   \n",
       "3    [114, -1, 371, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "4    [0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 94...   \n",
       "..                                                 ...   \n",
       "499  [0, -1, 253, 0, -1, 120, 8, 0, 0, -1, 232, -1,...   \n",
       "500  [0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 11...   \n",
       "501  [0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 13...   \n",
       "502  [0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 547...   \n",
       "503  [0, -1, 59, 0, -1, 111, 5, 0, 0, -1, 0, -1, 41...   \n",
       "\n",
       "                                                     1    2  \n",
       "0    [334.0, -1.0, 900.0, 0.0, -1.0, 59.0, 8.0, 0.0...  [0]  \n",
       "1    [114.0, -1.0, 363.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [1]  \n",
       "2    [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [0]  \n",
       "3    [334.0, -1.0, 900.0, 0.0, -1.0, 59.0, 8.0, 0.0...  [1]  \n",
       "4    [334.0, -1.0, 301.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [0]  \n",
       "..                                                 ...  ...  \n",
       "499  [61.0, -1.0, 387.0, 0.0, -1.0, 75.0, 8.0, 0.0,...  [1]  \n",
       "500  [465.0, -1.0, 371.0, 0.0, -1.0, 93.0, 8.0, 0.0...  [1]  \n",
       "501  [0.0, -1.0, 274.0, 0.0, -1.0, 120.0, 8.0, 0.0,...  [0]  \n",
       "502  [0.0, -1.0, 59.0, 0.0, -1.0, 111.0, 5.0, 0.0, ...  [1]  \n",
       "503  [276.0, -1.0, 374.0, 0.0, -1.0, 93.0, 8.0, 0.0...  [0]  \n",
       "\n",
       "[504 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a experimentation decklist with same format and push it to builder to build out a dataset. Put sequences into the sequence to text converters to see relation table with names.\n",
    "# See how accurate model is at predicting card relations withing a deck -> It should predict most cards as 1.\n",
    "\n",
    "# Create a function which allows you to type 5 cards and the create a dataset with every other card in the game as a subject and see what cards the model predicts will go well with your chosen cards.\n",
    "\n",
    "Experimentation_Dataset = Dataset_Builder('Dataset/Experimental_Deck_Lists.txt' , True)\n",
    "pd.DataFrame(Experimentation_Dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac5140d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Validation_Dataset:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbd1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate( [np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "class Positional_Embedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=False) \n",
    "    self.pos_encoding = positional_encoding(length=905, depth=d_model)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x*= np.sqrt(self.d_model) # Scale Values by their embedding dimensionality otherwise they could get overwhelmed by positional encoder\n",
    "    x = x + self.pos_encoding \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class DeciderSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, ffn, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ffn, activation='relu'), \n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Single Encoder Layer with DeciderMHA and Feed Forward layer\"\"\"\n",
    "  def __init__(self, d_model, ffn , dropout_rate ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.DSA = DeciderSelfAttention(num_heads=4, key_dim=100)       # Scaling Number of Heads increases parameters as this is a different implementation of mha compared to attention is all you need paper.\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    \n",
    "    x = self.DSA(x)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Full Encoder with embedding layer with dropout and encoder layers\"\"\"\n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.EL = [EncoderLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.EL[i](x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class SubjectSelfAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class SubjectCrossAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x , context):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        key=context)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class ComparatorLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self , d_model, ffn , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.SSA = SubjectSelfAttention(num_heads=4, key_dim=100)      \n",
    "    self.SCA = SubjectCrossAttention(num_heads=4, key_dim=100)\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    \n",
    "    x = self.SSA(x)\n",
    "\n",
    "    x = self.SCA(x , context)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "class Comparator(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.CL = [ComparatorLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.CL[i](x , context)\n",
    "\n",
    "    return x\n",
    "\n",
    "class FinalFeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),          #Flattens sentances for each card comparision , into a single 1d array , so it can generate probabilities properly, instead of shoving 100 x905 matrix straight through and generating 100 probabilities for each card comparision feature embedding\n",
    "      tf.keras.layers.Dense(50, activation='relu'),\n",
    "      tf.keras.layers.Dense(25, activation='relu'),\n",
    "      tf.keras.layers.Dropout(dropout_rate),\n",
    "      tf.keras.layers.Dense(1 , activation='sigmoid')\n",
    "      \n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.seq(x) \n",
    "    return x\n",
    "  \n",
    "class FullModel(tf.keras.Model):\n",
    "   def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = Encoder(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.com = Comparator(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.FFF = FinalFeedForward(dropout_rate)\n",
    "\n",
    "   def call(self, inputs):\n",
    "     context , x = inputs\n",
    "     \n",
    "     \n",
    "     \n",
    "     context = self.enc(context)\n",
    "     x = self.com(x , context)\n",
    "\n",
    "     x = self.FFF(context)\n",
    "\n",
    "     return x\n",
    "   \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6e68f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Train_Gen():\n",
    "    seti0 = [i[0] for i in Training_Validation_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Validation_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Validation_Dataset]\n",
    "    for _ in range(2000):\n",
    "        r_int = random.randint(0 , len(Training_Validation_Dataset) - 30)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+30])\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+30])\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+30])\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def Val_Gen():\n",
    "    seti0 = [i[0] for i in Experimentation_Dataset]\n",
    "    seti1 = [i[1] for i in Experimentation_Dataset]\n",
    "    seti2 = [i[2] for i in Experimentation_Dataset]\n",
    "    for _ in range(1000):\n",
    "        r_int = random.randint(0 , len(Experimentation_Dataset) - 50)\n",
    "        val_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50])\n",
    "        val_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50])\n",
    "        val_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+50])\n",
    "\n",
    "        yield (val_seti0,val_seti1), val_seti2\n",
    "\n",
    "def Experimentation_Gen():\n",
    "    seti0 = [i[0] for i in Experimentation_Dataset]\n",
    "    seti1 = [i[1] for i in Experimentation_Dataset]\n",
    "    seti2 = [i[2] for i in Experimentation_Dataset]\n",
    "\n",
    "    for _ in range(len(Experimentation_Dataset)):\n",
    "        # For some reason i doesnt iterate and gets stuck at 0 , so had to use random int\n",
    "        r_int = random.randint(0 , 90)\n",
    "        exp_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+1])\n",
    "        exp_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+1])\n",
    "        exp_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+1])\n",
    "\n",
    "        yield (exp_seti0,exp_seti1), exp_seti2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2603da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['full_model_9/comparator_9/positional__embedding_19/embedding_19/embeddings:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/query/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/key/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/value/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/layer_normalization_111/gamma:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/layer_normalization_111/beta:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/query/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/key/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/value/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/layer_normalization_112/gamma:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/layer_normalization_112/beta:0', 'dense_117/kernel:0', 'dense_117/bias:0', 'dense_118/kernel:0', 'dense_118/bias:0', 'full_model_9/comparator_9/comparator_layer_21/feed_forward_45/layer_normalization_113/gamma:0', 'full_model_9/comparator_9/comparator_layer_21/feed_forward_45/layer_normalization_113/beta:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/query/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/key/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/value/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/layer_normalization_114/gamma:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/layer_normalization_114/beta:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/query/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/key/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/value/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/layer_normalization_115/gamma:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/layer_normalization_115/beta:0', 'dense_119/kernel:0', 'dense_119/bias:0', 'dense_120/kernel:0', 'dense_120/bias:0', 'full_model_9/comparator_9/comparator_layer_22/feed_forward_46/layer_normalization_116/gamma:0', 'full_model_9/comparator_9/comparator_layer_22/feed_forward_46/layer_normalization_116/beta:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/query/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/key/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/value/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/layer_normalization_117/gamma:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/layer_normalization_117/beta:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/query/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/key/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/value/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/layer_normalization_118/gamma:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/layer_normalization_118/beta:0', 'dense_121/kernel:0', 'dense_121/bias:0', 'dense_122/kernel:0', 'dense_122/bias:0', 'full_model_9/comparator_9/comparator_layer_23/feed_forward_47/layer_normalization_119/gamma:0', 'full_model_9/comparator_9/comparator_layer_23/feed_forward_47/layer_normalization_119/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['full_model_9/comparator_9/positional__embedding_19/embedding_19/embeddings:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/query/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/key/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/value/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/multi_head_attention_66/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/layer_normalization_111/gamma:0', 'full_model_9/comparator_9/comparator_layer_21/subject_self_attention_21/layer_normalization_111/beta:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/query/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/key/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/value/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/multi_head_attention_67/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/layer_normalization_112/gamma:0', 'full_model_9/comparator_9/comparator_layer_21/subject_cross_attention_21/layer_normalization_112/beta:0', 'dense_117/kernel:0', 'dense_117/bias:0', 'dense_118/kernel:0', 'dense_118/bias:0', 'full_model_9/comparator_9/comparator_layer_21/feed_forward_45/layer_normalization_113/gamma:0', 'full_model_9/comparator_9/comparator_layer_21/feed_forward_45/layer_normalization_113/beta:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/query/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/key/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/value/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/multi_head_attention_68/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/layer_normalization_114/gamma:0', 'full_model_9/comparator_9/comparator_layer_22/subject_self_attention_22/layer_normalization_114/beta:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/query/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/key/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/value/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/multi_head_attention_69/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/layer_normalization_115/gamma:0', 'full_model_9/comparator_9/comparator_layer_22/subject_cross_attention_22/layer_normalization_115/beta:0', 'dense_119/kernel:0', 'dense_119/bias:0', 'dense_120/kernel:0', 'dense_120/bias:0', 'full_model_9/comparator_9/comparator_layer_22/feed_forward_46/layer_normalization_116/gamma:0', 'full_model_9/comparator_9/comparator_layer_22/feed_forward_46/layer_normalization_116/beta:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/query/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/key/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/value/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/multi_head_attention_70/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/layer_normalization_117/gamma:0', 'full_model_9/comparator_9/comparator_layer_23/subject_self_attention_23/layer_normalization_117/beta:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/query/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/query/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/key/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/key/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/value/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/value/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/attention_output/kernel:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/multi_head_attention_71/attention_output/bias:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/layer_normalization_118/gamma:0', 'full_model_9/comparator_9/comparator_layer_23/subject_cross_attention_23/layer_normalization_118/beta:0', 'dense_121/kernel:0', 'dense_121/bias:0', 'dense_122/kernel:0', 'dense_122/bias:0', 'full_model_9/comparator_9/comparator_layer_23/feed_forward_47/layer_normalization_119/gamma:0', 'full_model_9/comparator_9/comparator_layer_23/feed_forward_47/layer_normalization_119/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "20/20 [==============================] - 12s 455ms/step - loss: 0.8903 - accuracy: 0.4950 - val_loss: 0.7095 - val_accuracy: 0.4820\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 7s 361ms/step - loss: 0.7200 - accuracy: 0.5050 - val_loss: 0.7060 - val_accuracy: 0.4740\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 7s 362ms/step - loss: 0.7050 - accuracy: 0.5183 - val_loss: 0.7035 - val_accuracy: 0.5120\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 7s 362ms/step - loss: 0.6988 - accuracy: 0.5500 - val_loss: 0.6971 - val_accuracy: 0.5380\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 7s 362ms/step - loss: 0.6667 - accuracy: 0.6117 - val_loss: 0.7020 - val_accuracy: 0.5620\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 7s 363ms/step - loss: 0.6646 - accuracy: 0.6100 - val_loss: 0.7099 - val_accuracy: 0.5420\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 7s 363ms/step - loss: 0.6857 - accuracy: 0.5733 - val_loss: 0.7353 - val_accuracy: 0.5860\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 7s 363ms/step - loss: 0.6132 - accuracy: 0.6517 - val_loss: 0.7295 - val_accuracy: 0.5240\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 7s 363ms/step - loss: 0.5819 - accuracy: 0.6783 - val_loss: 1.1243 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 8s 383ms/step - loss: 0.5611 - accuracy: 0.6867 - val_loss: 0.9107 - val_accuracy: 0.5620\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 8s 398ms/step - loss: 0.5551 - accuracy: 0.7367 - val_loss: 0.9678 - val_accuracy: 0.5500\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 7s 365ms/step - loss: 0.5850 - accuracy: 0.7283 - val_loss: 1.3431 - val_accuracy: 0.5860\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 7s 364ms/step - loss: 0.5154 - accuracy: 0.7333 - val_loss: 1.0886 - val_accuracy: 0.5720\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 7s 365ms/step - loss: 0.4544 - accuracy: 0.7900 - val_loss: 1.1848 - val_accuracy: 0.5600\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 8s 407ms/step - loss: 0.4119 - accuracy: 0.8450 - val_loss: 1.4880 - val_accuracy: 0.5580\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 8s 406ms/step - loss: 0.3596 - accuracy: 0.8550 - val_loss: 1.0946 - val_accuracy: 0.6120\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 8s 401ms/step - loss: 0.3079 - accuracy: 0.8617 - val_loss: 1.0215 - val_accuracy: 0.6180\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 8s 406ms/step - loss: 0.2942 - accuracy: 0.8750 - val_loss: 1.8324 - val_accuracy: 0.6060\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 8s 407ms/step - loss: 0.2197 - accuracy: 0.9183 - val_loss: 1.4012 - val_accuracy: 0.6080\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 8s 406ms/step - loss: 0.1993 - accuracy: 0.9350 - val_loss: 1.7150 - val_accuracy: 0.6120\n"
     ]
    }
   ],
   "source": [
    "#Testing accuracy is much higher than training accuracy. This is due to dropouts causing lower accuracy during training but giving a more robust model when testing\n",
    "Model = FullModel(100 , 12647 , 1000 , 0.3 , 3)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model = 100)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "Model.compile(\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=optimizer,\n",
    "    metrics= 'accuracy' )\n",
    "\n",
    "history = Model.fit(Train_Gen() , epochs=20, \n",
    "                               validation_data = Val_Gen()  , steps_per_epoch=20 , batch_size=30 , validation_steps=10 , validation_batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c6848a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[0.00315407]\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "[0.00315407]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [151], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[38;5;66;03m# if Model.predict((decider , subject_card) , verbose=False)[0][0] > 0.1:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;66;03m#     print(Sliced_df[indx:indx+1]['name'])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mpredict_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [151], line 17\u001b[0m, in \u001b[0;36mpredict_matches\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#print(subject_card)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Model\u001b[38;5;241m.\u001b[39mpredict((decider , subject_card))[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# if Model.predict((decider , subject_card) , verbose=False)[0][0] > 0.1:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#     print(Sliced_df[indx:indx+1]['name'])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     22\u001b[0m count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def predict_matches():\n",
    "    #input_array = [input('enter') for _ in range(5)]\n",
    "    input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ,'Galaxy Summoner']\n",
    "    \n",
    "    decider = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "    count = 0\n",
    "    for indx in pd.DataFrame(Tokenized_sequence_database).index.values:\n",
    "        if count<6500:\n",
    "            pass\n",
    "        else:\n",
    "            if (count % 100) == 0:\n",
    "                print(count)\n",
    "            #subject_card = tf.convert_to_tensor([np.concatenate( (Tokenized_sequence_database[indx] , np.zeros(724)) , axis=None )])\n",
    "            subject_card = tf.convert_to_tensor([np.zeros(905) ])\n",
    "            #print(subject_card)\n",
    "            print(Model.predict((decider , subject_card))[0])\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # if Model.predict((decider , subject_card) , verbose=False)[0][0] > 0.1:\n",
    "            #     print(Sliced_df[indx:indx+1]['name'])\n",
    "            #     break\n",
    "        count+=1\n",
    "predict_matches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1cb6a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 8s 12ms/step\n",
      "0.16468253968253968\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame(Model.predict(Experimentation_Gen()))\n",
    "count = 0\n",
    "for i in pred[0]:\n",
    "    if i > 0.5:\n",
    "        count+=1\n",
    "print(count/len(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 219ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9966325]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k , trash = next(Experimentation_Gen())\n",
    "l1 , l2 = k\n",
    "singular_pred = Model.predict(k)\n",
    "singular_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9e8875d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"2 aqua effect monster water cannot be used as a synchro material. this card's name becomes des frog while it is on the field. if this card is in your graveyard: you can banish 1 frog monster from your graveyard; special summon this card.\"]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.sequences_to_texts([l2[0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d9ca161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200 % 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638286c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67126460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25bc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd683030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
