{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc825eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase Of Update\n",
    "response = requests.get('https://db.ygoprodeck.com/api/v7/cardinfo.php')\n",
    "json_response = response.json()\n",
    "dataset = pd.DataFrame(json_response['data'])\n",
    "\n",
    "dataset.to_csv('Dataset/Yugioh_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc1ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import random as random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "plt.style.use('Solarize_Light2')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "#import requests\n",
    "#import itertools\n",
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ef56086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "'''Load Dataset'''\n",
    "dataset = pd.read_csv('Dataset/Yugioh_Database.csv')\n",
    "dataset = dataset.drop(['Unnamed: 0' , 'frameType' , 'archetype' , 'ygoprodeck_url' , 'card_sets' , 'card_images' , 'card_prices' , 'banlist_info'],axis=1)\n",
    "dataset = dataset[dataset['type'] != 'XYZ Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Normal Monster']\n",
    "\n",
    "dataset = dataset[dataset['type'] != 'Skill Card']\n",
    "dataset = dataset[dataset['type'] != 'Monster Token']\n",
    "\n",
    "dataset.loc[dataset['type']=='Normal Monster', ['desc']] = 'NoInfo'\n",
    "dataset = dataset.fillna('0')\n",
    "dataset['level'] = dataset['level'].astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "'''Create Tokenized sequence database'''\n",
    "\n",
    "\n",
    "df = dataset['desc']         #Tokenizer is only trained on desc and based on that . Otherwise if trained on names it would blow vocab up to absurd amounts\n",
    "Sliced_df = dataset[['level' , 'race' , 'type' , 'attribute' , 'name' , 'desc']]\n",
    "\n",
    "for i in range(1,11,2):\n",
    "    Sliced_df.insert(loc=i, column='A'+str(i), value=-1)        # Adds seperator columns\n",
    "\n",
    "\n",
    "Sliced_df = Sliced_df.reset_index(drop=True)            # Need to reset the indexes so they are consistent\n",
    "df = df.reset_index(drop=True)                              \n",
    "\n",
    "tokenizer = Tokenizer(filters='\\r , \\n , \\\" ') # Speech marks stop names from being recognised by tokenizer\n",
    "tokenizer.fit_on_texts(df)\n",
    "tokenizer.word_index['0'] = 0           #Signifies Empty values\n",
    "tokenizer.word_index['-1'] = -1           #Signifies Seperators\n",
    "\n",
    "sequences = []\n",
    "padded_sequences = []\n",
    "Tokenized_sequence_database = []\n",
    "count = 0\n",
    "for i in Sliced_df.astype('string').to_numpy():\n",
    "    \n",
    "    sequences.append(tokenizer.texts_to_sequences(i))\n",
    "    \n",
    "\n",
    "for i in range(0,11):\n",
    "    padded_sequences.append( pad_sequences(np.array(sequences , dtype='object')[:,i], padding='post') ) \n",
    "\n",
    "Tokenized_sequence_database = np.concatenate(([padded_sequences[i] for i in range(11)]) , axis=1 )\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8bf6d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7, -1, 418, 0, -1, 170, 8, 0, 0, -1, 232, -1,...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...</td>\n",
       "      <td>[0.0, -1.0, 371.0, 0.0, -1.0, 117.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[194, -1, 274, 0, -1, 21, 8, 0, 0, -1, 297, -1...</td>\n",
       "      <td>[114.0, -1.0, 274.0, 0.0, -1.0, 75.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, -1, 41, 0, -1, 80, 5, 0, 0, -1, 0, -1, 527...</td>\n",
       "      <td>[97.0, -1.0, 303.0, 0.0, -1.0, 170.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 32...</td>\n",
       "      <td>[0.0, -1.0, 374.0, 0.0, -1.0, 117.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>[97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...</td>\n",
       "      <td>[7.0, -1.0, 364.0, 0.0, -1.0, 170.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>[97, -1, 274, 0, -1, 21, 8, 0, 0, -1, 280, -1,...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>[0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 52...</td>\n",
       "      <td>[114.0, -1.0, 438.0, 618.0, -1.0, 21.0, 8.0, 0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>[277, -1, 414, 0, -1, 93, 8, 0, 0, -1, 232, -1...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>[97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>896 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    [7, -1, 418, 0, -1, 170, 8, 0, 0, -1, 232, -1,...   \n",
       "1    [97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...   \n",
       "2    [194, -1, 274, 0, -1, 21, 8, 0, 0, -1, 297, -1...   \n",
       "3    [0, -1, 41, 0, -1, 80, 5, 0, 0, -1, 0, -1, 527...   \n",
       "4    [0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 32...   \n",
       "..                                                 ...   \n",
       "891  [97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...   \n",
       "892  [97, -1, 274, 0, -1, 21, 8, 0, 0, -1, 280, -1,...   \n",
       "893  [0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 52...   \n",
       "894  [277, -1, 414, 0, -1, 93, 8, 0, 0, -1, 232, -1...   \n",
       "895  [97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...   \n",
       "\n",
       "                                                     1    2  \n",
       "0    [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [1]  \n",
       "1    [0.0, -1.0, 371.0, 0.0, -1.0, 117.0, 8.0, 0.0,...  [1]  \n",
       "2    [114.0, -1.0, 274.0, 0.0, -1.0, 75.0, 8.0, 0.0...  [1]  \n",
       "3    [97.0, -1.0, 303.0, 0.0, -1.0, 170.0, 8.0, 0.0...  [1]  \n",
       "4    [0.0, -1.0, 374.0, 0.0, -1.0, 117.0, 8.0, 0.0,...  [1]  \n",
       "..                                                 ...  ...  \n",
       "891  [7.0, -1.0, 364.0, 0.0, -1.0, 170.0, 8.0, 0.0,...  [1]  \n",
       "892  [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [1]  \n",
       "893  [114.0, -1.0, 438.0, 618.0, -1.0, 21.0, 8.0, 0...  [0]  \n",
       "894  [0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...  [1]  \n",
       "895  [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [1]  \n",
       "\n",
       "[896 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Deck_Loader(directory):\n",
    "    '''Loads Decks from Deck_Lists.txt as arrays and stores those arrays in altered'''\n",
    "    file = open(directory , 'r')\n",
    "    read = file.readlines()\n",
    "    Deck_Array = []\n",
    "    flag = False\n",
    "\n",
    "    temp=[]\n",
    "\n",
    "    for count,line in enumerate(read):\n",
    "        \n",
    "        if '//' in read[count]:\n",
    "            flag = not flag\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            read[count] = read[count].replace('\\n','')\n",
    "            \n",
    "            if ('=='  in read[count]) or ('//'  in read[count])  :\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                for i in range(int(read[count][0])):\n",
    "                    temp.append(read[count][1:].strip())          #skip appending also remove white space\n",
    "\n",
    "        if (not flag) or (count == len(read) - 1):\n",
    "            Deck_Array.append(temp)\n",
    "            temp = []\n",
    "            flag = not flag\n",
    "            \n",
    "            \n",
    "    file.close()\n",
    "    return Deck_Array \n",
    "\n",
    "\n",
    "def stitcher(Deck_Index , Deck_Array):\n",
    "    '''Picks 5 random cards from a certain deck in a deck array and stitches them together'''\n",
    "    \n",
    "    decider = [random.choice(Deck_Array[Deck_Index]) for _ in range(5)]\n",
    "    output = np.concatenate(([Tokenized_sequence_database[i] for i in Sliced_df[Sliced_df['name'].isin(decider)].index.values]) )\n",
    "    if len(output) != 905:\n",
    "        return stitcher(Deck_Index , Deck_Array)\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "def random_no_match_generator(length):\n",
    "    '''Generated Datapoints which correspond to a card that doesnt relate to a given group of decider cards'''\n",
    "    out = []\n",
    "    for _ in range(length):\n",
    "        temp = []\n",
    "        subject_card = np.concatenate( ( random.choice(Tokenized_sequence_database)  , np.zeros(724)) , axis=None )\n",
    "        decider_cards = np.concatenate([random.choice(Tokenized_sequence_database) for _ in range(5)])\n",
    "\n",
    "        temp.append(decider_cards)\n",
    "        temp.append(subject_card)\n",
    "        temp.append([0])\n",
    "\n",
    "        out.append(temp)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def Dataset_Builder(directory , include_no_match):\n",
    "    '''Builds a dataset with some specificaiton. This could be a training/validation dataset or a experimentation dataset'''\n",
    "    altered = Deck_Loader(directory)\n",
    "    Built_Dataset = []\n",
    "    for deck_index,deck in enumerate(altered):\n",
    "        \n",
    "        for card_index in Sliced_df[Sliced_df['name'].isin(deck)].index.values:\n",
    "            \n",
    "            subject_card =  np.concatenate( (Tokenized_sequence_database[card_index] , np.zeros(724)) , axis=None ) # Extends subject to equal length of decider cards\n",
    "            for _ in range(2):\n",
    "                temp = []\n",
    "                decider_cards = stitcher(deck_index , altered)\n",
    "                temp.append(decider_cards)\n",
    "                temp.append(subject_card)\n",
    "                temp.append([1])\n",
    "                Built_Dataset.append(temp)\n",
    "\n",
    "\n",
    "    if (include_no_match):\n",
    "        Built_Dataset.extend( random_no_match_generator(448 ) )\n",
    "    random.shuffle(Built_Dataset)\n",
    "    random.shuffle(Built_Dataset)\n",
    "    random.shuffle(Built_Dataset)\n",
    "    return Built_Dataset\n",
    "\n",
    "\n",
    "\n",
    "Training_Validation_Dataset = Dataset_Builder('Dataset/Deck_Lists.txt' , True)\n",
    "pd.DataFrame(Training_Validation_Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d58ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[273, -1, 364, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[61.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, -1, 109, 0, -1, 80, 5, 0, 0, -1, 0, -1, 39...</td>\n",
       "      <td>[97.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 39...</td>\n",
       "      <td>[277.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, -1, 329, 0, -1, 80, 5, 0, 0, -1, 0, -1, 13...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 318...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>[61, -1, 364, 0, -1, 21, 8, 0, 0, -1, 138, -1,...</td>\n",
       "      <td>[114.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>[61, -1, 364, 0, -1, 379, 21, 8, 0, -1, 138, -...</td>\n",
       "      <td>[114.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[273, -1, 364, 0, -1, 59, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[0.0, -1.0, 329.0, 0.0, -1.0, 112.0, 5.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>[0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 138...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[114, -1, 364, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[273.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0   [273, -1, 364, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "1   [0, -1, 109, 0, -1, 80, 5, 0, 0, -1, 0, -1, 39...   \n",
       "2   [0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 39...   \n",
       "3   [0, -1, 329, 0, -1, 80, 5, 0, 0, -1, 0, -1, 13...   \n",
       "4   [0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 318...   \n",
       "..                                                ...   \n",
       "91  [61, -1, 364, 0, -1, 21, 8, 0, 0, -1, 138, -1,...   \n",
       "92  [61, -1, 364, 0, -1, 379, 21, 8, 0, -1, 138, -...   \n",
       "93  [273, -1, 364, 0, -1, 59, 8, 0, 0, -1, 138, -1...   \n",
       "94  [0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 138...   \n",
       "95  [114, -1, 364, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "\n",
       "                                                    1    2  \n",
       "0   [61.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [1]  \n",
       "1   [97.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [1]  \n",
       "2   [277.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [1]  \n",
       "3   [0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...  [1]  \n",
       "4   [0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...  [1]  \n",
       "..                                                ...  ...  \n",
       "91  [114.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [1]  \n",
       "92  [114.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [1]  \n",
       "93  [0.0, -1.0, 329.0, 0.0, -1.0, 112.0, 5.0, 0.0,...  [1]  \n",
       "94  [0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...  [1]  \n",
       "95  [273.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [1]  \n",
       "\n",
       "[96 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a experimentation decklist with same format and push it to builder to build out a dataset. Put sequences into the sequence to text converters to see relation table with names.\n",
    "# See how accurate model is at predicting card relations withing a deck -> It should predict most cards as 1.\n",
    "\n",
    "# Create a function which allows you to type 5 cards and the create a dataset with every other card in the game as a subject and see what cards the model predicts will go well with your chosen cards.\n",
    "\n",
    "Experimentation_Dataset = Dataset_Builder('Dataset/Experimental_Deck_Lists.txt' , False)\n",
    "pd.DataFrame(Experimentation_Dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ac5140d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 905), dtype=float64, numpy=\n",
       "array([[  0.,  -1.,  59.,   0.,  -1.,  80.,   5.,   0.,   0.,  -1.,   0.,\n",
       "         -1., 318.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  -1.,\n",
       "         14.,  16.,   7.,  46., 194.,  15., 187.,  59.,   8.,  11.,   3.,\n",
       "         57.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.]])>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.sequences_to_texts([Training_Validation_Dataset[542][1]])\n",
    "m , m2 = next(Experimentation_Gen())\n",
    "l1, l2 = m\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fbd1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate( [np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "class Positional_Embedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=False) \n",
    "    self.pos_encoding = positional_encoding(length=905, depth=d_model)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x*= np.sqrt(self.d_model) # Scale Values by their embedding dimensionality otherwise they could get overwhelmed by positional encoder\n",
    "    x = x + self.pos_encoding \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class DeciderSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, ffn, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ffn, activation='relu'), \n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Single Encoder Layer with DeciderMHA and Feed Forward layer\"\"\"\n",
    "  def __init__(self, d_model, ffn , dropout_rate ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.DSA = DeciderSelfAttention(num_heads=4, key_dim=100)       # Scaling Number of Heads increases parameters as this is a different implementation of mha compared to attention is all you need paper.\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    \n",
    "    x = self.DSA(x)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Full Encoder with embedding layer with dropout and encoder layers\"\"\"\n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.EL = [EncoderLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.EL[i](x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class SubjectSelfAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class SubjectCrossAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x , context):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        key=context)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class ComparatorLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self , d_model, ffn , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.SSA = SubjectSelfAttention(num_heads=4, key_dim=100)      \n",
    "    self.SCA = SubjectCrossAttention(num_heads=4, key_dim=100)\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    \n",
    "    x = self.SSA(x)\n",
    "\n",
    "    x = self.SCA(x , context)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "class Comparator(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.CL = [ComparatorLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.CL[i](x , context)\n",
    "\n",
    "    return x\n",
    "\n",
    "class FinalFeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),          #Flattens sentances for each card comparision , into a single 1d array , so it can generate probabilities properly, instead of shoving 100 x905 matrix straight through and generating 100 probabilities for each card comparision feature embedding\n",
    "      tf.keras.layers.Dense(50, activation='relu'),\n",
    "      tf.keras.layers.Dense(25, activation='relu'),\n",
    "      tf.keras.layers.Dropout(dropout_rate),\n",
    "      tf.keras.layers.Dense(1 , activation='sigmoid')\n",
    "      \n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.seq(x) \n",
    "    return x\n",
    "  \n",
    "class FullModel(tf.keras.Model):\n",
    "   def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = Encoder(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.com = Comparator(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.FFF = FinalFeedForward(dropout_rate)\n",
    "\n",
    "   def call(self, inputs):\n",
    "     context , x = inputs\n",
    "     \n",
    "     \n",
    "     \n",
    "     context = self.enc(context)\n",
    "     x = self.com(x , context)\n",
    "\n",
    "     x = self.FFF(context)\n",
    "\n",
    "     return x\n",
    "   \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e1a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e68f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Train_Gen():\n",
    "    seti0 = [i[0] for i in Training_Validation_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Validation_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Validation_Dataset]\n",
    "    for _ in range(2000):\n",
    "        r_int = random.randint(0 , 700)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+30])\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+30])\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+30])\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def Val_Gen():\n",
    "    seti0 = [i[0] for i in Training_Validation_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Validation_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Validation_Dataset]\n",
    "    for _ in range(330):\n",
    "        r_int = random.randint(730 , 830)\n",
    "        val_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50])\n",
    "        val_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50])\n",
    "        val_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+50])\n",
    "\n",
    "        yield (val_seti0,val_seti1), val_seti2\n",
    "\n",
    "def Experimentation_Gen():\n",
    "    seti0 = [i[0] for i in Experimentation_Dataset]\n",
    "    seti1 = [i[1] for i in Experimentation_Dataset]\n",
    "    seti2 = [i[2] for i in Experimentation_Dataset]\n",
    "\n",
    "    for _ in range(len(Experimentation_Dataset)):\n",
    "        # For some reason i doesnt iterate and gets stuck at 0 , so had to use random int\n",
    "        r_int = random.randint(0 , 90)\n",
    "        exp_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+1])\n",
    "        exp_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+1])\n",
    "        exp_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+1])\n",
    "\n",
    "        yield (exp_seti0,exp_seti1), exp_seti2\n",
    "\n",
    "def Experimentation_Gen2():\n",
    "    seti0 = [i[0] for i in Training_Validation_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Validation_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Validation_Dataset]\n",
    "\n",
    "    for _ in range(len(Training_Validation_Dataset)):\n",
    "        # For some reason i doesnt iterate and gets stuck at 0 , so had to use random int\n",
    "        r_int = random.randint(0 , 90)\n",
    "        exp_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+1])\n",
    "        exp_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+1])\n",
    "        exp_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+1])\n",
    "\n",
    "        yield (exp_seti0,exp_seti1), exp_seti2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2603da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['full_model_2/comparator_2/positional__embedding_5/embedding_5/embeddings:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/query/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/query/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/key/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/key/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/value/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/value/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/attention_output/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/attention_output/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/layer_normalization_24/gamma:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/layer_normalization_24/beta:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/query/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/query/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/key/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/key/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/value/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/value/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/attention_output/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/attention_output/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/layer_normalization_25/gamma:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/layer_normalization_25/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'full_model_2/comparator_2/comparator_layer_4/feed_forward_10/layer_normalization_26/gamma:0', 'full_model_2/comparator_2/comparator_layer_4/feed_forward_10/layer_normalization_26/beta:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/query/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/query/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/key/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/key/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/value/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/value/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/attention_output/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/attention_output/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/layer_normalization_27/gamma:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/layer_normalization_27/beta:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/query/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/query/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/key/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/key/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/value/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/value/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/attention_output/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/attention_output/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/layer_normalization_28/gamma:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/layer_normalization_28/beta:0', 'dense_26/kernel:0', 'dense_26/bias:0', 'dense_27/kernel:0', 'dense_27/bias:0', 'full_model_2/comparator_2/comparator_layer_5/feed_forward_11/layer_normalization_29/gamma:0', 'full_model_2/comparator_2/comparator_layer_5/feed_forward_11/layer_normalization_29/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['full_model_2/comparator_2/positional__embedding_5/embedding_5/embeddings:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/query/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/query/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/key/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/key/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/value/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/value/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/attention_output/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/multi_head_attention_14/attention_output/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/layer_normalization_24/gamma:0', 'full_model_2/comparator_2/comparator_layer_4/subject_self_attention_4/layer_normalization_24/beta:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/query/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/query/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/key/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/key/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/value/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/value/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/attention_output/kernel:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/multi_head_attention_15/attention_output/bias:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/layer_normalization_25/gamma:0', 'full_model_2/comparator_2/comparator_layer_4/subject_cross_attention_4/layer_normalization_25/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'full_model_2/comparator_2/comparator_layer_4/feed_forward_10/layer_normalization_26/gamma:0', 'full_model_2/comparator_2/comparator_layer_4/feed_forward_10/layer_normalization_26/beta:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/query/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/query/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/key/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/key/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/value/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/value/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/attention_output/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/multi_head_attention_16/attention_output/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/layer_normalization_27/gamma:0', 'full_model_2/comparator_2/comparator_layer_5/subject_self_attention_5/layer_normalization_27/beta:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/query/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/query/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/key/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/key/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/value/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/value/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/attention_output/kernel:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/multi_head_attention_17/attention_output/bias:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/layer_normalization_28/gamma:0', 'full_model_2/comparator_2/comparator_layer_5/subject_cross_attention_5/layer_normalization_28/beta:0', 'dense_26/kernel:0', 'dense_26/bias:0', 'dense_27/kernel:0', 'dense_27/bias:0', 'full_model_2/comparator_2/comparator_layer_5/feed_forward_11/layer_normalization_29/gamma:0', 'full_model_2/comparator_2/comparator_layer_5/feed_forward_11/layer_normalization_29/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "20/20 [==============================] - 8s 299ms/step - loss: 0.9200 - accuracy: 0.5100 - val_loss: 0.7017 - val_accuracy: 0.4520\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 5s 248ms/step - loss: 0.7348 - accuracy: 0.5367 - val_loss: 0.7119 - val_accuracy: 0.4360\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 5s 243ms/step - loss: 0.6668 - accuracy: 0.5983 - val_loss: 0.6385 - val_accuracy: 0.7920\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 5s 244ms/step - loss: 0.6265 - accuracy: 0.6567 - val_loss: 0.6589 - val_accuracy: 0.5580\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 5s 244ms/step - loss: 0.5864 - accuracy: 0.6900 - val_loss: 0.6098 - val_accuracy: 0.6520\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 5s 244ms/step - loss: 0.5426 - accuracy: 0.7433 - val_loss: 0.6521 - val_accuracy: 0.6720\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 5s 244ms/step - loss: 0.4761 - accuracy: 0.7983 - val_loss: 0.4751 - val_accuracy: 0.7660\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 5s 244ms/step - loss: 0.4165 - accuracy: 0.8367 - val_loss: 0.4506 - val_accuracy: 0.8220\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 5s 245ms/step - loss: 0.3586 - accuracy: 0.8733 - val_loss: 0.4056 - val_accuracy: 0.8400\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 5s 245ms/step - loss: 0.3389 - accuracy: 0.8767 - val_loss: 0.3543 - val_accuracy: 0.8800\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 5s 246ms/step - loss: 0.2914 - accuracy: 0.8967 - val_loss: 0.5802 - val_accuracy: 0.6700\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 5s 257ms/step - loss: 0.2861 - accuracy: 0.9017 - val_loss: 0.2632 - val_accuracy: 0.9360\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 5s 271ms/step - loss: 0.1669 - accuracy: 0.9500 - val_loss: 0.2998 - val_accuracy: 0.8700\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 5s 259ms/step - loss: 0.1176 - accuracy: 0.9717 - val_loss: 0.1610 - val_accuracy: 0.9660\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 5s 273ms/step - loss: 0.0658 - accuracy: 0.9950 - val_loss: 0.1297 - val_accuracy: 0.9660\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 5s 273ms/step - loss: 0.0634 - accuracy: 0.9933 - val_loss: 0.1152 - val_accuracy: 0.9860\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 5s 273ms/step - loss: 0.0425 - accuracy: 0.9967 - val_loss: 0.1877 - val_accuracy: 0.9360\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 5s 274ms/step - loss: 0.0361 - accuracy: 0.9983 - val_loss: 0.1824 - val_accuracy: 0.9480\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 5s 274ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 0.9640\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 5s 274ms/step - loss: 0.0465 - accuracy: 0.9900 - val_loss: 0.1013 - val_accuracy: 0.9660\n"
     ]
    }
   ],
   "source": [
    "#Testing accuracy is much higher than training accuracy. This is due to dropouts causing lower accuracy during training but giving a more robust model when testing\n",
    "Model = FullModel(100 , 12647 , 1000 , 0.3 , 2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model = 100)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "Model.compile(\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=optimizer,\n",
    "    metrics= 'accuracy' )\n",
    "\n",
    "history = Model.fit(Train_Gen() , epochs=20, \n",
    "                               validation_data = Val_Gen()  , steps_per_epoch=10 , batch_size=30 , validation_steps=10 , validation_batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6848a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 905), dtype=int32, numpy=\n",
       " array([[   0,   -1,   59,    0,   -1,   80,    5,    0,    0,   -1,    0,\n",
       "           -1,  403,   80,    0,    0,    0,    0,    0,    0,    0,   -1,\n",
       "          129,    7,   80,    5,   53,   27,    7,   80,    5,   22,    3,\n",
       "           43,  140,  152,   25,   27,   22,    2,  555,  155,   32,    3,\n",
       "          210,   10,    2,   41,   19,   50,    4,  141,   21, 1963, 3312,\n",
       "           70,    5,  913,   50,   12,    1,  491,  166,   29, 5146,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,   -1,   59,    0,   -1,  112,\n",
       "            5,    0,    0,   -1,    0,   -1, 1797,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,   -1,  264,   61,   80,  298,   11,    3,\n",
       "          210,   10,    2,   41,   53,   27,    7,  138,  320,   15,  804,\n",
       "          977,   22,    3,  140,   14,   16,   25,  195,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           -1,   41,    0,   -1,   80,    5,    0,    0,   -1,    0,   -1,\n",
       "         1318, 2598,   10,    2,    0,    0,    0,    0,    0,   -1,   12,\n",
       "           24,    1,   30,   17,  364,    8,    3,   33,   39,   54,   80,\n",
       "          260,   12,    1,   30,  146,  364,   20,    1,   39,   54,   80,\n",
       "          260,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,   -1,  228,    0,   -1,  112,    5,\n",
       "            0,    0,   -1,    0,   -1, 1441,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,   -1,   37,   17,  120,  182,   42,   49,   15,\n",
       "           17,  115,    5,   13,  295,  237,  325,    3,  377,  104,    2,\n",
       "           16,   15,  204,   19,   12,    1,   67,   55,   25,   74,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   -1,\n",
       "           59,    0,   -1,   80,    5,    0,    0,   -1,    0,   -1, 2524,\n",
       "         5336,    0,    0,    0,    0,    0,    0,    0,   -1,   12,    1,\n",
       "           30,  138,  320,  387,   27,    7,    8,    3,   33,  136,   55,\n",
       "           25,  195,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(1, 905), dtype=float64, numpy=\n",
       " array([[ 0.000e+00, -1.000e+00,  7.030e+02,  0.000e+00, -1.000e+00,\n",
       "          8.000e+01,  5.000e+00,  0.000e+00,  0.000e+00, -1.000e+00,\n",
       "          0.000e+00, -1.000e+00,  4.886e+03,  1.980e+02,  1.900e+01,\n",
       "          1.204e+03,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00, -1.000e+00,  8.800e+01,  7.000e+00,  1.380e+02,\n",
       "          3.200e+02,  2.480e+02,  1.400e+01,  1.600e+01,  7.000e+00,\n",
       "          1.380e+02,  3.200e+02,  1.000e+01,  5.520e+02,  1.100e+01,\n",
       "          3.000e+00,  3.500e+01,  3.100e+01,  1.500e+01,  1.450e+02,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "          0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00]])>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k , trash = next(Experimentation_Gen())\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1cb6a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 1s 7ms/step\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame(Model.predict(Experimentation_Gen()))\n",
    "count = 0\n",
    "for i in pred[0]:\n",
    "    if i > 0.5:\n",
    "        count+=1\n",
    "print(count/len(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0   1.0\n",
       "1   1.0\n",
       "2   1.0\n",
       "3   1.0\n",
       "4   1.0\n",
       "..  ...\n",
       "91  1.0\n",
       "92  1.0\n",
       "93  1.0\n",
       "94  1.0\n",
       "95  1.0\n",
       "\n",
       "[96 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e8875d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00167721, -0.00528715,  0.00417234, ..., -0.00424869,\n",
       "          0.00017318, -0.0004217 ],\n",
       "        [-0.00293111, -0.00696292,  0.0052358 , ...,  0.0019203 ,\n",
       "         -0.00177525,  0.00711708],\n",
       "        [-0.00647978,  0.0007808 ,  0.00619664, ..., -0.00530256,\n",
       "         -0.00028723,  0.00677076],\n",
       "        ...,\n",
       "        [ 0.00116395,  0.00744231,  0.00433091, ...,  0.00440546,\n",
       "          0.00501922, -0.00790692],\n",
       "        [-0.00415578, -0.00531502,  0.00805803, ...,  0.00173602,\n",
       "         -0.0001517 , -0.00076972],\n",
       "        [ 0.00306569,  0.00010045, -0.00261832, ...,  0.00033116,\n",
       "         -0.00807252,  0.00424483]], dtype=float32),\n",
       " array([ 5.7757636e-05,  0.0000000e+00,  1.9563393e-05, -9.3085670e-05,\n",
       "         3.1239178e-05, -3.4268709e-05,  3.4750217e-05,  6.8396315e-05,\n",
       "        -6.8145142e-05,  0.0000000e+00,  5.7017871e-05,  4.8869231e-05,\n",
       "         6.9783149e-05,  1.2241572e-04, -8.0888865e-05,  5.1826446e-05,\n",
       "        -3.0762742e-05, -6.2643187e-05,  0.0000000e+00,  1.9489021e-04,\n",
       "         9.4561168e-05,  4.0376053e-05, -5.1295792e-05, -6.6984081e-05,\n",
       "        -1.0408564e-04, -7.7472105e-05, -6.6759851e-05, -6.6275614e-05,\n",
       "        -5.0589071e-05,  3.3222401e-05, -2.5743773e-05, -4.8700778e-05,\n",
       "        -1.3949366e-04, -4.9013630e-05, -3.5567740e-05,  3.7101236e-05,\n",
       "         4.9258462e-05,  2.2717191e-05,  1.1659283e-05,  0.0000000e+00,\n",
       "        -7.3758129e-05, -3.6020814e-05, -2.5336640e-05, -7.5469121e-05,\n",
       "         0.0000000e+00, -4.6812278e-05, -5.7913596e-05,  2.4034682e-05,\n",
       "        -5.7670692e-05,  9.8038501e-05], dtype=float32),\n",
       " array([[ 0.21472849, -0.24994172,  0.17834397, ...,  0.23183808,\n",
       "          0.2790736 ,  0.13113892],\n",
       "        [-0.04875164, -0.03066024,  0.07464004, ..., -0.17572236,\n",
       "          0.12817836, -0.17238843],\n",
       "        [-0.09880871,  0.02630829, -0.03952587, ..., -0.01152729,\n",
       "         -0.12320534, -0.07245115],\n",
       "        ...,\n",
       "        [ 0.16531247, -0.2259816 , -0.07063444, ..., -0.1878334 ,\n",
       "          0.13762814,  0.22624974],\n",
       "        [-0.05840287,  0.05555418, -0.077332  , ...,  0.05832273,\n",
       "          0.06140416,  0.07273121],\n",
       "        [-0.01045286, -0.17180662, -0.20544876, ..., -0.00867519,\n",
       "         -0.17049791,  0.09146618]], dtype=float32),\n",
       " array([-9.4419127e-05, -1.3782670e-04,  4.9630493e-05,  3.0276415e-04,\n",
       "        -3.3094711e-04,  2.8844453e-05,  4.9263948e-05, -7.8339508e-05,\n",
       "        -3.3546984e-04,  0.0000000e+00,  5.0622893e-05,  1.8692869e-04,\n",
       "        -9.2829803e-05, -1.0336352e-04, -1.8016463e-04,  9.8317709e-05,\n",
       "         3.7301128e-05, -2.5718589e-04, -2.7180836e-04, -7.0619426e-05,\n",
       "        -1.7107096e-04, -6.8997921e-05, -8.6877604e-05, -2.3334405e-04,\n",
       "        -7.6856140e-06], dtype=float32),\n",
       " array([[-0.00895608],\n",
       "        [ 0.26242536],\n",
       "        [ 0.08444641],\n",
       "        [ 0.01257386],\n",
       "        [ 0.21977153],\n",
       "        [ 0.31281427],\n",
       "        [ 0.39401665],\n",
       "        [ 0.2978006 ],\n",
       "        [-0.1384678 ],\n",
       "        [ 0.18965715],\n",
       "        [-0.3870941 ],\n",
       "        [ 0.11817987],\n",
       "        [-0.25798967],\n",
       "        [-0.39610592],\n",
       "        [ 0.04474919],\n",
       "        [ 0.04360503],\n",
       "        [-0.18599607],\n",
       "        [-0.09176061],\n",
       "        [ 0.10384959],\n",
       "        [ 0.27182502],\n",
       "        [-0.03770602],\n",
       "        [ 0.13543333],\n",
       "        [ 0.01694171],\n",
       "        [ 0.29705104],\n",
       "        [-0.36450988]], dtype=float32),\n",
       " array([7.03282e-05], dtype=float32)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ca161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638286c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67126460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25bc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd683030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
