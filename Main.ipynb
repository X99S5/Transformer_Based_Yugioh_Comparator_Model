{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc825eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase Of Update\n",
    "response = requests.get('https://db.ygoprodeck.com/api/v7/cardinfo.php')\n",
    "json_response = response.json()\n",
    "dataset = pd.DataFrame(json_response['data'])\n",
    "\n",
    "dataset.to_csv('Dataset/Yugioh_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc1ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import random as random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "plt.style.use('Solarize_Light2')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "#import requests\n",
    "#import itertools\n",
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef56086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "'''Load Dataset'''\n",
    "dataset = pd.read_csv('Dataset/Yugioh_Database.csv')\n",
    "dataset = dataset.drop(['Unnamed: 0' , 'frameType' , 'archetype' , 'ygoprodeck_url' , 'card_sets' , 'card_images' , 'card_prices' , 'banlist_info'],axis=1)\n",
    "dataset = dataset[dataset['type'] != 'XYZ Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Normal Monster']\n",
    "\n",
    "dataset = dataset[dataset['type'] != 'Skill Card']\n",
    "dataset = dataset[dataset['type'] != 'Monster Token']\n",
    "\n",
    "dataset.loc[dataset['type']=='Normal Monster', ['desc']] = 'NoInfo'\n",
    "dataset = dataset.fillna('0')\n",
    "dataset['level'] = dataset['level'].astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "'''Create Tokenized sequence database'''\n",
    "\n",
    "\n",
    "df = dataset['desc']         #Tokenizer is only trained on desc and based on that . Otherwise if trained on names it would blow vocab up to absurd amounts\n",
    "Sliced_df = dataset[['level' , 'race' , 'type' , 'attribute' , 'name' , 'desc']]\n",
    "\n",
    "for i in range(1,11,2):\n",
    "    Sliced_df.insert(loc=i, column='A'+str(i), value=-1)        # Adds seperator columns\n",
    "\n",
    "\n",
    "Sliced_df = Sliced_df.reset_index(drop=True)            # Need to reset the indexes so they are consistent\n",
    "df = df.reset_index(drop=True)                              \n",
    "\n",
    "tokenizer = Tokenizer(filters='\\r , \\n , \\\" ') # Speech marks stop names from being recognised by tokenizer\n",
    "tokenizer.fit_on_texts(df)\n",
    "tokenizer.word_index['0'] = 0           #Signifies Empty values\n",
    "tokenizer.word_index['-1'] = -1           #Signifies Seperators\n",
    "\n",
    "sequences = []\n",
    "padded_sequences = []\n",
    "Tokenized_sequence_database = []\n",
    "count = 0\n",
    "for i in Sliced_df.astype('string').to_numpy():\n",
    "    \n",
    "    sequences.append(tokenizer.texts_to_sequences(i))\n",
    "    \n",
    "\n",
    "for i in range(0,11):\n",
    "    padded_sequences.append( pad_sequences(np.array(sequences , dtype='object')[:,i], padding='post') ) \n",
    "\n",
    "Tokenized_sequence_database = np.concatenate(([padded_sequences[i] for i in range(11)]) , axis=1 )\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf6d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[114, -1, 411, 0, -1, 21, 8, 0, 0, -1, 297, -1...</td>\n",
       "      <td>[0.0, -1.0, 228.0, 0.0, -1.0, 112.0, 5.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 21...</td>\n",
       "      <td>[97.0, -1.0, 147.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[194, -1, 274, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[114.0, -1.0, 274.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[7, -1, 364, 0, -1, 234, 0, 0, 0, -1, 138, -1,...</td>\n",
       "      <td>[0.0, -1.0, 329.0, 0.0, -1.0, 112.0, 5.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, -1, 329, 0, -1, 112, 5, 0, 0, -1, 0, -1, 6...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>[0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 48...</td>\n",
       "      <td>[114.0, -1.0, 330.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>[0, -1, 329, 0, -1, 80, 5, 0, 0, -1, 0, -1, 29...</td>\n",
       "      <td>[7.0, -1.0, 866.0, 0.0, -1.0, 21.0, 8.0, 0.0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>[97, -1, 246, 0, -1, 379, 21, 8, 0, -1, 232, -...</td>\n",
       "      <td>[7.0, -1.0, 866.0, 0.0, -1.0, 170.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>[0, -1, 274, 0, -1, 117, 8, 0, 0, -1, 138, -1,...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>[0, -1, 41, 0, -1, 80, 5, 0, 0, -1, 0, -1, 566...</td>\n",
       "      <td>[0.0, -1.0, 113.0, 0.0, -1.0, 80.0, 5.0, 0.0, ...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>544 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    [114, -1, 411, 0, -1, 21, 8, 0, 0, -1, 297, -1...   \n",
       "1    [0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 21...   \n",
       "2    [194, -1, 274, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "3    [7, -1, 364, 0, -1, 234, 0, 0, 0, -1, 138, -1,...   \n",
       "4    [0, -1, 329, 0, -1, 112, 5, 0, 0, -1, 0, -1, 6...   \n",
       "..                                                 ...   \n",
       "539  [0, -1, 59, 0, -1, 112, 5, 0, 0, -1, 0, -1, 48...   \n",
       "540  [0, -1, 329, 0, -1, 80, 5, 0, 0, -1, 0, -1, 29...   \n",
       "541  [97, -1, 246, 0, -1, 379, 21, 8, 0, -1, 232, -...   \n",
       "542  [0, -1, 274, 0, -1, 117, 8, 0, 0, -1, 138, -1,...   \n",
       "543  [0, -1, 41, 0, -1, 80, 5, 0, 0, -1, 0, -1, 566...   \n",
       "\n",
       "                                                     1    2  \n",
       "0    [0.0, -1.0, 228.0, 0.0, -1.0, 112.0, 5.0, 0.0,...  [0]  \n",
       "1    [97.0, -1.0, 147.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [0]  \n",
       "2    [114.0, -1.0, 274.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [1]  \n",
       "3    [0.0, -1.0, 329.0, 0.0, -1.0, 112.0, 5.0, 0.0,...  [0]  \n",
       "4    [0.0, -1.0, 59.0, 0.0, -1.0, 112.0, 5.0, 0.0, ...  [0]  \n",
       "..                                                 ...  ...  \n",
       "539  [114.0, -1.0, 330.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [0]  \n",
       "540  [7.0, -1.0, 866.0, 0.0, -1.0, 21.0, 8.0, 0.0, ...  [1]  \n",
       "541  [7.0, -1.0, 866.0, 0.0, -1.0, 170.0, 8.0, 0.0,...  [0]  \n",
       "542  [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [1]  \n",
       "543  [0.0, -1.0, 113.0, 0.0, -1.0, 80.0, 5.0, 0.0, ...  [0]  \n",
       "\n",
       "[544 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Deck_Loader(directory):\n",
    "    '''Loads Decks from Deck_Lists.txt as arrays and stores those arrays in altered'''\n",
    "    file = open(directory , 'r')\n",
    "    read = file.readlines()\n",
    "    Deck_Array = []\n",
    "    flag = False\n",
    "\n",
    "    temp=[]\n",
    "\n",
    "    for count,line in enumerate(read):\n",
    "        \n",
    "        if '//' in read[count]:\n",
    "            flag = not flag\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            read[count] = read[count].replace('\\n','')\n",
    "            \n",
    "            if ('=='  in read[count]) or ('//'  in read[count])  :\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                for i in range(int(read[count][0])):\n",
    "                    temp.append(read[count][1:].strip())          #skip appending also remove white space\n",
    "\n",
    "        if (not flag) or (count == len(read) - 1):\n",
    "            Deck_Array.append(temp)\n",
    "            temp = []\n",
    "            flag = not flag\n",
    "            \n",
    "            \n",
    "    file.close()\n",
    "    return Deck_Array \n",
    "\n",
    "\n",
    "def stitcher(Deck_Index , Deck_Array):\n",
    "    '''Picks 5 random cards from a certain deck in a deck array and stitches them together'''\n",
    "    \n",
    "    decider = [random.choice(Deck_Array[Deck_Index]) for _ in range(5)]\n",
    "    output = np.concatenate(([Tokenized_sequence_database[i] for i in Sliced_df[Sliced_df['name'].isin(decider)].index.values]) )\n",
    "    if len(output) != 905:\n",
    "        return stitcher(Deck_Index , Deck_Array)\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "def random_no_match_generator(length):\n",
    "    '''Generated Datapoints which correspond to a card that doesnt relate to a given group of decider cards'''\n",
    "    out = []\n",
    "    for _ in range(length):\n",
    "        temp = []\n",
    "        subject_card = np.concatenate( ( random.choice(Tokenized_sequence_database)  , np.zeros(724)) , axis=None )\n",
    "        decider_cards = np.concatenate([random.choice(Tokenized_sequence_database) for _ in range(5)])\n",
    "\n",
    "        temp.append(decider_cards)\n",
    "        temp.append(subject_card)\n",
    "        temp.append([0])\n",
    "\n",
    "        out.append(temp)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def Dataset_Builder(directory , include_no_match):\n",
    "    '''Builds a dataset with some specificaiton. This could be a training/validation dataset or a experimentation dataset'''\n",
    "    altered = Deck_Loader(directory)\n",
    "    Built_Dataset = []\n",
    "    for deck_index,deck in enumerate(altered):\n",
    "        \n",
    "        for card_index in Sliced_df[Sliced_df['name'].isin(deck)].index.values:\n",
    "            \n",
    "            subject_card =  np.concatenate( (Tokenized_sequence_database[card_index] , np.zeros(724)) , axis=None ) # Extends subject to equal length of decider cards\n",
    "            for _ in range(2):\n",
    "                temp = []\n",
    "                decider_cards = stitcher(deck_index , altered)\n",
    "                temp.append(decider_cards)\n",
    "                temp.append(subject_card)\n",
    "                temp.append([1])\n",
    "                Built_Dataset.append(temp)\n",
    "\n",
    "\n",
    "    if (include_no_match):\n",
    "        Built_Dataset.extend( random_no_match_generator(272) )\n",
    "    random.shuffle(Built_Dataset)\n",
    "    random.shuffle(Built_Dataset)\n",
    "    random.shuffle(Built_Dataset)\n",
    "    return Built_Dataset\n",
    "\n",
    "\n",
    "\n",
    "Training_Validation_Dataset = Dataset_Builder('Dataset/Deck_Lists.txt' , True)\n",
    "pd.DataFrame(Training_Validation_Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d58ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5140d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts([Training_Validation_Dataset[542][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate( [np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "class Positional_Embedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=False) \n",
    "    self.pos_encoding = positional_encoding(length=905, depth=d_model)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x*= np.sqrt(self.d_model) # Scale Values by their embedding dimensionality otherwise they could get overwhelmed by positional encoder\n",
    "    x = x + self.pos_encoding \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class DeciderSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, ffn, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ffn, activation='relu'), \n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Single Encoder Layer with DeciderMHA and Feed Forward layer\"\"\"\n",
    "  def __init__(self, d_model, ffn , dropout_rate ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.DSA = DeciderSelfAttention(num_heads=4, key_dim=100)       # Scaling Number of Heads increases parameters as this is a different implementation of mha compared to attention is all you need paper.\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    \n",
    "    x = self.DSA(x)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Full Encoder with embedding layer with dropout and encoder layers\"\"\"\n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.EL = [EncoderLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.EL[i](x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class SubjectSelfAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class SubjectCrossAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x , context):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        key=context)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class ComparatorLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self , d_model, ffn , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.SSA = SubjectSelfAttention(num_heads=4, key_dim=100)      \n",
    "    self.SCA = SubjectCrossAttention(num_heads=4, key_dim=100)\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    \n",
    "    x = self.SSA(x)\n",
    "\n",
    "    x = self.SCA(x , context)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "class Comparator(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.CL = [ComparatorLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.CL[i](x , context)\n",
    "\n",
    "    return x\n",
    "\n",
    "class FinalFeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),          #Flattens sentances for each card comparision , into a single 1d array , so it can generate probabilities properly, instead of shoving 100 x905 matrix straight through and generating 100 probabilities for each card comparision feature embedding\n",
    "      tf.keras.layers.Dense(50, activation='relu'),\n",
    "      tf.keras.layers.Dense(25, activation='relu'),\n",
    "      tf.keras.layers.Dropout(dropout_rate),\n",
    "      tf.keras.layers.Dense(1 , activation='sigmoid')\n",
    "      \n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.seq(x) \n",
    "    return x\n",
    "  \n",
    "class FullModel(tf.keras.Model):\n",
    "   def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = Encoder(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.com = Comparator(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.FFF = FinalFeedForward(dropout_rate)\n",
    "\n",
    "   def call(self, inputs):\n",
    "     context , x = inputs\n",
    "     \n",
    "     \n",
    "     \n",
    "     context = self.enc(context)\n",
    "     x = self.com(x , context)\n",
    "\n",
    "     x = self.FFF(context)\n",
    "\n",
    "     return x\n",
    "   \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e1a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e68f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gen():\n",
    "    for _ in range(330):\n",
    "        r_int = random.randint(0 , 200)\n",
    "        train_seti0 = tf.convert_to_tensor([i[0] for i in Training_Validation_Dataset][r_int:r_int+30])\n",
    "        train_seti1 = tf.convert_to_tensor([i[1] for i in Training_Validation_Dataset][r_int:r_int+30])\n",
    "        train_seti2 = tf.convert_to_tensor([i[2] for i in Training_Validation_Dataset][r_int:r_int+30])\n",
    "    \n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def val_gen():\n",
    "\n",
    "    for _ in range(330):\n",
    "        r_int = random.randint(230 , 400)\n",
    "        test_seti0 = tf.convert_to_tensor([i[0] for i in Training_Validation_Dataset][r_int:r_int+50])\n",
    "        test_seti1 = tf.convert_to_tensor([i[1] for i in Training_Validation_Dataset][r_int:r_int+50])\n",
    "        test_seti2 = tf.convert_to_tensor([i[2] for i in Training_Validation_Dataset][r_int:r_int+50])\n",
    "\n",
    "    \n",
    "        yield (test_seti0,test_seti1), test_seti2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing accuracy is much higher than training accuracy. This is due to dropouts causing lower accuracy during training but giving a more robust model when testing\n",
    "Model = FullModel(100 , 12647 , 1000 , 0.1 , 2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model = 100)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "Model.compile(\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=optimizer,\n",
    "    metrics= 'accuracy' )\n",
    "\n",
    "history = Model.fit(train_gen() , epochs=30, \n",
    "                               validation_data = val_gen()  , steps_per_epoch=10 , batch_size=30 , validation_steps=10 , validation_batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6848a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6a5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8875d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ca161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638286c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67126460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25bc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd683030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
