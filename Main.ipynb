{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc825eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from IPython.display import display, HTML , clear_output\n",
    "import ast\n",
    "\n",
    "import random as random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "plt.style.use('Solarize_Light2')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "import requests\n",
    "import itertools\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0] , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fb4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Incase Of Update\n",
    "### If you are using the program for first time, run this once to download data about all current yugioh cards.\n",
    "response = requests.get('https://db.ygoprodeck.com/api/v7/cardinfo.php')\n",
    "json_response = response.json()\n",
    "dataset = pd.DataFrame(json_response['data'])\n",
    "\n",
    "dataset.to_csv('Dataset/Yugioh_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc1ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef56086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "'''Load Dataset'''\n",
    "dataset = pd.read_csv('Dataset/Yugioh_Database.csv')\n",
    "dataset = dataset.drop(['Unnamed: 0' , 'frameType' , 'archetype' , 'ygoprodeck_url' , 'card_sets' , 'card_prices' , 'banlist_info'],axis=1)\n",
    "dataset = dataset[dataset['type'] != 'XYZ Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Normal Monster']\n",
    "\n",
    "dataset = dataset[dataset['type'] != 'Skill Card']\n",
    "dataset = dataset[dataset['type'] != 'Monster Token']\n",
    "\t \n",
    "# dataset = dataset[dataset['type'] != 'Spell Card']\n",
    "# dataset = dataset[dataset['type'] != 'Trap Card']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Monster']\n",
    "dataset = dataset[dataset['type'] != 'XYZ Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Monster']\n",
    "dataset = dataset[dataset['type'] != 'Link Monster']\n",
    "# Staple Removal\n",
    "dataset = dataset[[not i for i in dataset['name'].isin(['Ash Blossom & Joyous Spring' , 'Effect Veiler' , 'Ghost Ogre & Snow Rabbit' ,'Ghost Belle & Haunted Mansion',\n",
    "                                                        'Infinite Impermanence' , 'Red Reboot' , 'Called by the Grave' , 'Forbidden Droplet' , 'Crossout Designator',\n",
    "                                                        'Nibiru, the Primal Being', 'Harpie\\\"s Feather Duster' , 'Lightning Storm' , 'Pot of Prosperity' , 'Pot of Desires',\n",
    "                                                        'Pot of Duality' , 'Pot of Extravagance' , 'Triple Tactics Talents' , 'Torrential Tribute' , 'Dark Ruler No More' , \n",
    "                                                        'Red Reboot', 'D.D. Crow' , 'PSY-Framegear Gamma' , 'Maxx \\\"C\\\"' , 'Dimension Shifter' , 'Droll & Lock Bird' , \n",
    "                                                        'Accesscode Talker', 'Apollousa, Bow of the Goddess', 'Borreload Dragon' , 'Borrelsword Dragon', 'Knightmare Unicorn',\n",
    "                                                        'Predaplant Verte Anaconda' , 'Knightmare Phoenix' , 'Knightmare Cerberus' , 'Underworld Goddess of the Closed World',\n",
    "                                                        'Borreload Savage Dragon' , 'Token Collector' , 'Evenly Matched' , 'Forbidden Chalice' , 'Cosmic Cyclone' , 'Contact \\\"C\\\"',\n",
    "                                                        'Retaliating \\\"C\\\"' , 'Gadarla, the Mystery Dust Kaiju' , 'Solemn Judgment' , 'Dimensional Barrier' , 'Solemn Strike',\n",
    "                                                         'Ice Dragon\\'s Prison' , 'Gozen Match' ])]]\n",
    "\n",
    "dataset.loc[dataset['type']=='Normal Monster', ['desc']] = 'NoInfo'\n",
    "dataset = dataset.fillna('0')\n",
    "dataset['level'] = dataset['level'].astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "'''Create Tokenized sequence database'''\n",
    "\n",
    "\n",
    "df = dataset['desc']         #Tokenizer is only trained on desc and based on that . Otherwise if trained on names too it would blow vocab up to absurd amounts\n",
    "Sliced_df = dataset[['level' , 'race' , 'type' , 'attribute' , 'name' , 'desc']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sliced_df = Sliced_df.reset_index(drop=True)            # Need to reset the indexes so they are consistent\n",
    "df = df.reset_index(drop=True)\n",
    "dataset = dataset.reset_index(drop=True)                              \n",
    "\n",
    "tokenizer = Tokenizer(filters='\\r , \\n , \\\" ') \n",
    "tokenizer.fit_on_texts(df)\n",
    "tokenizer.word_index['0'] = 0           #Signifies Empty values\n",
    "\n",
    "sequences = []\n",
    "padded_sequences = []\n",
    "Tokenized_sequence_database = []\n",
    "count = 0\n",
    "for i in Sliced_df.astype('string').to_numpy():\n",
    "    \n",
    "    sequences.append(tokenizer.texts_to_sequences(i))\n",
    "    \n",
    "\n",
    "for i in range(0,6):\n",
    "    padded_sequences.append( pad_sequences(np.array(sequences , dtype='object')[:,i], padding='post') ) \n",
    "\n",
    "Tokenized_sequence_database = np.concatenate(([padded_sequences[i] for i in range(6)]) , axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d024094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf6d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deck_Loader(directory):\n",
    "    '''Loads Decks from Deck_Lists.txt as arrays and stores those arrays in altered'''\n",
    "    file = open(directory , 'r')\n",
    "    read = file.readlines()\n",
    "    Deck_Array = []\n",
    "    flag = False\n",
    "\n",
    "    temp=[]\n",
    "\n",
    "    for count,line in enumerate(read):\n",
    "        \n",
    "        if '//' in read[count]:\n",
    "            flag = not flag\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            read[count] = read[count].replace('\\n','')\n",
    "            \n",
    "            if ('=='  in read[count]) or ('//'  in read[count])  :\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                for i in range(int(read[count][0])):\n",
    "                    temp.append(read[count][1:].strip())          \n",
    "\n",
    "        if (not flag) or (count == len(read) - 1):\n",
    "            Deck_Array.append(temp)\n",
    "            temp = []\n",
    "            flag = not flag\n",
    "            \n",
    "            \n",
    "    file.close()\n",
    "    return Deck_Array \n",
    "\n",
    "def Dataset_Builder(Direc):\n",
    "    Loaded_Decks = Deck_Loader(Direc)\n",
    "\n",
    "    Output = []\n",
    "    for Deck in Loaded_Decks:\n",
    "        for _ in range(30):\n",
    "            choices = Sliced_df[Sliced_df['name'].isin(Deck)].index.values          # Prevents staples from leaking into data as sliceddf is filtered from staples.\n",
    "\n",
    "            indexes = random.sample(choices.tolist() , 5)\n",
    "            Decider = np.concatenate([Tokenized_sequence_database[i] for i in indexes])\n",
    "            \n",
    "            Output.append([Decider , [1]])\n",
    "\n",
    "    for _ in range(len(Output)):\n",
    "        Decks = random.sample(Loaded_Decks , 5)\n",
    "        choices1 = Sliced_df[Sliced_df['name'].isin(Decks[0])].index.values \n",
    "        choices2 = Sliced_df[Sliced_df['name'].isin(Decks[1])].index.values\n",
    "        choices3 = Sliced_df[Sliced_df['name'].isin(Decks[2])].index.values\n",
    "        choices4 = Sliced_df[Sliced_df['name'].isin(Decks[3])].index.values\n",
    "        choices5 = Sliced_df[Sliced_df['name'].isin(Decks[4])].index.values\n",
    "\n",
    "        indexes = np.concatenate([random.choices(choices1.tolist() , k=1) , random.choices(choices2.tolist() , k=1) , random.choices(choices3.tolist() , k=1) , random.choices(choices4.tolist() , k=1) , random.choices(choices5.tolist() , k=1)])\n",
    "\n",
    "        Decider = np.concatenate([Tokenized_sequence_database[i] for i in indexes])\n",
    "\n",
    "        Output.append([Decider , [0]])\n",
    "    \n",
    "    random.shuffle(Output)\n",
    "    random.shuffle(Output)\n",
    "    random.shuffle(Output)\n",
    "    return Output\n",
    "\n",
    "Training_Dataset = Dataset_Builder('Dataset/Training_Deck_Lists.txt')\n",
    "\n",
    "pd.DataFrame(Training_Dataset)\n",
    "\n",
    "#pd.DataFrame(Training_Dataset)\n",
    "#x,y = Training_Dataset[0][0]\n",
    "#tokenizer.sequences_to_texts(np.array(y))\n",
    "#np.array(x).shape       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5796e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d58ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5140d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Embedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size , d_model ) \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "  def call(self, x):\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class DeciderSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, ffn, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ffn, activation='relu'),  \n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Single Encoder Layer with DeciderMHA and Feed Forward layer\"\"\"\n",
    "  def __init__(self, d_model, ffn , dropout_rate ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.DSA = DeciderSelfAttention(num_heads=2, key_dim=d_model , dropout = dropout_rate)       \n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    \n",
    "    x = self.DSA(x)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Full Encoder with embedding layer with dropout and encoder layers\"\"\"\n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.EL = [EncoderLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    #self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self,x):\n",
    "    \n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    #x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.EL[i](x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class FinalFeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),          \n",
    "      # tf.keras.layers.Dense(25, activation='relu'),\n",
    "      # tf.keras.layers.Dropout(dropout_rate),\n",
    "      tf.keras.layers.Dense(1 , activation='sigmoid')\n",
    "      \n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.seq(x) \n",
    "    return x\n",
    "  \n",
    "class FullModel(tf.keras.Model):\n",
    "   def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = Encoder(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.FFF = FinalFeedForward(dropout_rate)\n",
    "\n",
    "   def call(self, x):\n",
    "     \n",
    "     x = self.enc(x)\n",
    "    \n",
    "     x = self.FFF(x)\n",
    "\n",
    "     return x\n",
    "   \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e1a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e68f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    \n",
    "    for _ in range(100000):\n",
    "        r_int = random.randint(0 , int(len(Training_Dataset)*0.8) - 5)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+5] , dtype='float32')\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+5], dtype='float32')\n",
    "        \n",
    "    \n",
    "        yield train_seti0,train_seti1\n",
    "\n",
    "def Val_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    \n",
    "    for _ in range(100000):\n",
    "        r_int = random.randint(int(len(Training_Dataset)*0.8) , len(Training_Dataset) - 50)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50] , dtype='float32')\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50] , dtype='float32')\n",
    "                  \n",
    "    \n",
    "        yield train_seti0,train_seti1\n",
    "\n",
    "def Pred_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    \n",
    "    for _ in range(1):\n",
    "        r_int = random.randint(int(len(Training_Dataset)*0.8) , len(Training_Dataset) - 50)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50] , dtype='float32')\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50] , dtype='float32')\n",
    "                  \n",
    "    \n",
    "        yield train_seti0,train_seti1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2603da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run to train a new model otherwise load one of the prebuilt archive models from the next section\n",
    "\n",
    "# Model = FullModel(200 , 5000 , 200 *2 , 0.1 , 1)\n",
    "\n",
    "# learning_rate = CustomSchedule(d_model = 200)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate , beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "#     log_dir='Logs/Logs24' ,histogram_freq=1\n",
    "# )\n",
    " \n",
    "# Model.compile(\n",
    "#     loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "#     optimizer=optimizer,\n",
    "#     metrics= 'accuracy' )\n",
    "\n",
    "# history = Model.fit(Train_Split_Gen() , epochs=4, \n",
    "#                                validation_data = Val_Split_Gen()  , steps_per_epoch=40 , batch_size=5 , \n",
    "#                                validation_steps=50 , validation_batch_size=50 , callbacks=[tensorboard_callback] )\n",
    "\n",
    "##Model.save_weights('Saved_Models/___')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef9527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "576e9ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### Run a section to Load a certain Model\n",
    "\n",
    "### 6468 \n",
    "ArchivedModel = FullModel(225 , 5000 , 225 *2 , 0.1 , 1)\n",
    "ArchivedModel.load_weights('Saved_Models/Model6468/Model6468_Weights')\n",
    "\n",
    "### 6220\n",
    "#ArchivedModel = FullModel(225 , 5000 , 225 *2 , 0.1 , 1)\n",
    "#ArchivedModel.load_weights('Saved_Models/Model6220/Model6220_Weights')\n",
    "\n",
    "### 6372\n",
    "# ArchivedModel = FullModel(200 , 5000 , 200 *2 , 0.1 , 1)\n",
    "# ArchivedModel.load_weights('Saved_Models/Model6372/Model6372_Weights')\n",
    "\n",
    "### 6548\n",
    "#ArchivedModel = FullModel(200 , 5000 , 200 *2 , 0.1 , 1)\n",
    "#ArchivedModel.load_weights('Saved_Models/Model6548/Model6548_Weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a008673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelStats(Model):\n",
    "    '''Pass in a trained Model to see some stats about it'''\n",
    "    \n",
    "    Data , True_Labels = next(Pred_Split_Gen())\n",
    "    Predictions = Model.predict(Data)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    vals = pd.DataFrame([{'z' : 0 ,'x' : i[0],'y' : j[0]} for i,j in zip(Predictions , True_Labels.numpy())])\n",
    "\n",
    "    ### Plot prediction values against true Labels\n",
    "    sns.scatterplot(data=vals , x='z' , y='x' , hue='y' , ax=axes[0])\n",
    "    axes[0].set_title('Prediction against True Labels' , fontsize=14)\n",
    "    axes[0].set_xlabel('')\n",
    "    axes[0].set_ylabel('Predictions')\n",
    "    axes[0].legend(title='True Labels')\n",
    "    \n",
    "\n",
    "    Accuracy = []\n",
    "    Threshold = []\n",
    "    for threshold in range(0,20,1):\n",
    "        count = 0\n",
    "        for i in vals.values:\n",
    "            if i[1] > threshold/20:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "            \n",
    "            if pred == i[2]:\n",
    "                count+=1\n",
    "        \n",
    "        Accuracy.append(count/len(vals))\n",
    "        Threshold.append(threshold/20)\n",
    "    \n",
    "    sns.lineplot(x=Threshold , y=Accuracy ,ax=axes[1])\n",
    "    axes[1].set_title('Accuracy against Threshold for a single batch of validation data' , fontsize=12)\n",
    "    axes[1].set_xlabel('Threshold value')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "ModelStats(ArchivedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80df794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50364525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_tester(Model , input_array):\n",
    "    ''' Gives a prediction on how related a group of 5 cards are'''\n",
    "    indexes = Sliced_df[Sliced_df['name'].isin(input_array)].index.values\n",
    "    Decider = tf.convert_to_tensor([ np.concatenate([Tokenized_sequence_database[i] for i in indexes])    ])\n",
    "\n",
    "    return Model(Decider)\n",
    "\n",
    "\n",
    "input_array = ['Galaxy-Eyes Photon Dragon' , 'Junk Converter'  , 'Doppelwarrior',  'Bystial Baldrake'   , 'Road Synchron' ]\n",
    "\n",
    "Model_tester(ArchivedModel ,input_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6848a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_matches(start , end , threshold  , input_array , Model):\n",
    "    \"\"\" Takes in an input of 4 cards you would like to search a match for, and searches a database based on the start and end indexs provided. Choose threshold based on how strong a prediction must be before a card is recommended as a match\"\"\"\n",
    "    \n",
    "    indexes = Sliced_df[Sliced_df['name'].isin(input_array)].index.values\n",
    "    Incomplete_Decider = np.concatenate([Tokenized_sequence_database[i] for i in indexes])\n",
    "\n",
    "    count = 0\n",
    "    chosen_indexes = []\n",
    "    for indx in pd.DataFrame(Tokenized_sequence_database).index.values:\n",
    "        if count<start:\n",
    "            pass\n",
    "        elif count<end:\n",
    "            if (count % 100) == 0:\n",
    "                print(count)\n",
    "            Decider = tf.convert_to_tensor([np.concatenate([ Incomplete_Decider  , Tokenized_sequence_database[indx] ])])\n",
    "            \n",
    "            \n",
    "            if Model.predict(Decider , verbose=False)[0][0] > threshold:\n",
    "                #print(Sliced_df[indx:indx+1]['name'])\n",
    "                chosen_indexes.append(indx)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        count+=1\n",
    "\n",
    "    image_urls = [ast.literal_eval(dataset['card_images'][i])[0]['image_url'] for i in chosen_indexes]\n",
    "    image_html = ''.join([f'<img src=\"{url}\" style=\"width:340px; margin:10px; display:inline-block;\">' for url in image_urls])\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ]\n",
    "\n",
    "predict_matches(0, 100 , 0.5 ,input_array , ArchivedModel )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8875d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ca161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638286c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67126460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25bc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd683030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
