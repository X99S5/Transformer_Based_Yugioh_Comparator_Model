{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc825eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89fb4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase Of Update\n",
    "# response = requests.get('https://db.ygoprodeck.com/api/v7/cardinfo.php')\n",
    "# json_response = response.json()\n",
    "# dataset = pd.DataFrame(json_response['data'])\n",
    "\n",
    "# dataset.to_csv('Dataset/Yugioh_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdc1ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from IPython.display import display, HTML , clear_output\n",
    "import ast\n",
    "\n",
    "import random as random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "plt.style.use('Solarize_Light2')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "#import requests\n",
    "#import itertools\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0] , True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ef56086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "'''Load Dataset'''\n",
    "dataset = pd.read_csv('Dataset/Yugioh_Database.csv')\n",
    "dataset = dataset.drop(['Unnamed: 0' , 'frameType' , 'archetype' , 'ygoprodeck_url' , 'card_sets' , 'card_prices' , 'banlist_info'],axis=1)\n",
    "dataset = dataset[dataset['type'] != 'XYZ Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Normal Monster']\n",
    "\n",
    "dataset = dataset[dataset['type'] != 'Skill Card']\n",
    "dataset = dataset[dataset['type'] != 'Monster Token']\n",
    "\t \n",
    "# dataset = dataset[dataset['type'] != 'Spell Card']\n",
    "# dataset = dataset[dataset['type'] != 'Trap Card']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Monster']\n",
    "dataset = dataset[dataset['type'] != 'XYZ Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Monster']\n",
    "dataset = dataset[dataset['type'] != 'Link Monster']\n",
    "# Staple Removal\n",
    "dataset = dataset[[not i for i in dataset['name'].isin(['Ash Blossom & Joyous Spring' , 'Effect Veiler' , 'Ghost Ogre & Snow Rabbit' ,'Ghost Belle & Haunted Mansion',\n",
    "                                                        'Infinite Impermanence' , 'Red Reboot' , 'Called by the Grave' , 'Forbidden Droplet' , 'Crossout Designator',\n",
    "                                                        'Nibiru, the Primal Being', 'Harpie\\\"s Feather Duster' , 'Lightning Storm' , 'Pot of Prosperity' , 'Pot of Desires',\n",
    "                                                        'Pot of Duality' , 'Pot of Extravagance' , 'Triple Tactics Talents' , 'Torrential Tribute' , 'Dark Ruler No More' , \n",
    "                                                        'Red Reboot', 'D.D. Crow' , 'PSY-Framegear Gamma' , 'Maxx \\\"C\\\"' , 'Dimension Shifter' , 'Droll & Lock Bird' , \n",
    "                                                        'Accesscode Talker', 'Apollousa, Bow of the Goddess', 'Borreload Dragon' , 'Borrelsword Dragon', 'Knightmare Unicorn',\n",
    "                                                        'Predaplant Verte Anaconda' , 'Knightmare Phoenix' , 'Knightmare Cerberus' , 'Underworld Goddess of the Closed World',\n",
    "                                                        'Borreload Savage Dragon' , 'Token Collector' , 'Evenly Matched' , 'Forbidden Chalice' , 'Cosmic Cyclone' , 'Contact \\\"C\\\"',\n",
    "                                                        'Retaliating \\\"C\\\"' , 'Gadarla, the Mystery Dust Kaiju' , 'Solemn Judgment' , 'Dimensional Barrier' , 'Solemn Strike',\n",
    "                                                         'Ice Dragon\\'s Prison' , 'Gozen Match' ])]]\n",
    "\n",
    "dataset.loc[dataset['type']=='Normal Monster', ['desc']] = 'NoInfo'\n",
    "dataset = dataset.fillna('0')\n",
    "dataset['level'] = dataset['level'].astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "'''Create Tokenized sequence database'''\n",
    "\n",
    "\n",
    "df = dataset['desc']         #Tokenizer is only trained on desc and based on that . Otherwise if trained on names it would blow vocab up to absurd amounts\n",
    "Sliced_df = dataset[['level' , 'race' , 'type' , 'attribute' , 'name' , 'desc']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sliced_df = Sliced_df.reset_index(drop=True)            # Need to reset the indexes so they are consistent\n",
    "df = df.reset_index(drop=True)\n",
    "dataset = dataset.reset_index(drop=True)                              \n",
    "\n",
    "tokenizer = Tokenizer(filters='\\r , \\n , \\\" ') # Speech marks stop names from being recognised by tokenizer\n",
    "tokenizer.fit_on_texts(df)\n",
    "tokenizer.word_index['0'] = 0           #Signifies Empty values\n",
    "\n",
    "sequences = []\n",
    "padded_sequences = []\n",
    "Tokenized_sequence_database = []\n",
    "count = 0\n",
    "for i in Sliced_df.astype('string').to_numpy():\n",
    "    \n",
    "    sequences.append(tokenizer.texts_to_sequences(i))\n",
    "    \n",
    "\n",
    "for i in range(0,6):\n",
    "    padded_sequences.append( pad_sequences(np.array(sequences , dtype='object')[:,i], padding='post') ) \n",
    "\n",
    "Tokenized_sequence_database = np.concatenate(([padded_sequences[i] for i in range(6)]) , axis=1 )\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d024094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_urls = [ast.literal_eval(dataset['card_images'][i])[0]['image_url'] for i in range(5406,5407)]\n",
    "# image_html = ''.join([f'<img src=\"{url}\" style=\"width:340px; margin:20px; display:inline-block;\">' for url in image_urls])\n",
    "# display(HTML(image_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bf6d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deck_Loader(directory):\n",
    "    '''Loads Decks from Deck_Lists.txt as arrays and stores those arrays in altered'''\n",
    "    file = open(directory , 'r')\n",
    "    read = file.readlines()\n",
    "    Deck_Array = []\n",
    "    flag = False\n",
    "\n",
    "    temp=[]\n",
    "\n",
    "    for count,line in enumerate(read):\n",
    "        \n",
    "        if '//' in read[count]:\n",
    "            flag = not flag\n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            read[count] = read[count].replace('\\n','')\n",
    "            \n",
    "            if ('=='  in read[count]) or ('//'  in read[count])  :\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                for i in range(int(read[count][0])):\n",
    "                    temp.append(read[count][1:].strip())          #skip appending also remove white space\n",
    "\n",
    "        if (not flag) or (count == len(read) - 1):\n",
    "            Deck_Array.append(temp)\n",
    "            temp = []\n",
    "            flag = not flag\n",
    "            \n",
    "            \n",
    "    file.close()\n",
    "    return Deck_Array \n",
    "\n",
    "def Dataset_Builder1(Direc):\n",
    "    Loaded_Decks = Deck_Loader(Direc)\n",
    "    label_translation = {'1' : [1 , 0 , 0 , 0 ] , '2' : [0 , 1 , 0 , 0] , '3' : [0 , 0 , 1 , 0] , '4' : [0 , 0 , 0 , 1] }\n",
    "    \n",
    "    output = []\n",
    "    for Deck in Loaded_Decks:\n",
    "        for _ in range(2):\n",
    "            choices = Sliced_df[Sliced_df['name'].isin(Deck)].index.values          # Prevents staples from leaking into data as sliceddf is filtered from staples.\n",
    "\n",
    "            indexes = random.choices(choices.tolist() , k=5)\n",
    "            Decider = np.concatenate([Tokenized_sequence_database[i] for i in indexes])\n",
    "            try:\n",
    "                image_urls = [ast.literal_eval(dataset['card_images'][i])[0]['image_url'] for i in indexes]\n",
    "                image_html = ''.join([f'<img src=\"{url}\" style=\"width:340px; margin:20px; display:inline-block;\">' for url in image_urls])\n",
    "                display(HTML(image_html))\n",
    "                label = input()\n",
    "                clear_output()\n",
    "\n",
    "                \n",
    "                output.append([Decider, label_translation[label]])\n",
    "                      \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    pd.DataFrame(output).to_csv('Dataset/Training_Testing_Dataset.csv' , mode='a' , index=False, header=False)\n",
    "\n",
    "          \n",
    "def Dataset_Builder2(Direc):\n",
    "    Loaded_Decks = Deck_Loader(Direc)\n",
    "    label_translation = {'1' : [1 , 0 , 0 , 0 ] , '2' : [0 , 1 , 0 , 0] , '3' : [0 , 0 , 1 , 0] , '4' : [0 , 0 , 0 , 1] }\n",
    "\n",
    "    output = []\n",
    "    for _ in range(5):\n",
    "        Decks = random.sample(Loaded_Decks , 2)\n",
    "        choices1 = Sliced_df[Sliced_df['name'].isin(Decks[0])].index.values \n",
    "        choices2 = Sliced_df[Sliced_df['name'].isin(Decks[1])].index.values\n",
    "        indexes = np.concatenate([random.choices(choices1.tolist() , k=3),random.choices(choices2.tolist() , k=2)])\n",
    "\n",
    "        Decider = np.concatenate([Tokenized_sequence_database[i] for i in indexes])\n",
    "       \n",
    "        try:\n",
    "                image_urls = [ast.literal_eval(dataset['card_images'][i])[0]['image_url'] for i in indexes]\n",
    "                image_html = ''.join([f'<img src=\"{url}\" style=\"width:340px; margin:20px; display:inline-block;\">' for url in image_urls])\n",
    "                display(HTML(image_html))\n",
    "                label = input()\n",
    "                clear_output()\n",
    "\n",
    "                \n",
    "                output.append([Decider, label_translation[label]])\n",
    "                      \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    pd.DataFrame(output).to_csv('Dataset/Training_Testing_Dataset.csv' , mode='a' , index=False, header=False)\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "Dataset_Builder2('Dataset/Training_Deck_Lists.txt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pd.DataFrame(Training_Dataset)\n",
    "#x,y = Training_Dataset[0][0]\n",
    "#tokenizer.sequences_to_texts(np.array(y))\n",
    "#np.array(x).shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5796e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Training_Dataset[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d58ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Training_Dataset)[0:1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5140d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Embedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size ,d_model ) \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "  def call(self, x):\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class DeciderSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, ffn, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ffn, activation='relu'),  \n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Single Encoder Layer with DeciderMHA and Feed Forward layer\"\"\"\n",
    "  def __init__(self, d_model, ffn , dropout_rate ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.DSA = DeciderSelfAttention(num_heads=2, key_dim=d_model , dropout = dropout_rate)       \n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    \n",
    "    x = self.DSA(x)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Full Encoder with embedding layer with dropout and encoder layers\"\"\"\n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.EL = [EncoderLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    #self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self,x):\n",
    "    \n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    #x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.EL[i](x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class FinalFeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),          \n",
    "      # tf.keras.layers.Dense(25, activation='relu'),\n",
    "      # tf.keras.layers.Dropout(dropout_rate),\n",
    "      tf.keras.layers.Dense(1 , activation='sigmoid')\n",
    "      \n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.seq(x) \n",
    "    return x\n",
    "  \n",
    "class FullModel(tf.keras.Model):\n",
    "   def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = Encoder(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.FFF = FinalFeedForward(dropout_rate)\n",
    "\n",
    "   def call(self, x):\n",
    "     \n",
    "     x = self.enc(x)\n",
    "    \n",
    "     x = self.FFF(x)\n",
    "\n",
    "     return x\n",
    "   \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46854f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba82c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e1a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e68f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Dataset]\n",
    "    for _ in range(100000):\n",
    "        r_int = random.randint(0 , int(len(Training_Dataset)*0.8) - 16)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+16] , dtype='float32')\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+16], dtype='float32')\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+16] , dtype='float32')     #mha needs float tensor\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def Val_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Dataset]\n",
    "    for _ in range(100000):\n",
    "        r_int = random.randint(int(len(Training_Dataset)*0.8) , len(Training_Dataset) - 50)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+50] , dtype='float32')\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+50] , dtype='float32')\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+50] , dtype='float32')             \n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def Pred_Split_Gen():\n",
    "    seti0 = [i[0] for i in Training_Dataset]\n",
    "    seti1 = [i[1] for i in Training_Dataset]\n",
    "    seti2 = [i[2] for i in Training_Dataset]\n",
    "    for _ in range(1):\n",
    "        r_int = random.randint(int(len(Training_Dataset)*0.8) , len(Training_Dataset) - 300)\n",
    "        train_seti0 = tf.convert_to_tensor(seti0[r_int:r_int+300] , dtype='float32')\n",
    "        train_seti1 = tf.convert_to_tensor(seti1[r_int:r_int+300] , dtype='float32')\n",
    "        train_seti2 = tf.convert_to_tensor(seti2[r_int:r_int+300] , dtype='float32')\n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2603da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 6s 36ms/step - loss: 0.8669 - accuracy: 0.4844 - val_loss: 0.7226 - val_accuracy: 0.4836\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.7592 - accuracy: 0.5075 - val_loss: 0.7416 - val_accuracy: 0.4964\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.7388 - accuracy: 0.5194 - val_loss: 0.8419 - val_accuracy: 0.4912\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.7410 - accuracy: 0.5138 - val_loss: 0.7144 - val_accuracy: 0.5072\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.7279 - accuracy: 0.5369 - val_loss: 0.7314 - val_accuracy: 0.4760\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.7083 - accuracy: 0.5594 - val_loss: 0.7668 - val_accuracy: 0.4864\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.7234 - accuracy: 0.5619 - val_loss: 0.8198 - val_accuracy: 0.5208\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.7216 - accuracy: 0.5519 - val_loss: 0.7470 - val_accuracy: 0.5276\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.6975 - accuracy: 0.5788 - val_loss: 0.7493 - val_accuracy: 0.5020\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.6954 - accuracy: 0.5819 - val_loss: 0.7628 - val_accuracy: 0.5056\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.6519 - accuracy: 0.6187 - val_loss: 0.7742 - val_accuracy: 0.5100\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.6752 - accuracy: 0.6162 - val_loss: 0.7720 - val_accuracy: 0.5232\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.6483 - accuracy: 0.6406 - val_loss: 0.7988 - val_accuracy: 0.5328\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.6577 - accuracy: 0.6275 - val_loss: 0.8187 - val_accuracy: 0.5020\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.5896 - accuracy: 0.6950 - val_loss: 0.9289 - val_accuracy: 0.4928\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.5691 - accuracy: 0.7075 - val_loss: 0.9309 - val_accuracy: 0.5296\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.5330 - accuracy: 0.7319 - val_loss: 0.9579 - val_accuracy: 0.5520\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.5293 - accuracy: 0.7456 - val_loss: 0.9396 - val_accuracy: 0.5244\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.5269 - accuracy: 0.7487 - val_loss: 0.8626 - val_accuracy: 0.5320\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.4766 - accuracy: 0.7800 - val_loss: 0.9290 - val_accuracy: 0.5728\n"
     ]
    }
   ],
   "source": [
    "Model = FullModel(176 , 5000 , 176*2 , 0.5 , 1)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model = 176)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate , beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir='Logs/Logs23' ,histogram_freq=1\n",
    ")\n",
    " \n",
    "Model.compile(\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=optimizer,\n",
    "    metrics= 'accuracy' )\n",
    "\n",
    "history = Model.fit(Train_Split_Gen() , epochs=20, \n",
    "                               validation_data = Val_Split_Gen()  , steps_per_epoch=100 , batch_size=16 , \n",
    "                               validation_steps=50 , validation_batch_size=50 , callbacks=[tensorboard_callback] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5b2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.save_weights('Saved_Models/Model5728/Model5728_Weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e1ef9527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(300, 5, 176), dtype=float32, numpy=\n",
       " array([[[   0.,   49.,    0., ...,    0.,    0.,    0.],\n",
       "         [  76.,  150.,    0., ...,    0.,    0.,    0.],\n",
       "         [  76.,  150.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 282.,  248.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,  289.,    0., ...,    0.,    0.,    0.]],\n",
       " \n",
       "        [[ 110.,  382.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 282.,  382.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,  760.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 376.,  382.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,  289.,    0., ...,    0.,    0.,    0.]],\n",
       " \n",
       "        [[   7., 1177.,    0., ...,    0.,    0.,    0.],\n",
       "         [  76., 1177.,    0., ...,    0.,    0.,    0.],\n",
       "         [   7., 1177.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,   49.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,  289.,    0., ...,    0.,    0.,    0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 211.,  368.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 137.,  300.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,   49.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,   49.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 137.,  300.,    0., ...,    0.,    0.,    0.]],\n",
       " \n",
       "        [[   0.,   49.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 274.,  112.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,   49.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 376.,  382.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,   49.,    0., ...,    0.,    0.,    0.]],\n",
       " \n",
       "        [[   0.,   40.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,   49.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 110.,  371.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,   49.,    0., ...,    0.,    0.,    0.],\n",
       "         [ 137.,  371.,    0., ...,    0.,    0.,    0.]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(300, 5, 176), dtype=float32, numpy=\n",
       " array([[[  0., 108.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       " \n",
       "        [[137., 402.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       " \n",
       "        [[274., 537.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[137., 402.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       " \n",
       "        [[274., 112.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       " \n",
       "        [[  0., 289.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]]], dtype=float32)>)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576e9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 13ms/step\n",
      "Accuracy is: 0.51threshold is: 0.0\n",
      "Accuracy is: 0.5366666666666666threshold is: 0.05\n",
      "Accuracy is: 0.56threshold is: 0.1\n",
      "Accuracy is: 0.59threshold is: 0.15\n",
      "Accuracy is: 0.5833333333333334threshold is: 0.2\n",
      "Accuracy is: 0.58threshold is: 0.25\n",
      "Accuracy is: 0.57threshold is: 0.3\n",
      "Accuracy is: 0.56threshold is: 0.35\n",
      "Accuracy is: 0.56threshold is: 0.4\n",
      "Accuracy is: 0.5833333333333334threshold is: 0.45\n",
      "Accuracy is: 0.5566666666666666threshold is: 0.5\n",
      "Accuracy is: 0.55threshold is: 0.55\n",
      "Accuracy is: 0.5266666666666666threshold is: 0.6\n",
      "Accuracy is: 0.5266666666666666threshold is: 0.65\n",
      "Accuracy is: 0.54threshold is: 0.7\n",
      "Accuracy is: 0.5433333333333333threshold is: 0.75\n",
      "Accuracy is: 0.5366666666666666threshold is: 0.8\n",
      "Accuracy is: 0.5166666666666667threshold is: 0.85\n",
      "Accuracy is: 0.5166666666666667threshold is: 0.9\n",
      "Accuracy is: 0.5033333333333333threshold is: 0.95\n"
     ]
    }
   ],
   "source": [
    "### Plot prediction values against true Labels\n",
    "Data , True_Labels = next(Pred_Split_Gen())\n",
    "Predictions = Model.predict(Data)\n",
    "\n",
    "vals = pd.DataFrame([{'z' : 0 ,'x' : i[0],'y' : j[0]} for i,j in zip(Predictions , True_Labels.numpy())])\n",
    "\n",
    "#sns.scatterplot(data=vals , x='z' , y='x' , hue='y')\n",
    "\n",
    "for threshold in range(0,20,1):\n",
    "    count = 0\n",
    "    for i in vals.values:\n",
    "        if i[1] > threshold/20:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = 0\n",
    "        \n",
    "        if pred == i[2]:\n",
    "            count+=1\n",
    "    print('Accuracy is: ' + str(count/len(vals)) + 'threshold is: ' + str(threshold/20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5880b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in vals.values:\n",
    "    print(len(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50364525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.68559194]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array = ['Junk Synchron' , 'Junk Converter'    , 'Doppelwarrior'   , 'Road Synchron' , 'Bystial Baldrake' ,'Fleur Synchron']\n",
    "\n",
    "\n",
    "indexes = Sliced_df[Sliced_df['name'].isin(input_array)].index.values\n",
    "Decider = tf.convert_to_tensor([[Tokenized_sequence_database[i] for i in indexes[0:5]]])\n",
    "\n",
    "\n",
    "Subject = tf.convert_to_tensor([[Tokenized_sequence_database[indexes[5]] , np.zeros(176 ) , np.zeros(176 ) , np.zeros(176 ) , np.zeros(176 )]] )\n",
    "Subject\n",
    "Model((Decider , Subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6848a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_matches():\n",
    "    #input_array = [input('enter') for _ in range(5)]\n",
    "    input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ,'Galaxy Summoner' , 'Photon Vanisher']\n",
    "    \n",
    "    decider = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "    count = 0\n",
    "    for indx in pd.DataFrame(Tokenized_sequence_database).index.values:\n",
    "        if count<0:\n",
    "            pass\n",
    "        else:\n",
    "            if (count % 100) == 0:\n",
    "                print(count)\n",
    "            subject_card = tf.convert_to_tensor([np.concatenate( (Tokenized_sequence_database[indx] , np.zeros(724)) , axis=None )])\n",
    "            \n",
    "            \n",
    "            if Model.predict((decider , subject_card) , verbose=False)[0][0] > 0.5:\n",
    "                print(Sliced_df[indx:indx+1]['name'])\n",
    "                \n",
    "                \n",
    "        count+=1\n",
    "predict_matches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1cb6a5bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Experimentation_Gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [251], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(Model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mExperimentation_Gen\u001b[49m()))\n\u001b[0;32m      2\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pred[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Experimentation_Gen' is not defined"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame(Model.predict(Experimentation_Gen()))\n",
    "count = 0\n",
    "for i in pred[0]:\n",
    "    if i > 0.5:\n",
    "        count+=1\n",
    "print(count/len(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 219ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9966325]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k , trash = next(Experimentation_Gen())\n",
    "l1 , l2 = k\n",
    "singular_pred = Model.predict(k)\n",
    "singular_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9e8875d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"2 aqua effect monster water cannot be used as a synchro material. this card's name becomes des frog while it is on the field. if this card is in your graveyard: you can banish 1 frog monster from your graveyard; special summon this card.\"]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.sequences_to_texts([l2[0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d9ca161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1065,   -1,  398,    0,   -1,  294,   20,    8,    0,   -1,  251,\n",
       "         -1, 1618,   10,    5,  731,    0,    0,    0,    0,    0,   -1,\n",
       "        200,    1,    6,   87,    7,   10,  231,  508,   78,  134,   83,\n",
       "        315,   78,  129,   89,   75,   22,  113,   76,   38,    5,   44,\n",
       "          9,    5,   47,   78,   58,   89,   32,    3,   42,  262,   78,\n",
       "        384,  125,    3,   48,   29,   98,  197,    7,    4,   13,  140,\n",
       "         29,  188,    5,   92,    1,    6,   24,   31,    2,   20,   10,\n",
       "       1618,   10,    5,  731,   18,   19,   25,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenized_sequence_database[4281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638286c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1332: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.start` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1383: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1383: save (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\profiler.py:150: maybe_create_event_file (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
     ]
    }
   ],
   "source": [
    "Model = FullModel(100 , 12647 , 1000 , 0.1 , 2)\n",
    "writer = tf.summary.create_file_writer('Logs/GraphViz')\n",
    "\n",
    "input_array = ['Galaxy Wizard' , 'Galaxy Soldier' , 'Photon Orbital' , 'Galaxy-Eyes Photon Dragon' ,'Galaxy Summoner']\n",
    "\n",
    "decider1 = tf.convert_to_tensor([stitcher(0 , [] , False, input_array)])\n",
    "subject_card1 = tf.convert_to_tensor(    [np.concatenate( (Tokenized_sequence_database[7] , np.zeros(724)) , axis=None )]     ) \n",
    "\n",
    "@tf.function\n",
    "def my_func(x):\n",
    "    return Model(x)\n",
    "\n",
    "tf.summary.trace_on(graph=True,profiler=True)\n",
    "out = my_func((decider1 , subject_card1))\n",
    "\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name='function_Trace',\n",
    "        step=0,\n",
    "        profiler_outdir='Logs\\\\GraphViz\\\\'\n",
    "\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67126460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25bc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd683030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
