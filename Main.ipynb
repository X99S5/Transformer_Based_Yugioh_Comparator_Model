{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc825eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fb4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase Of Update\n",
    "response = requests.get('https://db.ygoprodeck.com/api/v7/cardinfo.php')\n",
    "json_response = response.json()\n",
    "dataset = pd.DataFrame(json_response['data'])\n",
    "\n",
    "dataset.to_csv('Dataset/Yugioh_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc1ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import itertools\n",
    "import random as random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "plt.style.use('Solarize_Light2')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When shutting down notebook\n",
    "session.close()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef56086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = pd.read_csv('Dataset/Yugioh_Database.csv')\n",
    "dataset = dataset.drop(['Unnamed: 0' , 'frameType' , 'archetype' , 'ygoprodeck_url' , 'card_sets' , 'card_images' , 'card_prices' , 'banlist_info'],axis=1)\n",
    "dataset = dataset[dataset['type'] != 'XYZ Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Synchro Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Fusion Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Effect Monster']\n",
    "dataset = dataset[dataset['type'] != 'Pendulum Normal Monster']\n",
    "\n",
    "dataset = dataset[dataset['type'] != 'Skill Card']\n",
    "dataset = dataset[dataset['type'] != 'Monster Token']\n",
    "\n",
    "dataset.loc[dataset['type']=='Normal Monster', ['desc']] = 'NoInfo'\n",
    "dataset = dataset.fillna('0')\n",
    "dataset['level'] = dataset['level'].astype('int32')\n",
    "\n",
    "#dataset[dataset['id'] == 65518099]\n",
    "\n",
    "\n",
    "df = dataset['desc']         #Tokenizer is only trained on desc and based on that . Otherwise if trained on names it would blow vocab up to absurd amounts\n",
    "Sliced_df = dataset[['level' , 'race' , 'type' , 'attribute' , 'name' , 'desc']]\n",
    "\n",
    "for i in range(1,11,2):\n",
    "    Sliced_df.insert(loc=i, column='A'+str(i), value=-1)        # Adds seperator columns\n",
    "\n",
    "\n",
    "Sliced_df = Sliced_df.reset_index(drop=True)            # Need to reset the indexes so they are consistent\n",
    "df = df.reset_index(drop=True)                              \n",
    "\n",
    "tokenizer = Tokenizer(filters='\\r , \\n , \\\" ') # Speech marks stop names from being recognised by tokenizer\n",
    "tokenizer.fit_on_texts(df)\n",
    "tokenizer.word_index['0'] = 0           #Signifies Empty values\n",
    "tokenizer.word_index['-1'] = -1           #Signifies Seperators\n",
    "\n",
    "sequences = []\n",
    "padded_sequences = []\n",
    "Tokenized_sequence_database = []\n",
    "count = 0\n",
    "for i in Sliced_df.astype('string').to_numpy():\n",
    "    \n",
    "    sequences.append(tokenizer.texts_to_sequences(i))\n",
    "     \n",
    "\n",
    "for i in range(0,11):\n",
    "    padded_sequences.append( pad_sequences(np.array(sequences , dtype='object')[:,i], padding='post') ) \n",
    "\n",
    "Tokenized_sequence_database = np.concatenate(([padded_sequences[i] for i in range(11)]) , axis=1 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=======================================\n",
    "\n",
    "\n",
    "''' Deck Based DataSet'''\n",
    "\n",
    "\n",
    "file = open('Dataset/Deck_Lists.txt' , 'r')\n",
    "read = file.readlines()\n",
    "altered = []\n",
    "flag = False\n",
    "\n",
    "temp=[]\n",
    "\n",
    "for count,line in enumerate(read):\n",
    "    \n",
    "    if '//' in read[count]:\n",
    "        flag = not flag\n",
    "    \n",
    "    if flag:\n",
    "        \n",
    "        read[count] = read[count].replace('\\n','')\n",
    "        \n",
    "        if ('=='  in read[count]) or ('//'  in read[count])  :\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "           for i in range(int(read[count][0])):\n",
    "              temp.append(read[count][1:].strip())          #skip appending also remove white space\n",
    "\n",
    "    if (not flag) or (count == len(read) - 1):\n",
    "        altered.append(temp)\n",
    "        temp = []\n",
    "        flag = not flag\n",
    "        \n",
    "        \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf6d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...</td>\n",
       "      <td>[332.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 184...</td>\n",
       "      <td>[0.0, -1.0, 274.0, 0.0, -1.0, 117.0, 8.0, 0.0,...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, -1, 329, 0, -1, 80, 5, 0, 0, -1, 0, -1, 29...</td>\n",
       "      <td>[273.0, -1.0, 418.0, 0.0, -1.0, 93.0, 8.0, 0.0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[332, -1, 254, 0, -1, 21, 8, 0, 0, -1, 138, -1...</td>\n",
       "      <td>[7.0, -1.0, 246.0, 0.0, -1.0, 21.0, 8.0, 0.0, ...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[61, -1, 414, 0, -1, 379, 21, 8, 0, -1, 280, -...</td>\n",
       "      <td>[114.0, -1.0, 1185.0, 0.0, -1.0, 21.0, 8.0, 0....</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>[332, -1, 254, 0, -1, 85, 8, 0, 0, -1, 198, -1...</td>\n",
       "      <td>[97.0, -1.0, 580.0, 0.0, -1.0, 21.0, 8.0, 0.0,...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>[0, -1, 703, 0, -1, 80, 5, 0, 0, -1, 0, -1, 19...</td>\n",
       "      <td>[114.0, -1.0, 286.0, 147.0, -1.0, 21.0, 8.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>[0, -1, 329, 0, -1, 112, 5, 0, 0, -1, 0, -1, 7...</td>\n",
       "      <td>[114.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>[97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...</td>\n",
       "      <td>[114.0, -1.0, 286.0, 147.0, -1.0, 21.0, 8.0, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>[0, -1, 329, 0, -1, 112, 5, 0, 0, -1, 0, -1, 3...</td>\n",
       "      <td>[0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    [97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...   \n",
       "1    [0, -1, 59, 0, -1, 80, 5, 0, 0, -1, 0, -1, 184...   \n",
       "2    [0, -1, 329, 0, -1, 80, 5, 0, 0, -1, 0, -1, 29...   \n",
       "3    [332, -1, 254, 0, -1, 21, 8, 0, 0, -1, 138, -1...   \n",
       "4    [61, -1, 414, 0, -1, 379, 21, 8, 0, -1, 280, -...   \n",
       "..                                                 ...   \n",
       "539  [332, -1, 254, 0, -1, 85, 8, 0, 0, -1, 198, -1...   \n",
       "540  [0, -1, 703, 0, -1, 80, 5, 0, 0, -1, 0, -1, 19...   \n",
       "541  [0, -1, 329, 0, -1, 112, 5, 0, 0, -1, 0, -1, 7...   \n",
       "542  [97, -1, 303, 0, -1, 170, 8, 0, 0, -1, 202, -1...   \n",
       "543  [0, -1, 329, 0, -1, 112, 5, 0, 0, -1, 0, -1, 3...   \n",
       "\n",
       "                                                     1    2  \n",
       "0    [332.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [1]  \n",
       "1    [0.0, -1.0, 274.0, 0.0, -1.0, 117.0, 8.0, 0.0,...  [1]  \n",
       "2    [273.0, -1.0, 418.0, 0.0, -1.0, 93.0, 8.0, 0.0...  [1]  \n",
       "3    [7.0, -1.0, 246.0, 0.0, -1.0, 21.0, 8.0, 0.0, ...  [0]  \n",
       "4    [114.0, -1.0, 1185.0, 0.0, -1.0, 21.0, 8.0, 0....  [0]  \n",
       "..                                                 ...  ...  \n",
       "539  [97.0, -1.0, 580.0, 0.0, -1.0, 21.0, 8.0, 0.0,...  [0]  \n",
       "540  [114.0, -1.0, 286.0, 147.0, -1.0, 21.0, 8.0, 0...  [1]  \n",
       "541  [114.0, -1.0, 364.0, 0.0, -1.0, 21.0, 8.0, 0.0...  [0]  \n",
       "542  [114.0, -1.0, 286.0, 147.0, -1.0, 21.0, 8.0, 0...  [1]  \n",
       "543  [0.0, -1.0, 59.0, 0.0, -1.0, 80.0, 5.0, 0.0, 0...  [0]  \n",
       "\n",
       "[544 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stitcher(deck_index):\n",
    "    '''Picks from decks'''\n",
    "    \n",
    "    decider = [random.choice(altered[deck_index]) for _ in range(5)]\n",
    "    output = np.concatenate(([Tokenized_sequence_database[i] for i in Sliced_df[Sliced_df['name'].isin(decider)].index.values]) )\n",
    "    if len(output) != 905:\n",
    "        return stitcher(deck_index)\n",
    "    else:\n",
    "        return output\n",
    "    \n",
    "Training_Testing_Dataset = []\n",
    "for deck_index,deck in enumerate(altered):\n",
    "    \n",
    "    for card_index in Sliced_df[Sliced_df['name'].isin(deck)].index.values:\n",
    "        data_point = []\n",
    "        \n",
    "        subject_card =  np.concatenate( (Tokenized_sequence_database[card_index] , np.zeros(724)) , axis=None ) # Extends subject to equal length of decider cards\n",
    "        for _ in range(2):\n",
    "            temp = []\n",
    "            decider_cards = stitcher(deck_index)\n",
    "            temp.append(decider_cards)\n",
    "            temp.append(subject_card)\n",
    "            temp.append([1])\n",
    "            Training_Testing_Dataset.append(temp)\n",
    "\n",
    "def random_no_match_generator(length):\n",
    "    out = []\n",
    "    for _ in range(length):\n",
    "        temp = []\n",
    "        subject_card = np.concatenate( ( random.choice(Tokenized_sequence_database)  , np.zeros(724)) , axis=None )\n",
    "        decider_cards = np.concatenate([random.choice(Tokenized_sequence_database) for _ in range(5)])\n",
    "\n",
    "        temp.append(decider_cards)\n",
    "        temp.append(subject_card)\n",
    "        temp.append([0])\n",
    "\n",
    "        out.append(temp)\n",
    "    \n",
    "    return out\n",
    "\n",
    "Training_Testing_Dataset.extend( random_no_match_generator(272) )\n",
    "random.shuffle(Training_Testing_Dataset)\n",
    "random.shuffle(Training_Testing_Dataset)\n",
    "random.shuffle(Training_Testing_Dataset)\n",
    "pd.DataFrame(Training_Testing_Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15d58ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick-play spell card purrely delicious memory choose 1 monster on the field and until the end of the next turn it cannot be destroyed by battle. after choosing a card then you can apply the following effect. ● discard 1 card and if you do special summon 1 level 1 purrely monster from your deck. a purrely xyz monster that has this card as material gains the following effect. ● gains 300 atk/def for each material attached to it.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([Training_Testing_Dataset[542][1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac5140d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Testing_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate( [np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "class Positional_Embedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=False) \n",
    "    self.pos_encoding = positional_encoding(length=905, depth=d_model)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x*= np.sqrt(self.d_model) # Scale Values by their embedding dimensionality otherwise they could get overwhelmed by positional encoder\n",
    "    x = x + self.pos_encoding \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class DeciderSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, ffn, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(ffn, activation='relu'), \n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Single Encoder Layer with DeciderMHA and Feed Forward layer\"\"\"\n",
    "  def __init__(self, d_model, ffn , dropout_rate ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.DSA = DeciderSelfAttention(num_heads=4, key_dim=100)       # Scaling Number of Heads increases parameters as this is a different implementation of mha compared to attention is all you need paper.\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    \n",
    "    \n",
    "    x = self.DSA(x)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Full Encoder with embedding layer with dropout and encoder layers\"\"\"\n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.EL = [EncoderLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.EL[i](x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class SubjectSelfAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class SubjectCrossAttention(BaseAttention):\n",
    "\n",
    "  def call(self, x , context):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        key=context)\n",
    "    \n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class ComparatorLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self , d_model, ffn , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.SSA = SubjectSelfAttention(num_heads=4, key_dim=100)      \n",
    "    self.SCA = SubjectCrossAttention(num_heads=4, key_dim=100)\n",
    "    self.FF = FeedForward(d_model, ffn , dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    \n",
    "    x = self.SSA(x)\n",
    "\n",
    "    x = self.SCA(x , context)\n",
    "\n",
    "    x = self.FF(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "class Comparator(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.Pos_Embedding = Positional_Embedding(vocab_size, d_model)\n",
    "    self.CL = [ComparatorLayer(d_model, ffn , dropout_rate) for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x , context):\n",
    "    x = self.Pos_Embedding(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.CL[i](x , context)\n",
    "\n",
    "    return x\n",
    "\n",
    "class FinalFeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self , dropout_rate):\n",
    "    super().__init__()\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),          #Flattens sentances for each card comparision , into a single 1d array , so it can generate probabilities properly, instead of shoving 100 x905 matrix straight through and generating 100 probabilities for each card comparision feature embedding\n",
    "      tf.keras.layers.Dense(50, activation='relu'),\n",
    "      tf.keras.layers.Dense(25, activation='relu'),\n",
    "      tf.keras.layers.Dropout(dropout_rate),\n",
    "      tf.keras.layers.Dense(1 , activation='sigmoid')\n",
    "      \n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    \n",
    "    x = self.seq(x) \n",
    "    return x\n",
    "  \n",
    "class FullModel(tf.keras.Model):\n",
    "   def __init__(self, d_model, vocab_size , ffn , dropout_rate , num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = Encoder(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.com = Comparator(d_model, vocab_size , ffn , dropout_rate , num_layers)\n",
    "    self.FFF = FinalFeedForward(dropout_rate)\n",
    "\n",
    "   def call(self, inputs):\n",
    "     context , x = inputs\n",
    "     \n",
    "     \n",
    "     \n",
    "     context = self.enc(context)\n",
    "     x = self.com(x , context)\n",
    "\n",
    "     x = self.FFF(context)\n",
    "\n",
    "     return x\n",
    "   \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e1a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e68f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gen():\n",
    "    for _ in range(330):\n",
    "        r_int = random.randint(0 , 200)\n",
    "        train_seti0 = tf.convert_to_tensor([i[0] for i in Training_Testing_Dataset][r_int:r_int+30])\n",
    "        train_seti1 = tf.convert_to_tensor([i[1] for i in Training_Testing_Dataset][r_int:r_int+30])\n",
    "        train_seti2 = tf.convert_to_tensor([i[2] for i in Training_Testing_Dataset][r_int:r_int+30])\n",
    "    \n",
    "    \n",
    "        yield (train_seti0,train_seti1), train_seti2\n",
    "\n",
    "def test_gen():\n",
    "\n",
    "    for _ in range(330):\n",
    "        r_int = random.randint(230 , 400)\n",
    "        test_seti0 = tf.convert_to_tensor([i[0] for i in Training_Testing_Dataset][r_int:r_int+50])\n",
    "        test_seti1 = tf.convert_to_tensor([i[1] for i in Training_Testing_Dataset][r_int:r_int+50])\n",
    "        test_seti2 = tf.convert_to_tensor([i[2] for i in Training_Testing_Dataset][r_int:r_int+50])\n",
    "\n",
    "    \n",
    "        yield (test_seti0,test_seti1), test_seti2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2603da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['full_model_63/comparator_63/positional__embedding_127/embedding_127/embeddings:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/query/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/query/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/key/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/key/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/value/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/value/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/attention_output/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/attention_output/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/layer_normalization_634/gamma:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/layer_normalization_634/beta:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/query/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/query/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/key/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/key/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/value/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/value/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/attention_output/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/attention_output/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/layer_normalization_635/gamma:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/layer_normalization_635/beta:0', 'dense_697/kernel:0', 'dense_697/bias:0', 'dense_698/kernel:0', 'dense_698/bias:0', 'full_model_63/comparator_63/comparator_layer_126/feed_forward_254/layer_normalization_636/gamma:0', 'full_model_63/comparator_63/comparator_layer_126/feed_forward_254/layer_normalization_636/beta:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/query/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/query/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/key/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/key/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/value/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/value/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/attention_output/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/attention_output/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/layer_normalization_637/gamma:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/layer_normalization_637/beta:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/query/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/query/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/key/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/key/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/value/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/value/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/attention_output/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/attention_output/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/layer_normalization_638/gamma:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/layer_normalization_638/beta:0', 'dense_699/kernel:0', 'dense_699/bias:0', 'dense_700/kernel:0', 'dense_700/bias:0', 'full_model_63/comparator_63/comparator_layer_127/feed_forward_255/layer_normalization_639/gamma:0', 'full_model_63/comparator_63/comparator_layer_127/feed_forward_255/layer_normalization_639/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['full_model_63/comparator_63/positional__embedding_127/embedding_127/embeddings:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/query/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/query/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/key/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/key/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/value/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/value/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/attention_output/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/multi_head_attention_380/attention_output/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/layer_normalization_634/gamma:0', 'full_model_63/comparator_63/comparator_layer_126/subject_self_attention_126/layer_normalization_634/beta:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/query/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/query/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/key/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/key/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/value/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/value/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/attention_output/kernel:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/multi_head_attention_381/attention_output/bias:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/layer_normalization_635/gamma:0', 'full_model_63/comparator_63/comparator_layer_126/subject_cross_attention_126/layer_normalization_635/beta:0', 'dense_697/kernel:0', 'dense_697/bias:0', 'dense_698/kernel:0', 'dense_698/bias:0', 'full_model_63/comparator_63/comparator_layer_126/feed_forward_254/layer_normalization_636/gamma:0', 'full_model_63/comparator_63/comparator_layer_126/feed_forward_254/layer_normalization_636/beta:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/query/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/query/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/key/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/key/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/value/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/value/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/attention_output/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/multi_head_attention_382/attention_output/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/layer_normalization_637/gamma:0', 'full_model_63/comparator_63/comparator_layer_127/subject_self_attention_127/layer_normalization_637/beta:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/query/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/query/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/key/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/key/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/value/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/value/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/attention_output/kernel:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/multi_head_attention_383/attention_output/bias:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/layer_normalization_638/gamma:0', 'full_model_63/comparator_63/comparator_layer_127/subject_cross_attention_127/layer_normalization_638/beta:0', 'dense_699/kernel:0', 'dense_699/bias:0', 'dense_700/kernel:0', 'dense_700/bias:0', 'full_model_63/comparator_63/comparator_layer_127/feed_forward_255/layer_normalization_639/gamma:0', 'full_model_63/comparator_63/comparator_layer_127/feed_forward_255/layer_normalization_639/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2\n",
      " 1/10 [==>...........................] - ETA: 26s - loss: 1.0337 - accuracy: 0.40003\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.9198 - accuracy: 0.4500 4\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.9551 - accuracy: 0.44445\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.9133 - accuracy: 0.47506\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.8896 - accuracy: 0.48007\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.8665 - accuracy: 0.48898\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.8471 - accuracy: 0.48109\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.8425 - accuracy: 0.483310\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8210 - accuracy: 0.492611\n",
      "10/10 [==============================] - 8s 536ms/step - loss: 0.8188 - accuracy: 0.5000 - val_loss: 0.7614 - val_accuracy: 0.5720\n",
      "Epoch 2/30\n",
      "12\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.6799 - accuracy: 0.633313\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.7472 - accuracy: 0.533314\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.7790 - accuracy: 0.511115\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.7504 - accuracy: 0.516716\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.7376 - accuracy: 0.526717\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.7264 - accuracy: 0.550018\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.7282 - accuracy: 0.528619\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.7235 - accuracy: 0.525020\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7230 - accuracy: 0.529621\n",
      "10/10 [==============================] - 4s 451ms/step - loss: 0.7232 - accuracy: 0.5200 - val_loss: 0.7372 - val_accuracy: 0.4520\n",
      "Epoch 3/30\n",
      "22\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.7425 - accuracy: 0.500023\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.7708 - accuracy: 0.500024\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.7870 - accuracy: 0.444425\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.7559 - accuracy: 0.475026\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.7484 - accuracy: 0.460027\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.7399 - accuracy: 0.477828\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.7253 - accuracy: 0.500029\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.7204 - accuracy: 0.512530\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7100 - accuracy: 0.522231\n",
      "10/10 [==============================] - 5s 481ms/step - loss: 0.7150 - accuracy: 0.5200 - val_loss: 0.6557 - val_accuracy: 0.6460\n",
      "Epoch 4/30\n",
      "32\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.7036 - accuracy: 0.466733\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.6654 - accuracy: 0.600034\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.6777 - accuracy: 0.544435\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.6711 - accuracy: 0.541736\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.6602 - accuracy: 0.553337\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.6523 - accuracy: 0.566738\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.6493 - accuracy: 0.566739\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.6466 - accuracy: 0.587540\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6497 - accuracy: 0.581541\n",
      "10/10 [==============================] - 5s 481ms/step - loss: 0.6668 - accuracy: 0.5667 - val_loss: 0.6535 - val_accuracy: 0.6180\n",
      "Epoch 5/30\n",
      "42\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.6294 - accuracy: 0.633343\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.6560 - accuracy: 0.533344\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.6419 - accuracy: 0.555645\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.6493 - accuracy: 0.550046\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.6425 - accuracy: 0.573347\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.6445 - accuracy: 0.566748\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.6373 - accuracy: 0.590549\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.6403 - accuracy: 0.595850\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6345 - accuracy: 0.603751\n",
      "10/10 [==============================] - 5s 477ms/step - loss: 0.6321 - accuracy: 0.6067 - val_loss: 0.6607 - val_accuracy: 0.5400\n",
      "Epoch 6/30\n",
      "52\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.5883 - accuracy: 0.600053\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.5931 - accuracy: 0.650054\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.5734 - accuracy: 0.700055\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.5584 - accuracy: 0.733356\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.5804 - accuracy: 0.706757\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.5868 - accuracy: 0.716758\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.5792 - accuracy: 0.728659\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.5584 - accuracy: 0.754260\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5618 - accuracy: 0.737061\n",
      "10/10 [==============================] - 5s 480ms/step - loss: 0.5618 - accuracy: 0.7333 - val_loss: 0.5502 - val_accuracy: 0.7620\n",
      "Epoch 7/30\n",
      "62\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.6118 - accuracy: 0.666763\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.6171 - accuracy: 0.700064\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.6038 - accuracy: 0.700065\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.5977 - accuracy: 0.691766\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.5856 - accuracy: 0.706767\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.5649 - accuracy: 0.727868\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.5601 - accuracy: 0.723869\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.5419 - accuracy: 0.737570\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5456 - accuracy: 0.733371\n",
      "10/10 [==============================] - 5s 482ms/step - loss: 0.5328 - accuracy: 0.7467 - val_loss: 0.5124 - val_accuracy: 0.8020\n",
      "Epoch 8/30\n",
      "72\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.4757 - accuracy: 0.800073\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.4284 - accuracy: 0.816774\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.4739 - accuracy: 0.788975\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.4674 - accuracy: 0.800076\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.4613 - accuracy: 0.800077\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.4647 - accuracy: 0.811178\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.4519 - accuracy: 0.819079\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.4619 - accuracy: 0.804280\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4491 - accuracy: 0.818581\n",
      "10/10 [==============================] - 5s 480ms/step - loss: 0.4540 - accuracy: 0.8133 - val_loss: 0.4980 - val_accuracy: 0.7560\n",
      "Epoch 9/30\n",
      "82\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.3516 - accuracy: 0.966783\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.3351 - accuracy: 0.950084\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.3311 - accuracy: 0.944485\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.3846 - accuracy: 0.883386\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.3785 - accuracy: 0.880087\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3733 - accuracy: 0.888988\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.3592 - accuracy: 0.890589\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.3695 - accuracy: 0.866790\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.3645 - accuracy: 0.866791\n",
      "10/10 [==============================] - 5s 481ms/step - loss: 0.3650 - accuracy: 0.8700 - val_loss: 0.4029 - val_accuracy: 0.8380\n",
      "Epoch 10/30\n",
      "92\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.2974 - accuracy: 0.900093\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.2715 - accuracy: 0.933394\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.2958 - accuracy: 0.933395\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.2851 - accuracy: 0.916796\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.2670 - accuracy: 0.926797\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.2612 - accuracy: 0.927898\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.2757 - accuracy: 0.919099\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.2655 - accuracy: 0.9167100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2622 - accuracy: 0.9185101\n",
      "10/10 [==============================] - 5s 481ms/step - loss: 0.2715 - accuracy: 0.9133 - val_loss: 0.4523 - val_accuracy: 0.7760\n",
      "Epoch 11/30\n",
      "102\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.2555 - accuracy: 0.9333103\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.2618 - accuracy: 0.9500104\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.2597 - accuracy: 0.9556105\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.2962 - accuracy: 0.9417106\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.3035 - accuracy: 0.9267107\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.3008 - accuracy: 0.9278108\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.3107 - accuracy: 0.9048109\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.3174 - accuracy: 0.8875110\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.3024 - accuracy: 0.9000111\n",
      "10/10 [==============================] - 5s 477ms/step - loss: 0.3029 - accuracy: 0.8967 - val_loss: 0.4500 - val_accuracy: 0.7940\n",
      "Epoch 12/30\n",
      "112\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.2267 - accuracy: 0.9000113\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.2573 - accuracy: 0.8833114\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.2620 - accuracy: 0.8889115\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.2578 - accuracy: 0.9083116\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.2468 - accuracy: 0.9133117\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.2271 - accuracy: 0.9278118\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.2254 - accuracy: 0.9238119\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.2243 - accuracy: 0.9208120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2146 - accuracy: 0.9296121\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.2228 - accuracy: 0.9233 - val_loss: 0.4102 - val_accuracy: 0.8100\n",
      "Epoch 13/30\n",
      "122\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.1453 - accuracy: 0.9667123\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.1868 - accuracy: 0.9333124\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.1883 - accuracy: 0.9333125\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.1863 - accuracy: 0.9417126\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.1815 - accuracy: 0.9467127\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.1724 - accuracy: 0.9556128\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.2002 - accuracy: 0.9333129\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.1882 - accuracy: 0.9417130\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.2111 - accuracy: 0.9296131\n",
      "10/10 [==============================] - 5s 478ms/step - loss: 0.2081 - accuracy: 0.9267 - val_loss: 0.3080 - val_accuracy: 0.8820\n",
      "Epoch 14/30\n",
      "132\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0961 - accuracy: 1.0000133\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.1059 - accuracy: 1.0000134\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.1443 - accuracy: 0.9889135\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.1327 - accuracy: 0.9917136\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.1328 - accuracy: 0.9867137\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.1485 - accuracy: 0.9778138\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.1442 - accuracy: 0.9810139\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.1466 - accuracy: 0.9792140\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.1628 - accuracy: 0.9667141\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.1657 - accuracy: 0.9600 - val_loss: 0.3077 - val_accuracy: 0.8640\n",
      "Epoch 15/30\n",
      "142\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.1593 - accuracy: 0.9333143\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.1454 - accuracy: 0.9500144\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.1345 - accuracy: 0.9556145\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.1297 - accuracy: 0.9667146\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.1219 - accuracy: 0.9733147\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.1204 - accuracy: 0.9778148\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.1163 - accuracy: 0.9762149\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.1121 - accuracy: 0.9792150\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.1084 - accuracy: 0.9815151\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.1099 - accuracy: 0.9800 - val_loss: 0.2513 - val_accuracy: 0.9100\n",
      "Epoch 16/30\n",
      "152\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0499 - accuracy: 1.0000153\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0648 - accuracy: 0.9833154\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.0702 - accuracy: 0.9889155\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0679 - accuracy: 0.9917156\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0730 - accuracy: 0.9933157\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0731 - accuracy: 0.9944158\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0737 - accuracy: 0.9905159\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0799 - accuracy: 0.9875160\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0849 - accuracy: 0.9852161\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.0900 - accuracy: 0.9867 - val_loss: 0.3290 - val_accuracy: 0.8500\n",
      "Epoch 17/30\n",
      "162\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0557 - accuracy: 1.0000163\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0981 - accuracy: 0.9833164\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.0893 - accuracy: 0.9889165\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0785 - accuracy: 0.9917166\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0760 - accuracy: 0.9933167\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0944 - accuracy: 0.9833168\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0940 - accuracy: 0.9857169\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0900 - accuracy: 0.9875170\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0904 - accuracy: 0.9889171\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.0904 - accuracy: 0.9900 - val_loss: 0.2726 - val_accuracy: 0.8800\n",
      "Epoch 18/30\n",
      "172\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0669 - accuracy: 0.9667173\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0946 - accuracy: 0.9667174\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.0837 - accuracy: 0.9778175\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0826 - accuracy: 0.9750176\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0738 - accuracy: 0.9800177\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0714 - accuracy: 0.9833178\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0699 - accuracy: 0.9857179\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0748 - accuracy: 0.9792180\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0734 - accuracy: 0.9815181\n",
      "10/10 [==============================] - 5s 480ms/step - loss: 0.0812 - accuracy: 0.9800 - val_loss: 0.2885 - val_accuracy: 0.8680\n",
      "Epoch 19/30\n",
      "182\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0563 - accuracy: 1.0000183\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0485 - accuracy: 1.0000184\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.0551 - accuracy: 1.0000185\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0606 - accuracy: 1.0000186\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0596 - accuracy: 1.0000187\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0552 - accuracy: 1.0000188\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0525 - accuracy: 1.0000189\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0565 - accuracy: 1.0000190\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0569 - accuracy: 1.0000191\n",
      "10/10 [==============================] - 5s 478ms/step - loss: 0.0561 - accuracy: 1.0000 - val_loss: 0.2802 - val_accuracy: 0.8720\n",
      "Epoch 20/30\n",
      "192\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0566 - accuracy: 1.0000193\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0444 - accuracy: 1.0000194\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.0531 - accuracy: 0.9889195\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0487 - accuracy: 0.9917196\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0482 - accuracy: 0.9933197\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0445 - accuracy: 0.9944198\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0485 - accuracy: 0.9952199\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0464 - accuracy: 0.9958200\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0486 - accuracy: 0.9963201\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.0480 - accuracy: 0.9967 - val_loss: 0.3208 - val_accuracy: 0.8660\n",
      "Epoch 21/30\n",
      "202\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0261 - accuracy: 1.0000203\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0642 - accuracy: 0.9833204\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.0560 - accuracy: 0.9889205\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0488 - accuracy: 0.9917206\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0574 - accuracy: 0.9867207\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0574 - accuracy: 0.9889208\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0554 - accuracy: 0.9905209\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0562 - accuracy: 0.9875210\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0548 - accuracy: 0.9889211\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.0520 - accuracy: 0.9900 - val_loss: 0.2333 - val_accuracy: 0.9100\n",
      "Epoch 22/30\n",
      "212\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0474 - accuracy: 1.0000213\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0334 - accuracy: 1.0000214\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.0350 - accuracy: 1.0000215\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0350 - accuracy: 1.0000216\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0343 - accuracy: 1.0000217\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0310 - accuracy: 1.0000218\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000219\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000220\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000221\n",
      "10/10 [==============================] - 5s 481ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9280\n",
      "Epoch 23/30\n",
      "222\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0166 - accuracy: 1.0000223\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0231 - accuracy: 1.0000224\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.0225 - accuracy: 1.0000225\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0224 - accuracy: 1.0000226\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0212 - accuracy: 1.0000227\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0237 - accuracy: 1.0000228\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0214 - accuracy: 1.0000229\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000230\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0226 - accuracy: 1.0000231\n",
      "10/10 [==============================] - 5s 478ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.1681 - val_accuracy: 0.9280\n",
      "Epoch 24/30\n",
      "232\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0109 - accuracy: 1.0000233\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0186 - accuracy: 1.0000234\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.0218 - accuracy: 1.0000235\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0224 - accuracy: 1.0000236\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0204 - accuracy: 1.0000237\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0222 - accuracy: 1.0000238\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0205 - accuracy: 1.0000239\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0199 - accuracy: 1.0000240\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0194 - accuracy: 1.0000241\n",
      "10/10 [==============================] - 5s 480ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9380\n",
      "Epoch 25/30\n",
      "242\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0449 - accuracy: 0.9667243\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0326 - accuracy: 0.9833244\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.0302 - accuracy: 0.9889245\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0262 - accuracy: 0.9917246\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0238 - accuracy: 0.9933247\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0250 - accuracy: 0.9944248\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0230 - accuracy: 0.9952249\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0225 - accuracy: 0.9958250\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0207 - accuracy: 0.9963251\n",
      "10/10 [==============================] - 5s 480ms/step - loss: 0.0240 - accuracy: 0.9967 - val_loss: 0.1961 - val_accuracy: 0.9200\n",
      "Epoch 26/30\n",
      "252\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0225 - accuracy: 1.0000253\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0165 - accuracy: 1.0000254\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.0155 - accuracy: 1.0000255\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0192 - accuracy: 1.0000256\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0184 - accuracy: 1.0000257\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0165 - accuracy: 1.0000258\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0156 - accuracy: 1.0000259\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000260\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000261\n",
      "10/10 [==============================] - 5s 478ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.9260\n",
      "Epoch 27/30\n",
      "262\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0096 - accuracy: 1.0000263\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0176 - accuracy: 1.0000264\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.0167 - accuracy: 1.0000265\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0152 - accuracy: 1.0000266\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0151 - accuracy: 1.0000267\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0142 - accuracy: 1.0000268\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0135 - accuracy: 1.0000269\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000270\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0136 - accuracy: 1.0000271\n",
      "10/10 [==============================] - 5s 481ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9240\n",
      "Epoch 28/30\n",
      "272\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0149 - accuracy: 1.0000273\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0103 - accuracy: 1.0000274\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.0102 - accuracy: 1.0000275\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0109 - accuracy: 1.0000276\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0108 - accuracy: 1.0000277\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0100 - accuracy: 1.0000278\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000279\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000280\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000281\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.1989 - val_accuracy: 0.9180\n",
      "Epoch 29/30\n",
      "282\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0068 - accuracy: 1.0000283\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0058 - accuracy: 1.0000284\n",
      " 3/10 [========>.....................] - ETA: 1s - loss: 0.0053 - accuracy: 1.0000285\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0046 - accuracy: 1.0000286\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0050 - accuracy: 1.0000287\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0050 - accuracy: 1.0000288\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000289\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000290\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000291\n",
      "10/10 [==============================] - 5s 477ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2280 - val_accuracy: 0.9180\n",
      "Epoch 30/30\n",
      "292\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0414 - accuracy: 0.9667293\n",
      " 2/10 [=====>........................] - ETA: 2s - loss: 0.0327 - accuracy: 0.9833294\n",
      " 3/10 [========>.....................] - ETA: 2s - loss: 0.0335 - accuracy: 0.9889295\n",
      " 4/10 [===========>..................] - ETA: 1s - loss: 0.0265 - accuracy: 0.9917296\n",
      " 5/10 [==============>...............] - ETA: 1s - loss: 0.0226 - accuracy: 0.9933297\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0218 - accuracy: 0.9944298\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0205 - accuracy: 0.9952299\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0191 - accuracy: 0.9958300\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0185 - accuracy: 0.9963301\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.0176 - accuracy: 0.9967 - val_loss: 0.2255 - val_accuracy: 0.9360\n"
     ]
    }
   ],
   "source": [
    "#Testing accuracy is much higher than training accuracy. This is due to dropouts causing lower accuracy during training but giving a more robust model when testing\n",
    "Model = FullModel(100 , 12647 , 1000 , 0.1 , 2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model = 100)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "Model.compile(\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=optimizer,\n",
    "    metrics= 'accuracy' )\n",
    "\n",
    "history = Model.fit(train_gen() , epochs=30, \n",
    "                               validation_data = test_gen()  , steps_per_epoch=10 , batch_size=30 , validation_steps=10 , validation_batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6848a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6a5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8875d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ca161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638286c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67126460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf9a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25bc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd683030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
